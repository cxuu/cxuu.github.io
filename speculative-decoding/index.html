<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <link rel="icon" href="/assets/images/logo.png"> <title>Accelerate LLM Inference with Speculative Decoding | Charles Xu</title> <title>Accelerate LLM Inference with Speculative Decoding | Charles Xu</title> <meta name="generator" content="Jekyll v4.3.2"/> <meta property="og:title" content="Accelerate LLM Inference with Speculative Decoding"/> <meta property="og:locale" content="en_US"/> <meta name="description" content="Many inference speedup techniques mirror the classic systems regime—such as caching, paging, tiling, pipelining, and speculative execution (e.g. branch prediction and cache prefetch). Speculative decoding, generalizing speculative execution to stochastic settings, produces several tokens in each forward pass, without changing the output distribution (model quality) or model parameters."/> <meta property="og:description" content="Many inference speedup techniques mirror the classic systems regime—such as caching, paging, tiling, pipelining, and speculative execution (e.g. branch prediction and cache prefetch). Speculative decoding, generalizing speculative execution to stochastic settings, produces several tokens in each forward pass, without changing the output distribution (model quality) or model parameters."/> <meta property="og:site_name" content="Charles Xu"/> <meta property="og:image" content="/assets/images/speculative-decoding/cover.png"/> <meta property="og:type" content="article"/> <meta property="article:published_time" content="2025-03-11T00:00:00+00:00"/> <meta name="twitter:card" content="summary_large_image"/> <meta property="twitter:image" content="/assets/images/speculative-decoding/cover.png"/> <meta property="twitter:title" content="Accelerate LLM Inference with Speculative Decoding"/> <script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-03-11T00:00:00+00:00","datePublished":"2025-03-11T00:00:00+00:00","description":"Many inference speedup techniques mirror the classic systems regime—such as caching, paging, tiling, pipelining, and speculative execution (e.g. branch prediction and cache prefetch). Speculative decoding, generalizing speculative execution to stochastic settings, produces several tokens in each forward pass, without changing the output distribution (model quality) or model parameters.","headline":"Accelerate LLM Inference with Speculative Decoding","image":"/assets/images/speculative-decoding/cover.png","mainEntityOfPage":{"@type":"WebPage","@id":"/speculative-decoding/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/assets/images/logo.png"}},"url":"/speculative-decoding/"}</script> <link href="/assets/css/bootstrap.min.css" rel="stylesheet"> <script src="/assets/js/jquery.min.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-8CYZ0N0EWJ"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-8CYZ0N0EWJ");</script> <script>!function(e,t,a,n,c,o,s){e.GoogleAnalyticsObject=c,e[c]=e[c]||function(){(e[c].q=e[c].q||[]).push(arguments)},e[c].l=1*new Date,o=t.createElement(a),s=t.getElementsByTagName(a)[0],o.async=1,o.src=n,s.parentNode.insertBefore(o,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-151349369-1","auto"),ga("send","pageview");</script> <script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{"token": "5b6e3d4ee3274005a2d3321f9bb0516c"}'></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0}};</script> </head> <body class="layout-post"> <noscript id="deferred-styles"> <link href="/assets/css/fontawesome.css" rel="stylesheet"> <link href="/assets/css/google-fonts.css" rel="stylesheet"> </noscript> <nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down"> <div class="container pr-0"> <a class="navbar-brand" href="/"> <img src="/assets/images/logo.png" alt="Charles Xu" height="32" width="32"> </a> <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button> <div class="collapse navbar-collapse" id="navbarMediumish"> <ul class="navbar-nav ml-auto"> <li class="nav-item"> <a class="nav-link" href="/about">About</a> </li> <li class="nav-item"> <a class="nav-link" href="/bookshelf">Bookshelf</a> </li> <li class="nav-item"> <a class="nav-link" href="/inspirations">Inspirations</a> </li> <li class="nav-item"> <a class="nav-link" href="/wiki">Wiki</a> </li> <li class="nav-item"> <a class="nav-link" href="/">Blog</a> </li> <script src="/assets/js/lunr.js"></script> <style>.lunrsearchresult .title{color:#d9230f}.lunrsearchresult .url{color:silver}.lunrsearchresult a{display:block;color:#777}.lunrsearchresult a:hover,.lunrsearchresult a:focus{text-decoration:none}.lunrsearchresult a:hover .title{text-decoration:underline}</style> <div style="width: 14px; height: 10px;"></div> <form class="bd-search" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);"> <input type="text" class="form-control text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/> </form> <div id="lunrsearchresults"> <ul></ul> </div> <script src="/assets/js/lunrsearchengine.js"></script> </ul> </div> </div> </nav> <div class="site-content"> <div class="container"> <div class="mainheading"> <h1 class="sitetitle">Charles Xu</h1> <p class="lead"> Essays, books, wiki on technologies, career, markets, and more. </p> </div> <div id="loading"> <div id="loading-image" class="lds-ellipsis"><div></div><div></div><div></div><div></div></div> </div> <script>$(window).on("load",function(){$("#loading").hide()});</script> <div class="main-content"> <div class="container"> <div class="row"> <div class="col-md-2 pl-0"> <div class="share sticky-top sticky-top-offset"> <p> Share </p> <ul> <li class="ml-1 mr-1"> <a target="_blank" href="https://twitter.com/intent/tweet?text=Accelerate LLM Inference with Speculative Decoding&url=charlesxu.io/speculative-decoding/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;"> <i class="fab fa-twitter"></i> </a> </li> <li class="ml-1 mr-1"> <a target="_blank" href="https://facebook.com/sharer.php?u=charlesxu.io/speculative-decoding/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;"> <i class="fab fa-facebook-f"></i> </a> </li> <li class="ml-1 mr-1"> <a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&url=charlesxu.io/speculative-decoding/" onclick="window.open(this.href, 'width=550,height=435');return false;"> <i class="fab fa-linkedin-in"></i> </a> </li> </ul> <div class="sep"> </div> <ul> <li> <a class="small smoothscroll" href="#disqus_thread"></a> </li> </ul> </div> </div> <div class="col-md-8 flex-first flex-md-unordered"> <div class="mainheading"> <h1 class="posttitle">Accelerate LLM Inference with Speculative Decoding</h1> </div> <img class="featured-image img-fluid lazyimg" style="min-width: 100%" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=" data-src="/assets/images/speculative-decoding/cover.png" alt="Accelerate LLM Inference with Speculative Decoding"> <div class="article-post"> <p>Many inference speedup techniques mirror the classic systems regime—such as caching, paging, tiling, pipelining, and speculative execution (e.g. branch prediction and cache prefetch). Speculative decoding, generalizing speculative execution to stochastic settings, produces several tokens in each forward pass, without changing the output distribution (model quality) or model parameters.</p> <p>This post discusses two approaches of Speculative Decoding:</p> <ol> <li>Speculative sampling with a draft model (2023 <a href="https://arxiv.org/abs/2211.17192">paper</a>)</li> <li>Multiple decoding heads with Tree Attention (2024 <a href="https://arxiv.org/abs/2401.10774">paper</a>)</li> </ol> <h3 id="speculative-sampling-with-a-draft-model">Speculative Sampling with a Draft Model</h3> <p>Speculative sampling, or rejection sampling, uses an approximation/draft model—smaller and faster than the model you want to accelerate—to generate $k$ tokens autoregressively and then uses the larger model to verify the $k$ tokens in one pass.</p> <h4 id="how-to-verify-each-speculative-token">How to verify each speculative token</h4> <p>Let $M_p$ be the target model and $M_q$ be the approximation model. <br/> Let $p(x)$ be the probability of $x$ under $M_p$ (a shorthand for $p(x_t | x_1, \ldots, x_{t-1})$). <br/> Let $q(x)$ be the probability of $x$ under $M_q$.</p> <p>Sample a token $x$ from $M_q$. Don’t wait for the big model to verify $x$. Continue to sample the next $k$ tokens from $M_q$.</p> <p>Then, for each speculative token $x$: <br/> If $p(x) \geq q(x)$, accept $x$. <br/> If $p(x) &lt; q(x)$, accept $x$ with probability $p(x) / q(x)$; if $x$ not accepted, sample again from an adjusted distribution of $M_p$, where $p’(x) = norm(max(0, p(x) - q(x)))$.</p> <p>It is <a href="https://arxiv.org/abs/2211.17192">proven</a> that $x$ sampled this way ensures $x \sim p(x)$.</p> <h4 id="verify-in-one-pass">Verify in one pass</h4> <p>The key insight is that the big model can verify the $k$ speculative tokens in one pass. The reason is that in the self-attention mechanism, the big model computes contextualized representations of all prefixes in parallel, i.e. the model outputs</p> \[p(x_1 | \text{prefix}), \quad p(x_2 | \text{prefix} + x_1), \quad \ldots, \quad p(x_k | \text{prefix} + x_1 + x_2 + \ldots + x_{k-1})\] <p>simultaneously.</p> <h3 id="multiple-decoding-heads-with-tree-attention">Multiple Decoding Heads with Tree Attention</h3> <p>Choosing the right draft model is hard. What if we just reuse the same model?</p> <p><a href="https://arxiv.org/abs/2401.10774">Medusa</a> proposes attaching multiple decoding heads to the same model. In the case of 2 heads, head 1 predicts the immediate next token, and head 2 predicts the second token after the prefix. Two heads output at the same time.</p> <div style="text-align: center"> <p><img src="/assets/images/speculative-decoding/medusa.png" width="600"/></p> </div> <p>(image <a href="https://sites.google.com/view/medusa-llm">source</a>)</p> <h4 id="training-multiple-decoding-heads">Training multiple decoding heads</h4> <p>The standard model only has one decoding head tasked to predict the next token. Thus, to use multiple decoding heads, we need to train the extra heads. The training needs:</p> <ul> <li>loss function</li> <li>tree attention &amp; adjusted positional encoding</li> </ul> <h4 id="loss-function">Loss function</h4> <p>Use the cross-entropy loss between the prediction of extra heads and the ground truth. To quote the paper:</p> <blockquote> <p>Given the ground truth token $y_{t+k+1}$ at position $t+k+1$, the loss for the $k$-th head is $L_k = - \log p_t^{(k)} (y_{t+k+1})$, where $p_t^{(k)} (y)$ denotes the probability of token $y$ predicted by the $k$-th head.</p> <p>We also observe that $L_k$ is larger when $k$ is larger, which is reasonable since the prediction of the $k$-th head is more uncertain when $k$ is larger. Therefore, we can add a weight $\lambda_k$ to $L_k$ to balance the loss of different heads. The final loss is:</p> \[\mathcal{L}_{\text{MEDUSA-1}} = \sum_{k=1}^{K} -\lambda_k \log p_t^{(k)}(y_{t+k+1}).\] </blockquote> <h4 id="tree-attention--adjusted-positional-encoding">Tree attention &amp; adjusted positional encoding</h4> <p>Suppose the first decoding head predicts 2 candidates for the next token, and the second decoding head predicts 3 candidates for the next next token. We have a tree of $2 \times 3 = 6$ branches. The tree structure creates two challenges:</p> <ol> <li> <p>We need to adjust attention mask such that a token generated from a specific candidate path can only attend to previous tokens within that same path and should ignore other branches.</p> </li> <li> <p>We need to adjust positional encoding because there are multiple candidates for the same position (as in depth in the tree).</p> </li> </ol> <p>The solution is Tree Attention shown below. Note that the attention mask exclusively permits attention flow from the current token back to its antecedent tokens.</p> <div style="text-align: center"> <p><img src="/assets/images/speculative-decoding/tree_attn.png" width="600"/></p> </div> <p>(image <a href="https://sites.google.com/view/medusa-llm">source</a>)</p> </div> <p> <small> <span class="post-date"><time class="post-date" datetime="2025-03-11">11 Mar 2025</time></span> </small> </p> <div class="after-post-cats"> <ul class="tags mb-4"> <li> <a class="smoothscroll" href="/categories#artificial-intelligence">artificial intelligence</a> </li> <li> <a class="smoothscroll" href="/categories#llm">llm</a> </li> </ul> </div> <div class="after-post-tags"> <ul class="tags"> </ul> </div> <div class="row PageNavigation d-flex justify-content-between font-weight-bold"> <a class="prev d-block col-md-6" href="/multi-head-attention/"> &laquo; Parallelizing Multi-Head Attention</a> <a class="next d-block col-md-6 text-lg-right" href="/parallel-tokenizer/">3.3x Faster HuggingFace Tokenizers for Single Sequence &raquo; </a> <div class="clearfix"></div> </div> </div> </div> </div> </div> <div class="alertbar"> <div class="container text-center"> <span><img src="/assets/images/logo.png" alt="Charles Xu" height="32" width="32"> &nbsp; Never miss a <b>story</b> from me, subscribe to my newsletter</span> <form action="https://gmail.us5.list-manage.com/subscribe/post?u=b3d456844a3860642cd584c1b&amp;id=3f0c5c8bcd" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate" target="_blank" novalidate> <div class="mc-field-group"> <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required> <input type="submit" value="Subscribe" name="subscribe" class="heart"> </div> </form> </div> </div> </div> <div class="jumbotron fortags"> <div class="d-md-flex h-100"> <div class="col-md-4 transpdark align-self-center text-center h-100"> <div class="d-md-flex align-items-center justify-content-center h-100"> <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2> </div> </div> <div class="col-md-8 p-5 align-self-center text-center"> <a class="mt-1 mb-1" href="/categories#git">git (3)</a> <a class="mt-1 mb-1" href="/categories#web">web (9)</a> <a class="mt-1 mb-1" href="/categories#microservices">microservices (8)</a> <a class="mt-1 mb-1" href="/categories#distributed-systems">distributed systems (6)</a> <a class="mt-1 mb-1" href="/categories#signal-processing">signal processing (1)</a> <a class="mt-1 mb-1" href="/categories#networking">networking (11)</a> <a class="mt-1 mb-1" href="/categories#istio">istio (4)</a> <a class="mt-1 mb-1" href="/categories#security">security (1)</a> <a class="mt-1 mb-1" href="/categories#docker">docker (2)</a> <a class="mt-1 mb-1" href="/categories#kubernetes">kubernetes (9)</a> <a class="mt-1 mb-1" href="/categories#operation">operation (4)</a> <a class="mt-1 mb-1" href="/categories#career">career (5)</a> <a class="mt-1 mb-1" href="/categories#go">go (1)</a> <a class="mt-1 mb-1" href="/categories#cloud">cloud (4)</a> <a class="mt-1 mb-1" href="/categories#investment">investment (2)</a> <a class="mt-1 mb-1" href="/categories#startup">startup (6)</a> <a class="mt-1 mb-1" href="/categories#oss">oss (1)</a> <a class="mt-1 mb-1" href="/categories#artificial-intelligence">artificial intelligence (3)</a> <a class="mt-1 mb-1" href="/categories#llm">llm (3)</a> </div> </div> </div> <footer class="footer"> <div class="container"> <div class="row"> <div class="col-md-6 col-sm-6 text-center text-lg-left" style="margin-bottom: 10px;"> Copyright © 2016-2025 Charles Xu </div> </div> </div> </footer> </div> <script src="/assets/js/popper.min.js"></script> <script src="/assets/js/bootstrap.min.js"></script> <script src="/assets/js/mediumish.js"></script> <script src="/assets/js/lazyload.js"></script> <script src="/assets/js/ie10-viewport-bug-workaround.js"></script> <link href="/assets/css/screen.css" rel="stylesheet"> <link href="/assets/css/main.css" rel="stylesheet"> </body> </html>