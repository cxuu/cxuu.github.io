<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <link rel="icon" href="/assets/images/logo.png"> <title>Parallelizing Multi-Head Attention | Charles Xu</title> <title>Parallelizing Multi-Head Attention | Charles Xu</title> <meta name="generator" content="Jekyll v4.3.2"/> <meta property="og:title" content="Parallelizing Multi-Head Attention"/> <meta property="og:locale" content="en_US"/> <meta name="description" content="In the multi-head attention mechanism, why after reshaping the projection matrices for Q/K/V from 3 dimensions to 4, we need to transpose the tokens dimension with the heads dimension?"/> <meta property="og:description" content="In the multi-head attention mechanism, why after reshaping the projection matrices for Q/K/V from 3 dimensions to 4, we need to transpose the tokens dimension with the heads dimension?"/> <meta property="og:site_name" content="Charles Xu"/> <meta property="og:image" content="/assets/images/multi-head-attention/cover.jpg"/> <meta property="og:type" content="article"/> <meta property="article:published_time" content="2025-02-28T00:00:00+00:00"/> <meta name="twitter:card" content="summary_large_image"/> <meta property="twitter:image" content="/assets/images/multi-head-attention/cover.jpg"/> <meta property="twitter:title" content="Parallelizing Multi-Head Attention"/> <script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-02-28T00:00:00+00:00","datePublished":"2025-02-28T00:00:00+00:00","description":"In the multi-head attention mechanism, why after reshaping the projection matrices for Q/K/V from 3 dimensions to 4, we need to transpose the tokens dimension with the heads dimension?","headline":"Parallelizing Multi-Head Attention","image":"/assets/images/multi-head-attention/cover.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"/multi-head-attention/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/assets/images/logo.png"}},"url":"/multi-head-attention/"}</script> <link href="/assets/css/bootstrap.min.css" rel="stylesheet"> <script src="/assets/js/jquery.min.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-8CYZ0N0EWJ"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-8CYZ0N0EWJ");</script> <script>!function(e,t,a,n,c,o,s){e.GoogleAnalyticsObject=c,e[c]=e[c]||function(){(e[c].q=e[c].q||[]).push(arguments)},e[c].l=1*new Date,o=t.createElement(a),s=t.getElementsByTagName(a)[0],o.async=1,o.src=n,s.parentNode.insertBefore(o,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-151349369-1","auto"),ga("send","pageview");</script> <script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{"token": "5b6e3d4ee3274005a2d3321f9bb0516c"}'></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0}};</script> </head> <body class="layout-post"> <noscript id="deferred-styles"> <link href="/assets/css/fontawesome.css" rel="stylesheet"> <link href="/assets/css/google-fonts.css" rel="stylesheet"> </noscript> <nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down"> <div class="container pr-0"> <a class="navbar-brand" href="/"> <img src="/assets/images/logo.png" alt="Charles Xu" height="32" width="32"> </a> <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button> <div class="collapse navbar-collapse" id="navbarMediumish"> <ul class="navbar-nav ml-auto"> <li class="nav-item"> <a class="nav-link" href="/about">About</a> </li> <li class="nav-item"> <a class="nav-link" href="/bookshelf">Bookshelf</a> </li> <li class="nav-item"> <a class="nav-link" href="/inspirations">Inspirations</a> </li> <li class="nav-item"> <a class="nav-link" href="/wiki">Wiki</a> </li> <li class="nav-item"> <a class="nav-link" href="/">Blog</a> </li> <script src="/assets/js/lunr.js"></script> <style>.lunrsearchresult .title{color:#d9230f}.lunrsearchresult .url{color:silver}.lunrsearchresult a{display:block;color:#777}.lunrsearchresult a:hover,.lunrsearchresult a:focus{text-decoration:none}.lunrsearchresult a:hover .title{text-decoration:underline}</style> <div style="width: 14px; height: 10px;"></div> <form class="bd-search" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);"> <input type="text" class="form-control text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/> </form> <div id="lunrsearchresults"> <ul></ul> </div> <script src="/assets/js/lunrsearchengine.js"></script> </ul> </div> </div> </nav> <div class="site-content"> <div class="container"> <div class="mainheading"> <h1 class="sitetitle">Charles Xu</h1> <p class="lead"> Essays, books, wiki on technologies, career, markets, and more. </p> </div> <div id="loading"> <div id="loading-image" class="lds-ellipsis"><div></div><div></div><div></div><div></div></div> </div> <script>$(window).on("load",function(){$("#loading").hide()});</script> <div class="main-content"> <div class="container"> <div class="row"> <div class="col-md-2 pl-0"> <div class="share sticky-top sticky-top-offset"> <p> Share </p> <ul> <li class="ml-1 mr-1"> <a target="_blank" href="https://twitter.com/intent/tweet?text=Parallelizing Multi-Head Attention&url=charlesxu.io/multi-head-attention/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;"> <i class="fab fa-twitter"></i> </a> </li> <li class="ml-1 mr-1"> <a target="_blank" href="https://facebook.com/sharer.php?u=charlesxu.io/multi-head-attention/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;"> <i class="fab fa-facebook-f"></i> </a> </li> <li class="ml-1 mr-1"> <a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&url=charlesxu.io/multi-head-attention/" onclick="window.open(this.href, 'width=550,height=435');return false;"> <i class="fab fa-linkedin-in"></i> </a> </li> </ul> <div class="sep"> </div> <ul> <li> <a class="small smoothscroll" href="#disqus_thread"></a> </li> </ul> </div> </div> <div class="col-md-8 flex-first flex-md-unordered"> <div class="mainheading"> <h1 class="posttitle">Parallelizing Multi-Head Attention</h1> </div> <img class="featured-image img-fluid lazyimg" style="min-width: 100%" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=" data-src="/assets/images/multi-head-attention/cover.jpg" alt="Parallelizing Multi-Head Attention"> <div class="article-post"> <p>In the multi-head attention mechanism, why after reshaping the projection matrices for Q/K/V from 3 dimensions to 4, we need to transpose the <code class="language-plaintext highlighter-rouge">tokens</code> dimension with the <code class="language-plaintext highlighter-rouge">heads</code> dimension?</p> <p>Using the canonical example code for Attention Heads below as an example, why do we need <code class="language-plaintext highlighter-rouge">Q = self.W_q(x).reshape(B, T, self.num_heads, self.head_dim).transpose(1, 2)</code>?</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_size</span> <span class="o">//</span> <span class="n">num_heads</span>  <span class="c1"># Split embeddings across heads
</span>
        <span class="c1"># Linear layers for Q, K, V
</span>        <span class="n">self</span><span class="p">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
        <span class="c1"># Fully connected output layer, i.e. $W^O$
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fc_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>  <span class="c1"># Batch, Seq_len, Embedding_dim
</span>        <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_q</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_k</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_v</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Compute attention scores (scaled dot-product attention)
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Normalize scores
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># Apply attention to values
</span>
        <span class="c1"># Concatenate and project back to embedding size
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc_out</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="c1"># Example usage
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>  <span class="c1"># Batch size 1, Sequence length 10, Embedding size 512
</span><span class="n">attention_layer</span> <span class="o">=</span> <span class="nc">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">embed_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">attention_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Should be [1, 10, 512]
</span></pre></td></tr></tbody></table></code></pre></div></div> <h3 id="review-multi-head-attention">Review Multi-head Attention</h3> <p>Instead of using a single attention head, transformers use multiple heads. Each attention head has its own set of projection matrices for Q, K, and V. Each head learns to focus on different types of relationships. For example:</p> <ul> <li>One head might focus on long-range dependencies, such as linking a subject to its verb.</li> <li>Another might focus on local context, such as detecting adjective-noun pairs.</li> <li>Another might specialize in syntax or semantics.</li> </ul> <p>Each head has independent learnable weight/projection matrices $W_i^Q, W_i^K, W_i^V$, where $i$ is the head index. Each weight matrix has shape $(D, D/h)$ where $D$ is the embedding dimension and $h$ is the number of heads.</p> <p>For input embeddings $X$ of shape $(B, T, D)$, the head $i$ computes:</p> \[Q_i = X W_i^Q, \quad K_i = X W_i^K, \quad V_i = X W_i^V\] <p>The multi-head attention then concatenates the per-head attention outputs and linearly mixes them:</p> \[\text{MultiHeadAttention}(Q, K, V) = \text{Concat}\left(\text{head}_1, \text{head}_2, \ldots, \text{head}_h\right)W^O\] <p>Each head has output shape $(B, T, D/h)$. The concatenated output has shape $(B, T, D)$. $W^O$ is trainable and has shape $(D, D)$. $W^O$ mixes information across heads and refines the final representation before passing it to the next layer.</p> <h3 id="reshaping-for-parallelization">Reshaping for Parallelization</h3> <p>Let’s break down <code class="language-plaintext highlighter-rouge">Q = self.W_q(x).reshape(B, T, self.num_heads, self.head_dim).transpose(1, 2)</code>:</p> <p><code class="language-plaintext highlighter-rouge">self.W_q</code> is a linear layer (<code class="language-plaintext highlighter-rouge">nn.Linear(embed_size, embed_size)</code>) that projects <code class="language-plaintext highlighter-rouge">x</code> into a new representation, specifically the query (<code class="language-plaintext highlighter-rouge">Q</code>) in multi-head attention. This linear transformation does not change the shape of <code class="language-plaintext highlighter-rouge">x</code>, which remains <code class="language-plaintext highlighter-rouge">(B, T, D)</code>.</p> <p>In <code class="language-plaintext highlighter-rouge">.reshape(B, T, self.num_heads, self.head_dim)</code>, we split the embedding dimension <code class="language-plaintext highlighter-rouge">D</code> into <code class="language-plaintext highlighter-rouge">self.num_heads</code> and <code class="language-plaintext highlighter-rouge">self.head_dim</code> (where <code class="language-plaintext highlighter-rouge">self.head_dim = D / self.num_heads</code>). For example, if <code class="language-plaintext highlighter-rouge">D = 512</code> and <code class="language-plaintext highlighter-rouge">self.num_heads = 8</code>, then <code class="language-plaintext highlighter-rouge">self.head_dim = 512 / 8 = 64</code>. The shape becomes: <code class="language-plaintext highlighter-rouge">(B, T, 8, 64)</code>.</p> <p>In <code class="language-plaintext highlighter-rouge">.transpose(1, 2)</code>, we swap the sequence length dimension (<code class="language-plaintext highlighter-rouge">T</code>) and the number of heads dimension (<code class="language-plaintext highlighter-rouge">num_heads</code>). Note that index 1 is the second column. This changes the shape:</p> <p>Before: <code class="language-plaintext highlighter-rouge">(B, T, num_heads, head_dim)</code> <br/> After: <code class="language-plaintext highlighter-rouge">(B, num_heads, T, head_dim)</code></p> <p>This rearrangement makes attention more efficient, because matrix multiplications can now parallelize across heads (each head operates independently on different parts of the embedding). The reason is the following.</p> <p>In the code <code class="language-plaintext highlighter-rouge">scores = torch.matmul(Q, K.transpose(-1, -2)) / (self.head_dim ** 0.5)</code>, <code class="language-plaintext highlighter-rouge">K.transpose(-1, -2)</code> swaps the last two dimensions so that <code class="language-plaintext highlighter-rouge">K</code> has the shape <code class="language-plaintext highlighter-rouge">K^T: (B, num_heads, head_dim, T)</code>.</p> <p>Then, the matrix multiplication <code class="language-plaintext highlighter-rouge">Q @ K^T</code> is <code class="language-plaintext highlighter-rouge">(B, num_heads, T, head_dim) @ (B, num_heads, head_dim, T)</code>, which results in <code class="language-plaintext highlighter-rouge">(B, num_heads, T, T)</code>.</p> <p>Thus, <code class="language-plaintext highlighter-rouge">Q @ K^T</code> becomes more efficient after <code class="language-plaintext highlighter-rouge">transpose(1, 2)</code> because:</p> <ul> <li><strong>Parallel Computation for Each Head</strong>: By keeping <code class="language-plaintext highlighter-rouge">num_heads</code> as the second dimension, each head’s computation happens independently but in parallel across the batch, using optimized GPU kernels.</li> <li><strong>Better Memory Access Patterns</strong>: GPUs are highly optimized for contiguous memory access. The transpose(1, 2) operation ensures that each head’s data is grouped together, improving cache efficiency during matrix multiplication.</li> </ul> </div> <p> <small> <span class="post-date"><time class="post-date" datetime="2025-02-28">28 Feb 2025</time></span> </small> </p> <div class="after-post-cats"> <ul class="tags mb-4"> <li> <a class="smoothscroll" href="/categories#artificial-intelligence">artificial intelligence</a> </li> <li> <a class="smoothscroll" href="/categories#llm">llm</a> </li> </ul> </div> <div class="after-post-tags"> <ul class="tags"> </ul> </div> <div class="row PageNavigation d-flex justify-content-between font-weight-bold"> <a class="prev d-block col-md-6" href="/lean-startup/"> &laquo; Notes: The Lean Startup</a> <div class="clearfix"></div> </div> </div> </div> </div> </div> <div class="alertbar"> <div class="container text-center"> <span><img src="/assets/images/logo.png" alt="Charles Xu" height="32" width="32"> &nbsp; Never miss a <b>story</b> from me, subscribe to my newsletter</span> <form action="https://gmail.us5.list-manage.com/subscribe/post?u=b3d456844a3860642cd584c1b&amp;id=3f0c5c8bcd" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate" target="_blank" novalidate> <div class="mc-field-group"> <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required> <input type="submit" value="Subscribe" name="subscribe" class="heart"> </div> </form> </div> </div> </div> <div class="jumbotron fortags"> <div class="d-md-flex h-100"> <div class="col-md-4 transpdark align-self-center text-center h-100"> <div class="d-md-flex align-items-center justify-content-center h-100"> <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2> </div> </div> <div class="col-md-8 p-5 align-self-center text-center"> <a class="mt-1 mb-1" href="/categories#git">git (3)</a> <a class="mt-1 mb-1" href="/categories#web">web (9)</a> <a class="mt-1 mb-1" href="/categories#microservices">microservices (8)</a> <a class="mt-1 mb-1" href="/categories#distributed-systems">distributed systems (6)</a> <a class="mt-1 mb-1" href="/categories#signal-processing">signal processing (1)</a> <a class="mt-1 mb-1" href="/categories#networking">networking (11)</a> <a class="mt-1 mb-1" href="/categories#istio">istio (4)</a> <a class="mt-1 mb-1" href="/categories#security">security (1)</a> <a class="mt-1 mb-1" href="/categories#docker">docker (2)</a> <a class="mt-1 mb-1" href="/categories#kubernetes">kubernetes (9)</a> <a class="mt-1 mb-1" href="/categories#operation">operation (4)</a> <a class="mt-1 mb-1" href="/categories#career">career (5)</a> <a class="mt-1 mb-1" href="/categories#go">go (1)</a> <a class="mt-1 mb-1" href="/categories#cloud">cloud (4)</a> <a class="mt-1 mb-1" href="/categories#investment">investment (2)</a> <a class="mt-1 mb-1" href="/categories#startup">startup (6)</a> <a class="mt-1 mb-1" href="/categories#oss">oss (1)</a> <a class="mt-1 mb-1" href="/categories#artificial-intelligence">artificial intelligence (1)</a> <a class="mt-1 mb-1" href="/categories#llm">llm (1)</a> </div> </div> </div> <footer class="footer"> <div class="container"> <div class="row"> <div class="col-md-6 col-sm-6 text-center text-lg-left" style="margin-bottom: 10px;"> Copyright © 2016-2025 Charles Xu </div> </div> </div> </footer> </div> <script src="/assets/js/popper.min.js"></script> <script src="/assets/js/bootstrap.min.js"></script> <script src="/assets/js/mediumish.js"></script> <script src="/assets/js/lazyload.js"></script> <script src="/assets/js/ie10-viewport-bug-workaround.js"></script> <link href="/assets/css/screen.css" rel="stylesheet"> <link href="/assets/css/main.css" rel="stylesheet"> </body> </html>