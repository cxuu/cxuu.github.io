function lunr_search(e){if(document.getElementById("lunrsearchresults").innerHTML="<ul></ul>",e){document.getElementById("lunrsearchresults").innerHTML="<p>Search results for '"+e+"'</p>"+document.getElementById("lunrsearchresults").innerHTML;var t=idx.search(e);if(0<t.length)for(var o=0;o<t.length;o++){var a=t[o].ref,i=documents[a].url,s=documents[a].title,n=documents[a].body.substring(0,160)+"...";document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML=document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML+"<li class='lunrsearchresult'><a href='"+i+"'><span class='title'>"+s+"</span><br /><span class='body'>"+n+"</span><br /><span class='url'>"+i+"</span></a></li>"}else document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML="<li class='lunrsearchresult'>No results found...</li>"}return!1}function lunr_search(e){if($("#lunrsearchresults").show(400),$("body").addClass("modal-open"),document.getElementById("lunrsearchresults").innerHTML='<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>',e){document.getElementById("modtit").innerHTML="<h5 class='modal-title'>Search results for '"+e+"'</h5>"+document.getElementById("modtit").innerHTML;var t=idx.search(e);if(0<t.length)for(var o=0;o<t.length;o++){var a=t[o].ref,i=documents[a].url,s=documents[a].title,n=documents[a].body.substring(0,160)+"...";document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML=document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML+"<li class='lunrsearchresult'><a href='"+i+"'><span class='title'>"+s+"</span><br /><small><span class='body'>"+n+"</span><br /><span class='url'>"+i+"</span></small></a></li>"}else document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML="<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>"}return!1}var documents=[{id:0,url:"/404.html",title:"404",body:" 404 Page does not exist! Please use the search bar at the top or visit our homepage! "},{id:1,url:"/about",title:"Who am I",body:"             1234567891011121314&lt;br /&gt;&lt;p&gt;&lt;a href= https://www. linkedin. com/in/cxuu/ &gt; &lt;i class= fab fa-linkedin &gt;&lt;/i&gt; Charles Xu&lt;/a&gt;&lt;br /&gt;&lt;a href= mailto:mail@charlesxu. io &gt; &lt;i class= fas fa-envelope &gt;&lt;/i&gt; mail@charlesxu. io&lt;/a&gt;&lt;br /&gt;&lt;a href= https://github. com/cxuu &gt; &lt;i class= fab fa-github &gt;&lt;/i&gt; cxuu&lt;/a&gt;&lt;/p&gt;          &lt;/div&gt;               Charles .         I am a Plumbing Lead at Snowflake building and unclogging the things that run things.  Previously, I worked at Cruise developing multi-tenant container platforms with self-driving Kubernetes and at Google Cloud building the (almost) production-ready service mesh Istio.     While working at Snowflake, I studied Management Science on the Stanford Farm.  Before moving to the Bay Area to pay so much in rent for so little, I was an undergrad at Duke studying CS, EE, and Finance. I would be a totally different person had I not met my rock star research advisor Prof. Jeff Chase. He taught me to make the investment early that pays dividends in the future. He exhorted me to be serious and sincere about life and work\u2014not to fight a fake war or someone else\u2019s war but to strive for what matters to the living or the dead.  He is also probably the only person who has read my thesis cover to cover.     I have been writing on this blog since 2016. My interests are Distributed Systems, Startups, and Career Development, but this blog engages a wider audience. It stands as a living record of my growth and struggles, triumphs and lessons in my work and life. I hope you find them useful, too.     I am always excited to hear from folks. Drop a note!    "},{id:2,url:"/bookshelf",title:"Bookshelf",
body:"    These are the books I have read and recommend, in no particular order. Titles in bold are the ones I enjoyed the most.   I think of life as a search problem. Yet in a lifetime, I could only traverseso many paths and possibilities. I find reading a powerful and rewarding passionthat allows me to learn what others have explored.   With millions of books to choose from, we are faced with another search problem. Like living, reading is highly personal. Alas, I hope this list is useful if youshare my interests below.    Startups Personal Development Relationships Management Career Growth Finance and Markets Engineering         Fundraising:       * Venture Deals: Be Smarter Than Your Lawyer and Venture Capitalist        Sugests what to do before and during fundraising.  Explains the terms in a term sheet.  Offers negociation tips.  See my notes here.            Secrets of Sand Hill Road: Venture Capital and How to Get It        VC returns follow power-law distribution, so VC needs nontrivial allocations in home-run winners operating in huge markets. Connect with VC through relationships. Cold emails hardly work. For series-A term sheets, push for having the capital \u201cP\u201d Preferred vote on voluntary conversion. Avoid board observers, because they might chime in discussions and their presence crowds the room and discourages straight talks. Preferred and common investors are not always aligned, particularly with acquisitions + liquidation preferences. The Board has fiduciary duty only the common shareholders.            Straight Talk for Startups: 100 Insider Rules for Beating the Odds        Aim for an order-of-magnitude improvement. Hire part-time experts rather than full-time trainees. Track unit economics and working capital. Conduct early, low-cost tests of your ideas. In this order: Idea, Technology, Product, Market, Economics, Scale. Keep your top performer happy and committed. Surprise them with bonuses, options refresh, larger scope or work. Reference check VC: talk to portfolio companies, former associates and partners. Deal directly with the decision makers at the VC. Dilution is relative; Out of cash is terminal. Always be thinking about the next round. Terms from prior rounds are hard to eliminate. Prioritize vanilla terms (no excessive liquidation preferences or ratchet), indicator of an aligned investor. Allow 6+ months for fundraising. Create urgency and scarcity. Don\u2019t share who else you are talking to.            Startup Incorporation for Founders        Delaware C Corp for startups. No LLC. Limited liability, ability to allocate ownership, easier recruiting &amp; fundraising, reduced co-founder riskPrefer Delaware because of better legal ecosystem, investor preference, ease of filing, board flexibility, privacy.        Early-Stage:       * The Lean Startup: How Today\u2019s Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses        The real progress for startups is not how many JIRA tickets we closed but how fast we gain validated learnings\u2014what creates value for customers and their willingness to pay\u2014while minimizing waste. The book offers a systemic approach to PMF. See my notes here.            * Build: An Unorthodox Guide to Making Things Worth Making        Navigating job levels and politics are not real personal growth. The only failure in your twenties is inaction. Ask for forgiveness, not permission. Do, fail, learn. Find every opportunity to work with the best of the best, because they will lead you to the career you want. With not enough data, some decisions must be opinion-driven, where you convince others with storytelling. Storytelling is the heart of sales and marketing. The story: you have a track record of good decisions, you analyzed risks and prepared mitigation, you truly understand your customers and their needs, and your proposal will benefit the business. People won\u2019t remember how you started but how you left. A great analogy allows a customer to instantly grasp a difficult feature. E. g. \u201c1,000 songs in your pocket\u201d. If your company is disruptive, prepared for strong reactions and emotions. Write a press release before building products. It crystallizes what features matter. Practice delayed intuition. Research and prototype before committing. The first 25 hires come down to your vision and network. Hire seed crystals. Always start the pitching process when you don\u2019t actually need money. Keep asking what\u2019s the next step to get us to a yes?Any stock or cash you give as a commission should vest over time. Build a relationship-first sales culture. Keep meetings on track with agenda: \u201cLet\u2019s talk about this outside the meeting, we still have lots to cover\u201d. If there was any controversial topic, the CEO should go to every board member, one-on-one, to walk them through it before the meeting. Perks should be subsidized but not free.            The SaaS Playbook: Build a Multimillion-Dollar Startup Without Venture Capital        Easiest way to raise funding is building a great business. Each $1,000 in MRR adds $60,000 in company value (assuming 5x multiple). To go from PMF to escape velocity: ICP, marketing channels, moats, pricing, team, strategies, funnel bottlenecks. Avoid feature requests that don\u2019t align with the product\u2019s strengths or vision. Say no to many good ideas to prioritize the best ones. Understand the problem behind customer feature requests, not just the requested solution. Compete against bigger competitors through pricing, innovative sales models, and superior product offerings. Understand reasons behind losing deals (features, pricing, compliance). Differentiate to avoid commoditization. Network effects and integrations can be effective moats. Pricing should reflect value. Underpricing can hinder growth. Expansion revenue should align with customer success and value metrics. Charge significantly more for enterprise plans. Consider freemium models only for simple, low-support, viral products. Regularly review and adjust pricing strategies. Balance marketing efforts between quick-result and long-term strategies. Track marketing effectiveness. Avoid excessive marketing spend before PMF. Qualify sales leads before demos to save time. Delegate roles not tasks when building a team.            Zero to One: Notes on Startups, or How to Build the Future        Competition is for losers. 0 to 1 is different from 1 to n. Every moment in business happens only once. It is easier to copy than to create. Leanness is a methodology, not a goal. Making small changes to things that alreadyexist might lead you to a local maximum, but it won\u2019t help you find the global maximum.            The Hard Thing About Hard Things: Building a Business When There Are No Easy Answers     Growth-Stage:       * High Growth Handbook: Scaling Startups From 10 to 10,000 People        Great startups prioritize distribution over product. Product itself is not defensive because of too many great engineers and second-mover advantages. Raise prices to test PMF, fund distribution and R&amp;D, and grow faster. Delegate. Audit your calendar regularly. Say no more often. Passing 50 hires, hold weekly staff meetings and start layering in HR. Empower smart people. Communicate context, not exert control. One of the cofounders should be dominant. Board members should be people who you wish to hire but are out of reach otherwise. Take a lower valuation if necessary to get the right board. Talk to board members individually for open-ended brainstorming. Key determinant of candidate conversion is how quickly you interview + make an offer. Reference check everyone: \u201cIf this person joined my company, would you join?\u201dSpend 30\u201350% of their time early on (scaling from 3 to 15 people) on recruiting.            * Amp It Up: Leading for Hypergrowth by Raising Expectations, Increasing Urgency, and Elevating Intensity        Raise your standards, B players drive A players out. Hire people ahead of their own curve. Hire for aptitude over experience. Give people the career opportunity of a lifetime, a huge motivation. Coaching struggling teammates to a better place is possible but rare. Regardless of open headcounts, maintain a top-talent list and check in periodically because1) if you wait until an opening, you can only tap the then-current suboptimal supply. 2) people currently occupying these roles might leave or could not grow as fast as the company. Staff ahead of need. Recruiting never stops. Trust is earned by delivering your promises. Always underpromise and overdeliver. Publicly admit regrettable decisions, which encourages everyone to do so. Apply focus and urgency. Time kills all deals. Time introduces risks, such as new entrants. The faster we separate from the competition, the more likely we are to succeed. \u201cPriority\u201d is a singular word. The moment you have many, you actually have none. Great execution is rarer than great strategy. Growth trumps everything else as a driver and predictor of long-term success. Once a start-up begins showing profits, investors conclude that either it doesn\u2019t know how to invest in further growth or that it has run out of growth opportunities.            Simple Rules: How to Thrive in a Complex World        Strategy and execution are both critical. Simple rules can bridge the gap between strategy and execution. Simple rules allow people to act without deliberating every decision. E. g. how frontline medics decide who gets medical care. Simple rules drive alignment across organizations and produce better decisions than more complicated models can, particularly when time and information are limited. 4-6 is the optimal number of rules. Three types of rules:1) Boundary rules\u2014whether to pursue or reject an opportunity,2) Priority rules\u2014rank options given limited resources,3) Stopping rules\u2014when to reverse or exit.            Blitzscaling: The Lightning-Fast Path to Building Massively Valuable Companies        Do the things that don\u2019t scale. Prioritize growth and speed over efficiency. Good insights, but could be condensed into just one chapter.        Relationships:       * Connect: Building Exceptional Relationships with Family, Friends, and Colleagues        Self-disclosure strengthens relationships, despite risk of being misunderstood. Stretch your relationship comfort zone 15% at a time. To communicate well we must express facts/cognitions and feelings/emotions. Emotions assign meaning, intensity, and importance to facts. Showing your vulnerability often brings people closer. Stick to your reality. Make no assumption about others. It is a profound difference between\u201cI feel irritated and dismissed\u201d and \u201cI feel that you don\u2019t care. \u201d            * Difficult Conversations: How to Discuss What Matters Most        Inquire instead of assuming intentions. Elaborate my emotions, and listen to theirs. Intentions, emotions, and identities are complex. I cannot control but prepare for others\u2019 reaction. Explore joint contributions to the dispute. Start with mines to avoid their defensiveness, but include theirs. People want to be heard. Listen actively by paraphrasing, eye contact, notes taking, and deep questions.            Getting to Yes: Negotiating Agreement Without Giving In        Negotiation is everywhere in life. The fixed pie mentality is not appropriate, because you will see the same people again. No need to grab all the value. Never lie because people always find out. Don\u2019t give the numbers too early because you did not know everything at first. Practice is the only way to improve. Emphasize fairness and win-win. It is way easier to negotiate what is the right criteria and standard than to directly negotiate the price.            Never Split the Difference: Negotiating As If Your Life Depended On It        Mirroring upwards invites elaboration. Mirroring downwards (late-night FM DJ voice)builds empathy, trust, and calmness. Humans are emotional, sometimes irrational. Label the counterpart\u2019s emotion and motivation to get them to say \u201cthat\u2019s right\u201d. \u201cNo\u201d is not the end of negotiation. Explore alternatives.            Never Eat Alone, And Other Secrets To Success, One Relationship At A Time        Give before you take. You gain trust by helping others. Trust solidifies relationships. Relationships build institutions. Build the relationships long before you need them. Be audacious. Respect the gatekeeper (e. g. assistants) who will make or break your access to the decision makers. Follow up frequently. Forward relevant articles to your network. Go to conferences to meet people, not to learn new knowledge. You engender life-bonding loyalty when you help others with their health, wealth, and children. Build an online presence with personal, generous, and candid messages.            \u5982\u4f55\u7ed3\u4ea4\u6bd4\u4f60\u66f4\u4f18\u79c0\u7684\u4eba        \u4ea4\u670b\u53cb\u770b\u673a\u7f18\uff0c\u4f46\u66f4\u8981\u7ecf\u8425\u3002\u5feb\u901f\u5efa\u7acb\u8fde\u63a5\u7684\u529e\u6cd5\u662f\u627e\u548c\u5bf9\u65b9\u7684\u5171\u540c\u70b9\u3002\u8d5e\u7f8e\u662f\u6253\u6d88\u9694\u9602\u7684\u7b2c\u4e00\u6b65\u3002\u9500\u552e\u7684\u6838\u5fc3\u662f\u4e3a\u5ba2\u6237\u521b\u9020\u4ef7\u503c\u3002\u9ebb\u70e6\u6216\u5e2e\u52a9\u5f31\u8fde\u63a5\u7684\u4eba\u53ef\u4ee5\u628a\u5f31\u8fde\u63a5\u8f6c\u5316\u6210\u5f3a\u8fde\u63a5\u3002\u7ecf\u8fc7\u4e2d\u95f4\u4eba\u63a8\u8350\u8ba4\u8bc6\u662f\u6377\u5f84\u3002\u5411\u4e0a\u4ea4\u5f80\u9700\u8981\u5145\u5206\u7684\u51c6\u5907\u3002\u8bdd\u9898\u7684\u51c6\u5907\u5de5\u4f5c\u5360\u6210\u529f\u768480%\u3002\u4ece\u4e00\u5207\u53ef\u80fd\u7684\u6e20\u9053\u4e86\u89e3\u5bf9\u65b9\uff0c\u5305\u62ec\u4ed6\u7684\u51fa\u751f\u5730\u3001\u804c\u4e1a\u3001\u6559\u80b2\u3001\u516c\u53f8\u3001\u5bb6\u5ead\u3001\u7231\u597d\u3001\u516c\u76ca\uff0c\u627e\u5230\u548c\u5bf9\u65b9\u7684\u5171\u540c\u70b9\u3002\u62dc\u8bbf\u5927\u5496\u65f6\uff0c\u5e26\u5c0f\u793c\u54c1\uff0c\u65e0\u9700\u8d35\u91cd\uff0c\u4f46\u6709\u5fc3\u610f\uff0c\u6bd4\u5982\u8bf4\u4ed6\u559c\u6b22\u827a\u672f\uff0c\u53ef\u4ee5\u9001\u4e00\u4e2a\u4e16\u754c\u8457\u540d\u535a\u7269\u9986\u7684\u753b\u518c\uff1b\u5979\u559c\u6b22\u5386\u53f2\uff0c\u4f60\u53ef\u4ee5\u9001\u4e00\u672c\u597d\u7684\u5386\u53f2\u4f20\u8bb0\u3002\u4e3b\u52a8\u505a\u6d3b\u52a8\u6216\u8005\u4f1a\u8bae\u7684\u5fd7\u613f\u8005\u3001\u7ec4\u7ec7\u8005\u80fd\u521b\u9020\u5411\u4e0a\u793e\u4ea4\u7684\u673a\u4f1a\u3002\u63d0\u524d\u5230\u8fbe\u4f1a\u573a\u80fd\u51cf\u5c11\u51fa\u5e2d\u793e\u4ea4\u573a\u5408\u7684\u538b\u529b\u548c\u6050\u60e7\u3002\u4e3b\u52a8\u9080\u8bf7\u7ea6\u996d\u80fd\u514b\u670d\u793e\u4ea4\u6050\u60e7\uff0c\u83b7\u5f97\u4e3b\u573a\u4f18\u52bf\uff0c\u638c\u63a7\u65f6\u95f4\u5730\u70b9\u3002\u4e0d\u719f\u7684\u4eba\u4ece\u7ea6\u5348\u996d\u5f00\u59cb\uff0c\u5348\u9910\u4e0d\u5360\u7528\u4e2a\u4eba\u548c\u5bb6\u5ead\u7684\u65f6\u95f4\uff0c\u63a5\u53d7\u9080\u8bf7\u7684\u6982\u7387\u8981\u6bd4\u665a\u9910\u5927\u3002\u201d\u60a8\u597d\uff0c\u6211\u4e0b\u5468\u4e09\u6b63\u597d\u5230\u60a8\u516c\u53f8\u9644\u8fd1\u529e\u4e8b\uff0c\u4e0d\u77e5\u5348\u996d\u60a8\u6709\u5b89\u6392\u4e86\u5417\uff1f\u5e0c\u671b\u548c\u60a8\u5728\u9644\u8fd1\u5403\u4e2a\u4fbf\u996d\uff0c\u804a\u4e00\u804a\u3002\u201d\u7ea6\u4e0d\u719f\u7684\u4eba\u8981\u544a\u77e5\u89c1\u9762\u76ee\u7684\u548c\u60f3\u804a\u7684\u4e8b\u60c5\uff0c\u540c\u65f6\u95f4\u63a5\u5730\u63d0\u51fa\u4f60\u81ea\u5df1\u7684\u4ef7\u503c\uff0c\u6bd4\u5982\uff1a\u201d\u59d0\uff0c\u6211\u6700\u8fd1\u5728\u67d0\u94f6\u884c\u505a\u79c1\u4eba\u94f6\u884c\u7684\u5f00\u653e\u4e1a\u52a1\uff0c\u56e0\u4e3a\u60a8\u7684\u4eba\u8109\u5e7f\u6cdb\uff0c\u5f88\u60f3\u8ddf\u60a8\u53d6\u53d6\u7ecf\uff0c\u4e5f\u770b\u770b\u6211\u80fd\u5bf9\u60a8\u7684\u4e1a\u52a1\u6709\u4ec0\u4e48\u5e2e\u52a9\u6ca1\u6709\u3002\u60a8\u4e0b\u5468\u6709\u65f6\u95f4\u4e00\u8d77\u5403\u4e2a\u996d\u5417\uff1f\u201d \u9080\u8bf7\u4e00\u4e9b\u53ef\u80fd\u6709\u5408\u529b\u7684\u4eba\u4e00\u8d77\u5403\u996d\uff0c\u544a\u77e5\u88ab\u9080\u8bf7\u8005\u5176\u4ed6\u4e00\u8d77\u51fa\u5e2d\u7684\u4eba\uff0c\u53ef\u80fd\u4f1a\u5f15\u8d77\u4ed6\u4eec\u7684\u5174\u8da3\u3002\u5982\u679c\u4f60\u8ba4\u8bc6\u9910\u5385\u7684\u4e3b\u53a8\uff0c\u5728\u90a3\u91cc\u8bf7\u5ba2\u5c31\u66f4\u68d2\u4e86\uff0c\u4ed6\u80fd\u63d0\u524d\u5e2e\u4f60\u5b89\u6392\u6700\u597d\u7684\u5ea7\u4f4d\u548c\u83dc\u54c1\uff0c\u671f\u95f4\u8fd8\u80fd\u51fa\u6765\u548c\u4f60\u5bd2\u6684\u51e0\u53e5\uff0c\u95ee\u95ee\u5ba2\u4eba\u5403\u5f97\u600e\u4e48\u6837\u3002            The 20-Minute Networking Meeting - Executive Edition: Learn to Network. Get a Job.        Most jobs are filled by contacts through personal relationships. Networking meeting goals: Gather information, Add new contacts, Gain an evangelist. Always be prepared for your meetings. Leave such impression: You are positive (upbeat tone, language); You are strategic (you know why you are there); You are organized (keep track on topics and time, take notes); You are gracious (grateful for their time); You follow through (prompt follow-up, meaningful ongoing interactions). Ask how you can help them. Gently ask for referrals. Bad: \u201cWho else do you know that might be interested in buying from my line of custom clothing?\u201d Good: \u201cWho do you know someone interested in fashion?\u201d        Management:       Influence: The Psychology of Persuasion        In decision making, human often resort to shortcut/reflex, such asreciprocation, consistency between commitment and action, conforming withthose similar to us, adherence to authority, preference to folks we like, andequating scarcity with value. Awareness of such routines keeps oneself alertedto exploiters and magnifies your influence in work and life.            The Making of a Manager: What to Do When Everyone Looks to You        A good manager improves collective outcome with multiplier effects. Focus on purpose, people, and process. Prepare for meetings. Feedback is a gift. Manage expectations. Trust, delegate, grow.            The Manager\u2019s Path: A Guide for Tech Leaders Navigating Growth and Change        Be kind, not nice. Practice continuous feedback. Delegate. Listen. Empathize. Build personal connections. Praise in public; criticize in private. What you measure, you improve. Set expectations. Process is risk management. Be curious, ask questions, learn. Better to talk about learning instead of structure, transparency instead of process. Culture is how the things get done, without people having to think about it.            Good Boss, Bad Boss: How to Be the Best and Learn from the Worst        People quit bad managers, not companies. Direct reports watch everything you do (\u201ctoxic tandem\u201d). Watch your tone of voice, the way you look at people,the use of nicknames, a memory for faces, names and dates. These details refine your relationships. Effective leaders are both competent and benevolent. Humans prefer hierarchical relationships. The challenge is not to reinvent managementbut dampen known drawbacks.        Career Growth:       Simply Said: Communicating Better at Work and Beyond        Prioritize the audience needs. Use simple, concise words. Vocal variety (volume, speed, tone) maintains listener engagement. When presenting, explain what the audience is looking at before elaborating. Avoid \u201cinside jokes\u201d or references that may exclude parts of your audience. Listening is more powerful than speaking. Take notes, ask follow-ups, maintain eye contact. Consistency between words and actions builds trust and credibility. Ensure goals are SMART (Specific, Measurable, Attainable, Relevant, Time-bound). Articulate the importance of goals, especially from the perspective of your followers.            HBR Guide to Office Politics        Every office is political. You can\u2019t avoid it. Use politics to get more things done. Look for personal similarities that make it easier for you to connect professionally to your colleagues and boss. Don\u2019t slip down to your colleague\u2019s level, always take a higher road, call your colleague on the bad behavior. Build positive relationship with your boss instead of looking for ways to dethrone the pet. Build a network outside of your employer. To be looped in big projects, express interest in them. \u201cI know I\u2019m not on that assignment, but could I sit in on a status meeting to learn more about it?\u2019\u201d Once you\u2019re in the room, offer to pitch in. Raise your hand for any role to start with. Very few people rise to the top of their profession without allies. Form more than one allies. It is critical to have them backing your promotion. Build relationships before you need them. Transparency builds trust. Don\u2019t align yourself too strongly or permanently with anyone. Review and prune alliances. Escalate peer conflicts to your boss as a last resort: \u201cJohn, I don\u2019t think you and I are getting anywhere trying to resolve this issue. Would you be willing to go with me to ask Lydia for her help in working through a solution?\u201d            So Good They Can\u2019t Ignore You: Why Skills Trump Passion in the Quest for Work You Love        \u201cFollow you passion\u201d is a bad advice. A rare and valuable job requiresrare and valuable skills (career capital). Acquiring more career capital withdeliberate practice gives you the autonomy to pursuit the work you love. Deliberate practice means doing things that hurt: playing guitar pieces aboveyour skill level, or practicing the tennis backhand that you suck at.            The Holloway Guide to Equity Compensation        Detailed explanations of ISO, NSO, RSU, and taxes with lots of references. 83(b) election. Secondary markets. Right of first refusal. AMT trap. Liquidation overhang.            The Coding Career Handbook. Guides, Principles, Strategies, and Tactics        Learn in the public. Write a lot. Open source your knowledge. Good enough is better than best. Invest in new technologies. Know your tools.        Personal Development:       * The Almanack of Naval Ravikant: A Guide to Wealth and Happiness        Renting out your time or just working hard will not lead to financial freedom. 99% of effort is wasted. Wealth comes from judgment. Play long-term games with long-term people. Find a worthy mate; be worthy of a worthy mate. Find work that feels like play. All returns in life\u2014wealth, relationships, knowledge\u2014come from compound interest. Reading is faster than listening; doing is faster than watching. If you cannot decide, the answer is no.            Atomic Habits: An Easy &amp; Proven Way to Build Good Habits &amp; Break Bad Ones        Habits are compounding and shape who we are and what we will achieve. The best way to build a habit is to make it part of your identity(I am trying to quit smoking vs. I am not a smoker), attach the new habitto existing ones, make it small (atomic), and surround yourself with peoplewho have the habits you want. Willpower is limited; instead, design the environment to support your habits.            Quit: The Power of Knowing When to Walk Away        The same grit that won Ali the championships drove him to ignore signs to quit. Constantly reevaluate after learning new info. Free up resources to pursue better opportunities. Define quitting criteria (a state and a date) before starting. The worst time to make a decision is when you\u2019re \u201cin it. \u201dQuitting is about forecasting\u2014decision making with uncertainty. Make decision based on probability, expectation, and distribution. There is no point building the pedestal if you can\u2019t train the monkey. Tackle the hardest part of the problem first. Pedestal-building creates the illusion of progress and sunk cost. Be wary of escalation of commitment to a losing course of action. Quitting on time feels like quitting too early. Diversify your opportunities, keep exploring even if you are comfortable. Don\u2019t wait to be forced to quit to start exploring alternatives.            What I Wish I Knew When I Was 20: A Crash Course on Making Your Place in the World        Grant yourself permission instead of waiting for others to do so. Cold email the people you admire. If you are not failing sometimes, you arenot taking enough risks. Do not burn bridges. You are not going to like everyone and not everyone is going to like you, butthere is no need for enemies. To make good decision in dilemmas, think about howyou want to tell the story in a future job interview. Recognize your mistake,apologize early and profusely.            The Subtle Art of Not Giving a F*ck: A Counterintuitive Approach to Living a Good Life        There\u2019s no such thing as not giving a fuck. You must give a fuck about something. Rude gas station attendants and boring day-time TV shows do not deserve your limited attention. Prioritize your life based on personal values. You learn what matters to you through emotions and psychological pain. Happiness is about doing, not being. Nothing is once-and-for-all. Life is full of problems. True happiness occurs only when you find the problems you enjoy having and enjoy solving.        Finance and Markets:       * Fooled by Randomness: The Hidden Role of Chance in Life and in the Markets        We often confuse luck with skills, and noise with signals. Investment returns areunpredictable. Stay in the game. Manage risk to never get wiped out. View the past and the futurewith probability/uncertainty, but understandthe difference between probability and expectation.            The Psychology of Money: Timeless lessons on wealth, greed, and happiness        Aim to be reasonable, not rational. The highest form of wealth is the abilityto do whatever wherever whenever. Note that financial independence is not exactlyabout maximizing returns. Few things matter more with money thanunderstanding your own time horizon and not being persuaded by the actionsand behaviors of people playing different games than you are.            Flash Boys: A Wall Street Revolt        Wall street in 2010s. High frequency trading is not market making. It createstwice the volume, no additional liquidity, and perhaps a slightly worseexecution price for the public. I wonder if network switches and FPGA got so much fasterbecause of it.        Autobiography:       The Last Lecture        The last lecture delivered by Professor Randy Pausch after his pancreatic cancer diagnosis. Feel empowered to dream big and enable others\u2019 dreams. Try hard. Be kind.            When Breath Becomes Air        Memoir by a young neurosurgeon faced with terminal cancer. A bit about dying, but more about being alive.        Programming:       Site Reliability Engineering: How Google Runs Production Systems        Hope is not a strategy. Build systems to automate ops. Have an error budget instead of aiming 100% uptime. Monitor latency, traffic, errors, saturation. Push actionable alerts. Blameless postmortems.            Cloud Native Monitoring        Be outcome-driven: detect, triage, diagnose, resolve incidents. Metrics are numerical summary and aggregation and thus more efficient to query than logs and traces. Choose metric labels that don\u2019t have infinite cardinality. Prometheus is a metric format and storage. Prometheus storage is not horizontally scalable. Thanos, Cortex, Mimir, M3 are OSS alternatives to Prometheus storage. To scale a metrics platform: reduce cardinality using aggregation, reduce retention period, reduce resolution, reduce sampling rate, reduce query load.            Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems         Everything Curl         Kubernetes: Up and Running: Dive into the Future of Infrastructure         Docker: Up &amp; Running: Shipping Reliable Containers in Production         Terraform: Up &amp; Running: Writing Infrastructure as Code     Books I Could Not Recommend:       The 4-Hour Workweek: Escape 9-5, Live Anywhere, and Join the New Rich        Reading it felt like I just replied to a \u201cget rich quick\u201d email spam. I appreciate the emphasis on time management and delegation, but unfortunatelyMr. Ferris sees his employment no more than doing what was assigned, or merely an exchangefor money with time. I have to look elsewhere for aspiration, leadership, empowerment,relationships, and growth.        "},{id:3,url:"/categories",title:"Categories",body:""},{id:4,url:"/wiki/kubernetes/commands/",title:"Kubernetes Commands",body:"API resources: List all API objects in a namespace:    12kubectl api-resources --verbs=list --namespaced -o name  | xargs -n 1 kubectl get --show-kind --ignore-not-found -n ${NAMESPACE} because kubectl get all does not really give you all the resources. Events: List events in the last hours and sort by time.    1kubectl get events --sort-by='. lastTimestamp' -A Traffic: Port-forwarding: Listen on port 8888 locally, forwarding connection to 5000 on ${POD_NAME}    1kubectl -n ${NAMESPACE} port-forward pod/${POD_NAME} 8888:5000 Note that pod/mypod can also be svc/ or deployment/. This is useful to troubleshoot in-cluster services without exposing them. Port-forwarding works only for TCP traffic at the moment. Pods and Shells: Execute a command in a Pod:    123kubectl exec ${POD_NAME} -- echo hello worldkubectl exec ${POD_NAME} -c ${CONTAINER_NAME} -- curl https://www. example. org By label    123kubectl exec -it  $(kubectl get pod -n ${NAMESPACE} -l key=value --output=jsonpath={. items. . metadata. name})  -n ${NAMESPACE} -c ${CONTAINER_NAME} -- bash Open a shell to a Pod:    1kubectl exec -it ${POD_NAME} -- bash Spin up a debug Pod and open a shell to it:    1kubectl run -it ${POD_NAME} --image=debian --rm --command -- sh This is useful when the application Pod is super stripped down, such as usingdistroless base image that makes troubleshooting difficult due to the lack totooling. Find all unhealthy Pods:    1kubectl get po -A | grep -v Running | grep -v Completed Get Pod name by label:    1kubectl get po -n ${NAMESPACE} -l key=value --output=jsonpath={. items. . metadata. name} Logs: Get last 20 lines of logs:    1kubectl logs --tail=20 ${POD_NAME} Get logs in the last 3 hours:    1kubectl logs --since=3h ${POD_NAME} Get tail logs and stream new logs:    1kubectl logs --tail=20 ${POD_NAME} -f ####Aggregate logs from multiple Pods using label selector    1kubectl logs -l app=server --tail=20 -f Nodes: List all used NodePorts in a cluster:    12$ TEMPLATE='{{range . items}}{{range. spec. ports}}{{if . nodePort}}{{. nodePort}}{{. }}{{ \n }}{{end}}{{end}}{{end}}'$ kubectl get svc --all-namespaces -o go-template= ${TEMPLATE}  List all Nodes by creation time:    1kubectl get node --sort-by=. metadata. creationTimestamp Get Pod Count per Node:    123for n in $(kubectl get nodes --no-headers | cut -d     -f1); do  echo -n  ${n}:  ; kubectl get pods --all-namespaces --no-headers --field-selector spec. nodeName=${n} | wc -ldone Useful kubectl plugins: Namespace-wide rolling restart: Repo: LifeWay/kubectl-roll-plugin kubectl roll -n ${NAMEPSACE} will trigger a rolling restart of all StatefulSets, DaemonSets,and Deployments in a given namespace "},{id:5,url:"/wiki/go/development/",title:"More Effective Go",body:"Testing: Use google/go-cmp instead of reflect. DeepEqual to compareobjects, because cmp gives you detailed diff and allows flexible optionssuch as  IgnoreUnexported(typs . . . interface{}) EquateApprox(fraction, margin float64) SortMaps(lessFunc interface{})List of options: cmpoptsOptions for protobufs: protocmp Examples:    1234567891011import (   testing    github. com/google/go-cmp/cmp )func TestFoo(t *testing. T) {  if diff := cmp. Diff(want, got); diff !=    {    t. Fatalf( unexpected diff (-want +got):\n%s , diff)  }} During development, fmt. Printf( %+v , obj) shows the object in a readable way. Packages: It is an anti-pattern to have command-line flags or panics in external packages. Flags dictate how parameters are passed to the library and hence are not flexible. Unrecovered panics will crash the entire program, not just one goroutine. Use error instead. "},{id:6,url:"/",title:"Home",
body:" 12345678910&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;Popular&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row &gt;      &lt;!-- begin post --&gt;    123456789101112131415161718192021222324252627282930313233343536&lt;div class= listfeaturedtag h-100 &gt;  &lt;div class= row h-100 &gt;    &lt;div class= col-12 col-md-12 col-lg-5 pr-lg-0 &gt;      &lt;div class= h-100 &gt;        &lt;div class= wrapthumbnail &gt;          &lt;a href= /speculative-decoding/ &gt;                        &lt;img class= featured-box-img-cover lazyimg  src= data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=  data-src= /assets/images/speculative-decoding/cover. png &gt;                      &lt;/a&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;    &lt;div class= col-12 col-md-12 col-lg-7 &gt;      &lt;div class= h-100 card-group &gt;        &lt;div class= card &gt;          &lt;div class= card-body &gt;            &lt;h2 class= card-title &gt;              &lt;a class= text-dark  href= /speculative-decoding/ &gt;Accelerate LLM Inference with Speculative Decoding&lt;/a&gt;            &lt;/h2&gt;            &lt;h4 class= card-text &gt;Many inference speedup techniques mirror the classic systems regime\u2014such as caching, paging, tiling, pipelining, and speculative execution (e. g. branch prediction and cache prefetch). Speculative decoding,. . . &lt;/h4&gt;          &lt;/div&gt;          &lt;div class= card-footer b-0 bg-white mt-auto &gt;            &lt;div class= wrapfooter &gt;              &lt;span class= post-date &gt;11 Mar 2025&lt;/span&gt;              &lt;/span&gt;              &lt;!-- &lt;span class= post-read-more &gt;&lt;a href= //speculative-decoding/  title= Read Story &gt;&lt;svg class= svgIcon-use  width= 25  height= 25  viewbox= 0 0 25 25 &gt;&lt;path d= M19 6c0-1. 1-. 9-2-2-2H8c-1. 1 0-2 . 9-2 2v14. 66h. 012c. 01. 103. 045. 204. 12. 285a. 5. 5 0 0 0 . 706. 03L12. 5 16. 85l5. 662 4. 126a. 508. 508 0 0 0 . 708-. 03. 5. 5 0 0 0 . 118-. 285H19V6zm-6. 838 9. 97L7 19. 636V6c0-. 55. 45-1 1-1h9c. 55 0 1 . 45 1 1v13. 637l-5. 162-3. 668a. 49. 49 0 0 0-. 676 0z  fill-rule= evenodd &gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/span&gt; --&gt;              &lt;div class= clearfix &gt;&lt;/div&gt;            &lt;/div&gt;          &lt;/div&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/div&gt;     1    &lt;!-- begin post --&gt;    123456789101112131415161718192021222324252627282930313233343536&lt;div class= listfeaturedtag h-100 &gt;  &lt;div class= row h-100 &gt;    &lt;div class= col-12 col-md-12 col-lg-5 pr-lg-0 &gt;      &lt;div class= h-100 &gt;        &lt;div class= wrapthumbnail &gt;          &lt;a href= /multi-head-attention/ &gt;                        &lt;img class= featured-box-img-cover lazyimg  src= data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=  data-src= /assets/images/multi-head-attention/cover. jpg &gt;                      &lt;/a&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;    &lt;div class= col-12 col-md-12 col-lg-7 &gt;      &lt;div class= h-100 card-group &gt;        &lt;div class= card &gt;          &lt;div class= card-body &gt;            &lt;h2 class= card-title &gt;              &lt;a class= text-dark  href= /multi-head-attention/ &gt;Parallelizing Multi-Head Attention&lt;/a&gt;            &lt;/h2&gt;            &lt;h4 class= card-text &gt;In the multi-head attention mechanism, why after reshaping the projection matrices for Q/K/V from 3 dimensions to 4, we need to transpose the tokens dimension. . . &lt;/h4&gt;          &lt;/div&gt;          &lt;div class= card-footer b-0 bg-white mt-auto &gt;            &lt;div class= wrapfooter &gt;              &lt;span class= post-date &gt;28 Feb 2025&lt;/span&gt;              &lt;/span&gt;              &lt;!-- &lt;span class= post-read-more &gt;&lt;a href= //multi-head-attention/  title= Read Story &gt;&lt;svg class= svgIcon-use  width= 25  height= 25  viewbox= 0 0 25 25 &gt;&lt;path d= M19 6c0-1. 1-. 9-2-2-2H8c-1. 1 0-2 . 9-2 2v14. 66h. 012c. 01. 103. 045. 204. 12. 285a. 5. 5 0 0 0 . 706. 03L12. 5 16. 85l5. 662 4. 126a. 508. 508 0 0 0 . 708-. 03. 5. 5 0 0 0 . 118-. 285H19V6zm-6. 838 9. 97L7 19. 636V6c0-. 55. 45-1 1-1h9c. 55 0 1 . 45 1 1v13. 637l-5. 162-3. 668a. 49. 49 0 0 0-. 676 0z  fill-rule= evenodd &gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/span&gt; --&gt;              &lt;div class= clearfix &gt;&lt;/div&gt;            &lt;/div&gt;          &lt;/div&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/div&gt;     1    &lt;!-- begin post --&gt;    12345678910111213141516171819202122&lt;div class= listfeaturedtag h-100 &gt;  &lt;div class= row h-100 &gt;    &lt;div class= col-12 col-md-12 col-lg-5 pr-lg-0 &gt;      &lt;div class= h-100 &gt;        &lt;div class= wrapthumbnail &gt;          &lt;a href= /sales-lessons/ &gt;                        &lt;img class= featured-box-img-cover lazyimg  src= data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=  data-src= /assets/images/sales-lessons/cover. png &gt;                      &lt;/a&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;    &lt;div class= col-12 col-md-12 col-lg-7 &gt;      &lt;div class= h-100 card-group &gt;        &lt;div class= card &gt;          &lt;div class= card-body &gt;            &lt;h2 class= card-title &gt;              &lt;a class= text-dark  href= /sales-lessons/ &gt;Enterprise Sales&lt;/a&gt;            &lt;/h2&gt;            &lt;h4 class= card-text &gt;How to do product-led growth and hands-on outbound sales at the same time?    &lt;/h4&gt;            &lt;/div&gt;            &lt;div class= card-footer b-0 bg-white mt-auto &gt;              &lt;div class= wrapfooter &gt;                01 Jan 2023                &lt;/span&gt;                                &lt;div class= clearfix &gt;&lt;/div&gt;              &lt;/div&gt;            &lt;/div&gt;          &lt;/div&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;    1    &lt;!-- begin post --&gt;    12345678910111213141516171819202122&lt;div class= listfeaturedtag h-100 &gt;  &lt;div class= row h-100 &gt;    &lt;div class= col-12 col-md-12 col-lg-5 pr-lg-0 &gt;      &lt;div class= h-100 &gt;        &lt;div class= wrapthumbnail &gt;          &lt;a href= /more-advices/ &gt;                        &lt;img class= featured-box-img-cover lazyimg  src= data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=  data-src= /assets/images/more-advices/cover. png &gt;                      &lt;/a&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;    &lt;div class= col-12 col-md-12 col-lg-7 &gt;      &lt;div class= h-100 card-group &gt;        &lt;div class= card &gt;          &lt;div class= card-body &gt;            &lt;h2 class= card-title &gt;              &lt;a class= text-dark  href= /more-advices/ &gt;More Career Advices&lt;/a&gt;            &lt;/h2&gt;            &lt;h4 class= card-text &gt;Make sure to check out the previous post: Advices I wish I got at the start of my career.     &lt;/h4&gt;            &lt;/div&gt;            &lt;div class= card-footer b-0 bg-white mt-auto &gt;              &lt;div class= wrapfooter &gt;                06 Dec 2022                &lt;/span&gt;                                &lt;div class= clearfix &gt;&lt;/div&gt;              &lt;/div&gt;            &lt;/div&gt;          &lt;/div&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;    1    &lt;!-- begin post --&gt;    123456789101112131415161718192021222324252627282930313233343536&lt;div class= listfeaturedtag h-100 &gt;  &lt;div class= row h-100 &gt;    &lt;div class= col-12 col-md-12 col-lg-5 pr-lg-0 &gt;      &lt;div class= h-100 &gt;        &lt;div class= wrapthumbnail &gt;          &lt;a href= /streamlit-interview/ &gt;                        &lt;img class= featured-box-img-cover lazyimg  src= data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=  data-src= /assets/images/streamlit-interview/cover. jpeg &gt;                      &lt;/a&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;    &lt;div class= col-12 col-md-12 col-lg-7 &gt;      &lt;div class= h-100 card-group &gt;        &lt;div class= card &gt;          &lt;div class= card-body &gt;            &lt;h2 class= card-title &gt;              &lt;a class= text-dark  href= /streamlit-interview/ &gt;Interviewing Adrien Treuille, Founder CEO of Streamlit&lt;/a&gt;            &lt;/h2&gt;            &lt;h4 class= card-text &gt;Streamlit, about to raise its Series-C, was acquired by Snowflake for $800M in March 2022. In this conversation with Adrien, we chatted about OSS metrics,. . . &lt;/h4&gt;          &lt;/div&gt;          &lt;div class= card-footer b-0 bg-white mt-auto &gt;            &lt;div class= wrapfooter &gt;              &lt;span class= post-date &gt;21 Nov 2022&lt;/span&gt;              &lt;/span&gt;              &lt;!-- &lt;span class= post-read-more &gt;&lt;a href= //streamlit-interview/  title= Read Story &gt;&lt;svg class= svgIcon-use  width= 25  height= 25  viewbox= 0 0 25 25 &gt;&lt;path d= M19 6c0-1. 1-. 9-2-2-2H8c-1. 1 0-2 . 9-2 2v14. 66h. 012c. 01. 103. 045. 204. 12. 285a. 5. 5 0 0 0 . 706. 03L12. 5 16. 85l5. 662 4. 126a. 508. 508 0 0 0 . 708-. 03. 5. 5 0 0 0 . 118-. 285H19V6zm-6. 838 9. 97L7 19. 636V6c0-. 55. 45-1 1-1h9c. 55 0 1 . 45 1 1v13. 637l-5. 162-3. 668a. 49. 49 0 0 0-. 676 0z  fill-rule= evenodd &gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/span&gt; --&gt;              &lt;div class= clearfix &gt;&lt;/div&gt;            &lt;/div&gt;          &lt;/div&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/div&gt;     1    &lt;!-- begin post --&gt;    123456789101112131415161718192021222324252627282930313233343536&lt;div class= listfeaturedtag h-100 &gt;  &lt;div class= row h-100 &gt;    &lt;div class= col-12 col-md-12 col-lg-5 pr-lg-0 &gt;      &lt;div class= h-100 &gt;        &lt;div class= wrapthumbnail &gt;          &lt;a href= /accounting/ &gt;                        &lt;img class= featured-box-img-cover lazyimg  src= data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=  data-src= /assets/images/accounting/cover. jpeg &gt;                      &lt;/a&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;    &lt;div class= col-12 col-md-12 col-lg-7 &gt;      &lt;div class= h-100 card-group &gt;        &lt;div class= card &gt;          &lt;div class= card-body &gt;            &lt;h2 class= card-title &gt;              &lt;a class= text-dark  href= /accounting/ &gt;Accounting Advice for Founders&lt;/a&gt;            &lt;/h2&gt;            &lt;h4 class= card-text &gt;Notes derived from a guest lecture by Danny Wallace, Partner at PwC\u2019s Silicon Valley practice. For informational purposes only. Errors and omissions are my own. . . . &lt;/h4&gt;          &lt;/div&gt;          &lt;div class= card-footer b-0 bg-white mt-auto &gt;            &lt;div class= wrapfooter &gt;              &lt;span class= post-date &gt;05 Aug 2021&lt;/span&gt;              &lt;/span&gt;              &lt;!-- &lt;span class= post-read-more &gt;&lt;a href= //accounting/  title= Read Story &gt;&lt;svg class= svgIcon-use  width= 25  height= 25  viewbox= 0 0 25 25 &gt;&lt;path d= M19 6c0-1. 1-. 9-2-2-2H8c-1. 1 0-2 . 9-2 2v14. 66h. 012c. 01. 103. 045. 204. 12. 285a. 5. 5 0 0 0 . 706. 03L12. 5 16. 85l5. 662 4. 126a. 508. 508 0 0 0 . 708-. 03. 5. 5 0 0 0 . 118-. 285H19V6zm-6. 838 9. 97L7 19. 636V6c0-. 55. 45-1 1-1h9c. 55 0 1 . 45 1 1v13. 637l-5. 162-3. 668a. 49. 49 0 0 0-. 676 0z  fill-rule= evenodd &gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/span&gt; --&gt;              &lt;div class= clearfix &gt;&lt;/div&gt;            &lt;/div&gt;          &lt;/div&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/div&gt;     1    &lt;!-- begin post --&gt;    123456789101112131415161718192021222324252627282930313233343536&lt;div class= listfeaturedtag h-100 &gt;  &lt;div class= row h-100 &gt;    &lt;div class= col-12 col-md-12 col-lg-5 pr-lg-0 &gt;      &lt;div class= h-100 &gt;        &lt;div class= wrapthumbnail &gt;          &lt;a href= /ip-law/ &gt;                        &lt;img class= featured-box-img-cover lazyimg  src= data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=  data-src= /assets/images/ip-law/cover. jpeg &gt;                      &lt;/a&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;    &lt;div class= col-12 col-md-12 col-lg-7 &gt;      &lt;div class= h-100 card-group &gt;        &lt;div class= card &gt;          &lt;div class= card-body &gt;            &lt;h2 class= card-title &gt;              &lt;a class= text-dark  href= /ip-law/ &gt;Intellectual Property and Entrepreneurship&lt;/a&gt;            &lt;/h2&gt;            &lt;h4 class= card-text &gt;Notes on Intellectual Property (IP) law for founders and busy professionals. Not legal advice. For informational purposes only. Laws can change, so this article may. . . &lt;/h4&gt;          &lt;/div&gt;          &lt;div class= card-footer b-0 bg-white mt-auto &gt;            &lt;div class= wrapfooter &gt;              &lt;span class= post-date &gt;06 May 2021&lt;/span&gt;              &lt;/span&gt;              &lt;!-- &lt;span class= post-read-more &gt;&lt;a href= //ip-law/  title= Read Story &gt;&lt;svg class= svgIcon-use  width= 25  height= 25  viewbox= 0 0 25 25 &gt;&lt;path d= M19 6c0-1. 1-. 9-2-2-2H8c-1. 1 0-2 . 9-2 2v14. 66h. 012c. 01. 103. 045. 204. 12. 285a. 5. 5 0 0 0 . 706. 03L12. 5 16. 85l5. 662 4. 126a. 508. 508 0 0 0 . 708-. 03. 5. 5 0 0 0 . 118-. 285H19V6zm-6. 838 9. 97L7 19. 636V6c0-. 55. 45-1 1-1h9c. 55 0 1 . 45 1 1v13. 637l-5. 162-3. 668a. 49. 49 0 0 0-. 676 0z  fill-rule= evenodd &gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/span&gt; --&gt;              &lt;div class= clearfix &gt;&lt;/div&gt;            &lt;/div&gt;          &lt;/div&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/div&gt;     1    &lt;!-- begin post --&gt;    12345678910111213141516171819202122&lt;div class= listfeaturedtag h-100 &gt;  &lt;div class= row h-100 &gt;    &lt;div class= col-12 col-md-12 col-lg-5 pr-lg-0 &gt;      &lt;div class= h-100 &gt;        &lt;div class= wrapthumbnail &gt;          &lt;a href= /promo/ &gt;                        &lt;img class= featured-box-img-cover lazyimg  src= data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=  data-src= /assets/images/promo/cover. jpg &gt;                      &lt;/a&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;    &lt;div class= col-12 col-md-12 col-lg-7 &gt;      &lt;div class= h-100 card-group &gt;        &lt;div class= card &gt;          &lt;div class= card-body &gt;            &lt;h2 class= card-title &gt;              &lt;a class= text-dark  href= /promo/ &gt;Software Engineering Levels and Promotion&lt;/a&gt;            &lt;/h2&gt;            &lt;h4 class= card-text &gt;This post explains the expectation of each engineering level in the most concise and company-agnostic way and reveals the steps towards promotion.     &lt;/h4&gt;            &lt;/div&gt;            &lt;div class= card-footer b-0 bg-white mt-auto &gt;              &lt;div class= wrapfooter &gt;                31 Aug 2020                &lt;/span&gt;                                &lt;div class= clearfix &gt;&lt;/div&gt;              &lt;/div&gt;            &lt;/div&gt;          &lt;/div&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;    1    &lt;!-- begin post --&gt;    123456789101112131415161718192021222324252627282930313233343536&lt;div class= listfeaturedtag h-100 &gt;  &lt;div class= row h-100 &gt;    &lt;div class= col-12 col-md-12 col-lg-5 pr-lg-0 &gt;      &lt;div class= h-100 &gt;        &lt;div class= wrapthumbnail &gt;          &lt;a href= /one-on-one/ &gt;                        &lt;img class= featured-box-img-cover lazyimg  src= data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=  data-src= /assets/images/one-on-one/cover-border. jpg &gt;                      &lt;/a&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;    &lt;div class= col-12 col-md-12 col-lg-7 &gt;      &lt;div class= h-100 card-group &gt;        &lt;div class= card &gt;          &lt;div class= card-body &gt;            &lt;h2 class= card-title &gt;              &lt;a class= text-dark  href= /one-on-one/ &gt;What to Talk about in Effective 1-on-1s&lt;/a&gt;            &lt;/h2&gt;            &lt;h4 class= card-text &gt;Unlike in school when we get grades on every assignment and in every course, we get less frequent feedback in professional life, usually once or. . . &lt;/h4&gt;          &lt;/div&gt;          &lt;div class= card-footer b-0 bg-white mt-auto &gt;            &lt;div class= wrapfooter &gt;              &lt;span class= post-date &gt;21 Jun 2020&lt;/span&gt;              &lt;/span&gt;              &lt;!-- &lt;span class= post-read-more &gt;&lt;a href= //one-on-one/  title= Read Story &gt;&lt;svg class= svgIcon-use  width= 25  height= 25  viewbox= 0 0 25 25 &gt;&lt;path d= M19 6c0-1. 1-. 9-2-2-2H8c-1. 1 0-2 . 9-2 2v14. 66h. 012c. 01. 103. 045. 204. 12. 285a. 5. 5 0 0 0 . 706. 03L12. 5 16. 85l5. 662 4. 126a. 508. 508 0 0 0 . 708-. 03. 5. 5 0 0 0 . 118-. 285H19V6zm-6. 838 9. 97L7 19. 636V6c0-. 55. 45-1 1-1h9c. 55 0 1 . 45 1 1v13. 637l-5. 162-3. 668a. 49. 49 0 0 0-. 676 0z  fill-rule= evenodd &gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/span&gt; --&gt;              &lt;div class= clearfix &gt;&lt;/div&gt;            &lt;/div&gt;          &lt;/div&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/div&gt;     1    &lt;!-- begin post --&gt;    123456789101112131415161718192021222324252627282930313233343536&lt;div class= listfeaturedtag h-100 &gt;  &lt;div class= row h-100 &gt;    &lt;div class= col-12 col-md-12 col-lg-5 pr-lg-0 &gt;      &lt;div class= h-100 &gt;        &lt;div class= wrapthumbnail &gt;          &lt;a href= /advices/ &gt;                        &lt;img class= featured-box-img-cover lazyimg  src= data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=  data-src= /assets/images/advices/cover. jpeg &gt;                      &lt;/a&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;    &lt;div class= col-12 col-md-12 col-lg-7 &gt;      &lt;div class= h-100 card-group &gt;        &lt;div class= card &gt;          &lt;div class= card-body &gt;            &lt;h2 class= card-title &gt;              &lt;a class= text-dark  href= /advices/ &gt;Advices I wish I got at the start of my career&lt;/a&gt;            &lt;/h2&gt;            &lt;h4 class= card-text &gt;When I was a kid playing chess with my dad, he sometimes would offer me hints on some good moves. I would never make those. . . &lt;/h4&gt;          &lt;/div&gt;          &lt;div class= card-footer b-0 bg-white mt-auto &gt;            &lt;div class= wrapfooter &gt;              &lt;span class= post-date &gt;03 Nov 2019&lt;/span&gt;              &lt;/span&gt;              &lt;!-- &lt;span class= post-read-more &gt;&lt;a href= //advices/  title= Read Story &gt;&lt;svg class= svgIcon-use  width= 25  height= 25  viewbox= 0 0 25 25 &gt;&lt;path d= M19 6c0-1. 1-. 9-2-2-2H8c-1. 1 0-2 . 9-2 2v14. 66h. 012c. 01. 103. 045. 204. 12. 285a. 5. 5 0 0 0 . 706. 03L12. 5 16. 85l5. 662 4. 126a. 508. 508 0 0 0 . 708-. 03. 5. 5 0 0 0 . 118-. 285H19V6zm-6. 838 9. 97L7 19. 636V6c0-. 55. 45-1 1-1h9c. 55 0 1 . 45 1 1v13. 637l-5. 162-3. 668a. 49. 49 0 0 0-. 676 0z  fill-rule= evenodd &gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/span&gt; --&gt;              &lt;div class= clearfix &gt;&lt;/div&gt;            &lt;/div&gt;          &lt;/div&gt;        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/div&gt;     1&lt;/div&gt;  1234567891011&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;New Posts&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row listrecent &gt;    &lt;!-- begin post --&gt;    123456789101112131415161718192021222324252627&lt;div class= card h-100 &gt;  &lt;div class= maxthumb &gt;    &lt;a href= /lean-startup/ &gt;                        &lt;img class= img-fluid lazyimg  src= data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=  data-src= /assets/images/lean-startup/cover. jpeg  alt= Notes: The Lean Startup &gt;                  &lt;/a&gt;  &lt;/div&gt;  &lt;div class= card-body &gt;    &lt;h2 class= card-title &gt;      &lt;a class= text-dark  href= /lean-startup/ &gt;Notes: The Lean Startup&lt;/a&gt;    &lt;/h2&gt;    &lt;h4 class= card-text &gt;Careful planning and execution work for general management but not for startups. Perfect execution is futile if you end up building something nobody wants (waste). The real progress for startups. . . &lt;/h4&gt;  &lt;/div&gt;  &lt;div class= card-footer bg-white &gt;    &lt;div class= wrapfooter &gt;      &lt;span class= post-date &gt;21 Jan 2024&lt;/span&gt;      &lt;/span&gt;      &lt;!-- &lt;span class= post-read-more &gt;&lt;a href= /lean-startup/  title= Read Story &gt;&lt;svg class= svgIcon-use  width= 25  height= 25  viewbox= 0 0 25 25 &gt;&lt;path d= M19 6c0-1. 1-. 9-2-2-2H8c-1. 1 0-2 . 9-2 2v14. 66h. 012c. 01. 103. 045. 204. 12. 285a. 5. 5 0 0 0 . 706. 03L12. 5 16. 85l5. 662 4. 126a. 508. 508 0 0 0 . 708-. 03. 5. 5 0 0 0 . 118-. 285H19V6zm-6. 838 9. 97L7 19. 636V6c0-. 55. 45-1 1-1h9c. 55 0 1 . 45 1 1v13. 637l-5. 162-3. 668a. 49. 49 0 0 0-. 676 0z  fill-rule= evenodd &gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/span&gt; --&gt;      &lt;div class= clearfix &gt;&lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/div&gt;     1  &lt;!-- begin post --&gt;    123456789101112131415161718192021222324252627&lt;div class= card h-100 &gt;  &lt;div class= maxthumb &gt;    &lt;a href= /venture-deals/ &gt;                        &lt;img class= img-fluid lazyimg  src= data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=  data-src= /assets/images/venture-deals/cover. jpg  alt= Notes: Venture Deals &gt;                  &lt;/a&gt;  &lt;/div&gt;  &lt;div class= card-body &gt;    &lt;h2 class= card-title &gt;      &lt;a class= text-dark  href= /venture-deals/ &gt;Notes: Venture Deals&lt;/a&gt;    &lt;/h2&gt;    &lt;h4 class= card-text &gt;Before Fundraise: Allow minimum three to six months to raise money. Have a clean cut from last job to avoid IP disputes. Prepare data site (Certificate of Incorporation, Bylaws, board. . . &lt;/h4&gt;  &lt;/div&gt;  &lt;div class= card-footer bg-white &gt;    &lt;div class= wrapfooter &gt;      &lt;span class= post-date &gt;19 Jan 2024&lt;/span&gt;      &lt;/span&gt;      &lt;!-- &lt;span class= post-read-more &gt;&lt;a href= /venture-deals/  title= Read Story &gt;&lt;svg class= svgIcon-use  width= 25  height= 25  viewbox= 0 0 25 25 &gt;&lt;path d= M19 6c0-1. 1-. 9-2-2-2H8c-1. 1 0-2 . 9-2 2v14. 66h. 012c. 01. 103. 045. 204. 12. 285a. 5. 5 0 0 0 . 706. 03L12. 5 16. 85l5. 662 4. 126a. 508. 508 0 0 0 . 708-. 03. 5. 5 0 0 0 . 118-. 285H19V6zm-6. 838 9. 97L7 19. 636V6c0-. 55. 45-1 1-1h9c. 55 0 1 . 45 1 1v13. 637l-5. 162-3. 668a. 49. 49 0 0 0-. 676 0z  fill-rule= evenodd &gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/span&gt; --&gt;      &lt;div class= clearfix &gt;&lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/div&gt;     1  &lt;!-- begin post --&gt;    1234567891011121314151617&lt;div class= card h-100 &gt;  &lt;div class= maxthumb &gt;    &lt;a href= /scaling-istio/ &gt;                        &lt;img class= img-fluid lazyimg  src= data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=  data-src= /assets/images/scaling-istio/cover. jpg  alt= Scaling Istio &gt;                  &lt;/a&gt;  &lt;/div&gt;  &lt;div class= card-body &gt;    &lt;h2 class= card-title &gt;      &lt;a class= text-dark  href= /scaling-istio/ &gt;Scaling Istio&lt;/a&gt;    &lt;/h2&gt;    &lt;h4 class= card-text &gt;In a large, busy cluster, how do you scale Istio to address Istio-proxy Container being OOM-Killed and Istiod crashes if too many connected istio-proxies?    &lt;/h4&gt;    &lt;/div&gt;    &lt;div class= card-footer bg-white &gt;      &lt;div class= wrapfooter &gt;        22 Oct 2023        &lt;/span&gt;                &lt;div class= clearfix &gt;&lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;    1  &lt;!-- begin post --&gt;    1234567891011121314151617&lt;div class= card h-100 &gt;  &lt;div class= maxthumb &gt;    &lt;a href= /eks-sg/ &gt;                        &lt;img class= img-fluid lazyimg  src= data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=  data-src= /assets/images/eks-sg/cover3. jpeg  alt= Work Around Max Count of Security Group Rules on EKS &gt;                  &lt;/a&gt;  &lt;/div&gt;  &lt;div class= card-body &gt;    &lt;h2 class= card-title &gt;      &lt;a class= text-dark  href= /eks-sg/ &gt;Work Around Max Count of Security Group Rules on EKS&lt;/a&gt;    &lt;/h2&gt;    &lt;h4 class= card-text &gt;AWS EKS on VPC networks need AWS Security Group Rules (SG) to receipt ingress traffic. But what if you reach the max rules count in your SG?    &lt;/h4&gt;    &lt;/div&gt;    &lt;div class= card-footer bg-white &gt;      &lt;div class= wrapfooter &gt;        26 Sep 2023        &lt;/span&gt;                &lt;div class= clearfix &gt;&lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;    1  &lt;!-- begin post --&gt;    123456789101112131415161718192021222324252627&lt;div class= card h-100 &gt;  &lt;div class= maxthumb &gt;    &lt;a href= /source-ip-autoscale/ &gt;                        &lt;img class= img-fluid lazyimg  src= data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=  data-src= /assets/images/source-ip-autoscale/cover2. jpeg  alt= Layer-4 Load Balancer &amp; Zero-downtime Autoscaling and Upgrade &gt;                  &lt;/a&gt;  &lt;/div&gt;  &lt;div class= card-body &gt;    &lt;h2 class= card-title &gt;      &lt;a class= text-dark  href= /source-ip-autoscale/ &gt;Layer-4 Load Balancer &amp; Zero-downtime Autoscaling and Upgrade&lt;/a&gt;    &lt;/h2&gt;    &lt;h4 class= card-text &gt;Your Kubernetes cluster probably has a shared ingress for north-south traffic, coming from a cloud load balancer and lands on your favorite proxies like Envoy, or Istio gateways, or Nginx. . . . &lt;/h4&gt;  &lt;/div&gt;  &lt;div class= card-footer bg-white &gt;    &lt;div class= wrapfooter &gt;      &lt;span class= post-date &gt;06 Aug 2023&lt;/span&gt;      &lt;/span&gt;      &lt;!-- &lt;span class= post-read-more &gt;&lt;a href= /source-ip-autoscale/  title= Read Story &gt;&lt;svg class= svgIcon-use  width= 25  height= 25  viewbox= 0 0 25 25 &gt;&lt;path d= M19 6c0-1. 1-. 9-2-2-2H8c-1. 1 0-2 . 9-2 2v14. 66h. 012c. 01. 103. 045. 204. 12. 285a. 5. 5 0 0 0 . 706. 03L12. 5 16. 85l5. 662 4. 126a. 508. 508 0 0 0 . 708-. 03. 5. 5 0 0 0 . 118-. 285H19V6zm-6. 838 9. 97L7 19. 636V6c0-. 55. 45-1 1-1h9c. 55 0 1 . 45 1 1v13. 637l-5. 162-3. 668a. 49. 49 0 0 0-. 676 0z  fill-rule= evenodd &gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/span&gt; --&gt;      &lt;div class= clearfix &gt;&lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/div&gt;     1  &lt;!-- begin post --&gt;    1234567891011121314151617&lt;div class= card h-100 &gt;  &lt;div class= maxthumb &gt;    &lt;a href= /k8s-net/ &gt;                        &lt;img class= img-fluid lazyimg  src= data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=  data-src= /assets/images/k8s-net/cover. jpeg  alt= Kubernetes Networking From the First Principles &gt;                  &lt;/a&gt;  &lt;/div&gt;  &lt;div class= card-body &gt;    &lt;h2 class= card-title &gt;      &lt;a class= text-dark  href= /k8s-net/ &gt;Kubernetes Networking From the First Principles&lt;/a&gt;    &lt;/h2&gt;    &lt;h4 class= card-text &gt;We go from containers and network namespace to Pod-to-Pod, Pod-to-Service, and external-client-to-Service networking.     &lt;/h4&gt;    &lt;/div&gt;    &lt;div class= card-footer bg-white &gt;      &lt;div class= wrapfooter &gt;        01 Mar 2022        &lt;/span&gt;                &lt;div class= clearfix &gt;&lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;    1&lt;/div&gt;    &lt;/div&gt;                               1 (current)        2        3      &gt;     &raquo;       &lt;/span&gt;  "},{id:7,url:"/wiki/",title:"About This Wiki",body:"This is a knowledge base of tools, best practices, and productivity. Whereasblogs are my opinions and stories, wiki pages are living documents of indexedfacts for fast retrieval. It is unfortunately biased towards tech and startups, but I do not aspire toreplace wikipedia either. In spirit of open-source and learning in the public, Ihope you find answers here. This is work in progress. Let me know if you spot any error (contact info). Menu is on the left, or at the top for mobile. "},{id:8,url:"/inspirations",title:"Inspirations",body:"    These are some inspiring people, newsletters, and websites that I regularlylearn from.           Startups, Products, SaaS, Funding:       Paul Graham        Co-founder of Y Combinator and HackerNews.            Sam Altman        CEO of OpenAI. Former president of Y Combinator.            Lenny Rachitsky        Former Growth Product Manager at Airbnb. Weekly newsletter on product, growth, people management.            Harry Stabbings - 20VC        Podcast interviewing veterans in sales, growth, marketing, product,investment memo, and culture.            Acquired LP        Podcast on company building (pricing, hiring and strategy), VC basics,interviews with GPs.            Alex Clayton        General Partner at Meritech Capital. Primarily investing in enterprise software and infrastructure. Gives insightful S-1 breakdowns.            Leo Polovets        A software engineer turned VC. General Partner at Susa Ventures. Generous adviceon seed-stage logistics, pitching, and fundraising            Jos\xe9 Ancer - Silicon Hills Lawyer        A startup lawyer and Harvard JD who helps founders safely navigate high-stakeslegal and financing issues, while keeping VCs honest.            Andrew Chen        General Partner at Andreessen Horowitz. OG Rider Growth team at Uber. Prolific writer on consumer tech, including mobile, metrics, and user growth.          Career Development:       Avery Pennarun         Will Larson         StaffEng         Julie Zhuo         Cedric Chin     Productivity:       TypeTab     "},{id:9,url:"/wiki/istio/",title:"Istio",body:"Turn on sidecar proxy debug logging:    12kubectl exec ${POD_NAME} --   curl -X POST 'http://localhost:15000/logging?level=debug' Download istioctl at specific version:    1curl -sL https://istio. io/downloadIstioctl | ISTIO_VERSION=1. 9. 1 sh - Addressing long-running proxy in short-lived Pods: See The Good, Bad, and Ugly: Istio for Short-lived Pods. "},{id:10,url:"/wiki/networking/",title:"Networking",
body:"OpenSSL: Verify a certificate using openssl: 123$ kubectl get secret my-cert -n istio-system   -o 'go-template={% raw %}{{index . data  tls. crt }}{% endraw %}'   | base64 -d | openssl x509 -text -nooutReview TLS certificate chain given a hostname. : 1openssl x509 -text -noout -in &lt;(openssl s_client -connect google. com:443 -servername google. com)Curl: Force domain name resolution: 123DOMAIN=example. comLB_IP=10. 139. 0. 123curl -H  HOST: ${DOMAIN}  https://${DOMAIN} --resolve ${DOMAIN}:443:${LB_IP}This is useful in blue-green upgrades before cutting over DNS. For example, we can use this command to talk to the load balancer upstreambackends without DNS. Robust Curl:  Use -L to follow 301/302 redirects Use --fail to exit with non-zero code given 4xx and 5xx HTTP response.      By default, Curl does not consider 4xx and 5xx failure, since the HTTPrequest completed, but for application use cases, they almost certainly arehandled as errors    Use --retry &lt;count&gt; to retry request upon transient errors. Combined with --fail, --retry will also retry HTTP 4xx. Transient error means:     timeout   FTP 4xx response code   HTTP 5xx response code    Optional. Use --retry-delay 3 turns off exponential backoff to always wait 3 seconds before retrying.  Use --show-error to print any error message even in silent mode.  Use -v to turn on verbose logging. Example 1234567891011CURL_OPTS=(  -L  -v  --retry 5  --retry-delay 5  --fail  --show-error)curl  ${CURL_OPTS[@]}    -H  Authorization: token ${MY_TOKEN}    https://api. github. com/user/reposSilence the progress bar: 1curl -s https://example. com/big. file -o output. file"},{id:11,url:"/wiki/kubernetes/patterns/",title:"Kubernetes Patterns",body:"Run-once DaemonSet Pattern: Some Kubernetes providers, such as GKE and AKS, do not support custom nodeimages. The run-once DaemonSet pattern is useful to prime worker nodes forsecurity hardening or image prefetch. Regular DaemonSet does not work since theyrestart the Pod when the setup task is done. We want the DaemonSet to run once. We can do so by running the setup script in the init container, and run thepause container afterwards, which consumes little resources. Below is an example of run-once DaemonSet to implement the GKE CIS benchmarkrecommendation of enalbing --protect-kernel-defaults on the kubelet. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667apiVersion: apps/v1kind: DaemonSetmetadata: name: cis-protect-kernel-defaults namespace: kube-system labels:  app: protect-kernel-defaultsspec: selector:  matchLabels:   name: protect-kernel-defaults template:  metadata:   labels:    name: protect-kernel-defaults  spec:   hostNetwork: true   hostPID: true   hostIPC: true   volumes:    - name: root-mount     hostPath:      path: /   initContainers:    - image: alpine:3. 18     name: protect-kernel-defaults     command: [ /bin/sh ,  -c ]     # The advantage of args over mounting a configmap is that     # when the script is updated, kubernetes will rolling     # update Pods.      # By using chroot, you can run commands as if you were     # executing them directly on the node, not just inside     # a container.      args:      - |       ROOT_MOUNT_DIR= /node_root        KUBELET_CONFIG_FILE= /home/kubernetes/kubelet-config. yaml        WANT_SETTING= protectKernelDefaults: true        echo  Current kubelet config:        cat  ${ROOT_MOUNT_DIR}/${KUBELET_CONFIG_FILE}        # This check ensures `protectKernelDefaults' is defined once.        # It is invalid YAML to have duplicated keys.        found=$(grep  ${WANT_SETTING}   ${ROOT_MOUNT_DIR}/${KUBELET_CONFIG_FILE} )       if [[ ! -z  ${found}  ]]; then        echo  protectKernelDefaults is already set to true. Done.          exit 0       fi       echo  Updating kubelet config file.         echo  ${WANT_SETTING}  &gt;&gt;  ${ROOT_MOUNT_DIR}/${KUBELET_CONFIG_FILE}        echo  Restarting kubelet.         chroot  ${ROOT_MOUNT_DIR}  systemctl restart kubelet       echo  Done.       securityContext:      privileged: true     volumeMounts:      - name: root-mount       mountPath: /node_root   containers:    - name: pause     image: gcr. io/google_containers/pause:3. 2Best practices: See How to Configure Applications for High Availability in Kubernetes "},{id:12,url:"/wiki/go/pitfalls/",title:"Go Common Pitfalls",body:"Memory Model: Go is not sequentially consistent. 12345678910111213func main() {  var s string  var done bool  go func() {    s =  hello     done = true  }()  for !done {}  // Behavior undefined, possible to print   .   fmt. Println(s)}If you need ordering constraint, establish happens-before relationship as oneof the following:  If p imports q, q\u2019s init happens before p\u2019s.  Package main\u2019s init happens before main. main The go statement happens before the execution of the created goroutine A send (or close) on a channel happens before the receive Unlock happens before subsequent LockDeferred Calls: log. Fatal or os. Exit does not respect deferred calls whereas t. Fatal does,where t is a testing. T. In-line deferred statement is evaluated at the time of calling defer. 123456789101112131415func main() {  s :=  old   defer fmt. Println( defer inline , s)  defer func() {    fmt. Println( defer func , s)  }()  s =  new   fmt. Println(s)}/*OUTPUT:newdefer func newdefer inline old*/Deferred calls are executed after return: 12345678910func str() (s string) {  defer func() {    s =  prefix-  + s  }()  return  hello }func main() {  fmt. Println(str()) // prints  prefix-hello }Defer within loops hold resources for too long: 12345678910func main() {  var someChannel chan string  for filepath := range someChannel {    f, err := os. Open(filepath)    if err != nil {      log. Fatal(err)    }    defer f. Close()  }}Do this instead. 123456789101112func main() {  var someChannel chan string  for filepath := range someChannel {    func(filepath string) {      f, err := os. Open(filepath)      if err != nil {        log. Fatal(err)      }      defer f. Close()    }(filepath)  }}Defer cleanup as soon as possible: 12345678910111213141516func CopyFile(dstName, srcName string) error {  src, err := os. Open(srcName)  if err != nil {    return err  }  dst, err := os. Create(dstName)  if err != nil {    return err  }  _, err = io. Copy(dst, src)  dst. Close()  src. Close()  return err}The above code will not close src if os. Create(dstName) failed. Instead ofadding src. Close() before every return, use defer to cleanup as soon asyou can. Similarly, defer unlock as soon as you can. 12345678910111213141516func CopyFile(dstName, srcName string) error {  src, err := os. Open(srcName)  if err != nil {    return  }  defer src. Close()  dst, err := os. Create(dstName)  if err != nil {    return  }  defer dst. Close()  _, err = io. Copy(dst, src)  return err}Array and Slice: Array is a value type. Slice is a reference type. Be careful when you pass themto other functions. Internally, slice is defined as the following 12345type slice struct {  array unsafe. Pointer  len  int  cap  int}To duplicate a slice 123456s := []string{ hello ,  world }s2 := make([]string, len(s))copy(s2, s)// or thiss2 = append([]string{}, s. . . )Be careful with make: 12345678func main() {  // func make([]T, len, cap) []T, where cap is optional.   s := make([]int, 3)  s = append(s, 1)  s = append(s, 2)  s = append(s, 3)  fmt. Println(s) // [0 0 0 1 2 3]}Slicing retains the entire underlying array: 1234567891011121314func main() {  prefixCache := make(map[string][]byte)  for filepath := range someChannel {    bytes, err := ioutil. ReadFile(filepath)    if err != nil {      log. Fatal(err)    }    // This will not release the underlying array of `bytes`.     prefixCache[filepath] = bytes[:8]    // Instead, let's create a smaller copy.     prefixCache[filepath] = append([]byte{}, bytes[:8]. . . )  }}Interface Holding Nil Is Not Nil: 1234567func main() {  var a, b interface{}  fmt. Println(a == nil) // true  var p *int = nil  b = p  fmt. Println(b == nil) // false}Internally, Go interface is represented as 1234567type iface struct {  // tab holds the type of the interface and  // the type of the `data`.   tab *itab  // data points to the value held by the interface.   data unsafe. Pointer}In Go, an interface is nil only if both its type and value are nil. In the example above, b = (*int)(nil) means b\u2019s type is not nil. This behavior surprises people the most in error, which is an interface. See Why is my nil error value not equal to nil?. 1234567func returnsError() error {  var p *MyError = nil  if bad() {    p = ErrBad  }  return p // Always return a non-nil error. }Return an explicit nil instead. 123456func returnsError() error {\tif bad() {\t\treturn ErrBad\t}\treturn nil}Goroutine: Wait group: When main goroutine exits, everything dies. 1234func main() {  go println( hello )}// May not print hello at all. Instead, use WaitGroup as barrier. 12345678910111213func main() {  var wg sync. WaitGroup  // Important that Add happens before starting goroutine.   wg. Add(1)  go func() {    defer wg. Done()    println( hello )  }()  wg. Wait()}Scheduling and preemption: Go version 1. 14 introduced asynchronouspreemption, so that loops withoutfunction calls no longer potentially deadlock the scheduler or significantlydelay garbage collection. Previously, goroutines are only context switched when  blocked on syscalls, channels, locks, or sleep the goroutine has to grow its stack calling runtime. Gosched() directlyHence, a goroutine calling for {} will occupy the processor forever. "},{id:13,url:"/wiki/kubernetes/potholes/",title:"Kubernetes Potholes",body:"Webhook Blocking Webhook Deployment Itself: If your MutatingWebhookConfiguration has failurePolicy: Fail, then make sureto configure the webhook to ignore the webhook deployment itself, such as usinga namespace selector. I hit this issue with vault-agent-injector. The webhook \u201cvault. hashicorp. com\u201d is backed by vault-agent-injector-svc, whichrequires deployment/vault-agent-injector to be up to serve the webhook. Whenvault-agent-injector has 0 replica, the \u201cvault. hashicorp. com\u201d webhook preventsvault-agent-injector itself from scaling up despite HPA min=2. Because vault-agent-injector is down and the webhook is down, GKE netddaemonset pods cannot run on new nodes to configure CNI on these nodes. Newnodes are all NotReady, because runtime network not ready: NetworkReady=falsereason:NetworkPluginNotReady message:docker: network plugin is not ready: cniconfig uninitialized. This will cause any new nodes created as part of cluster autoscaling to beunhealthy, and will risk cascading failure of the cluster. 123456789101112131415161718192021222324\u276f kubectl get deploy -n vaultNAME          READY  UP-TO-DATE  AVAILABLE  AGEvault-agent-injector  0/2   0      0      193d\u276f kubectl describe deploy -n vaultName:          vault-agent-injectorNamespace:       vault. . . OldReplicaSets:  &lt;none&gt;NewReplicaSet:   vault-agent-injector-69988955f5 (0/2 replicas created)\u276f kubectl describe rs vault-agent-injector-69988955f5 -n vault. . . Events: Type   Reason    Age          From          Message ----   ------    ----         ----          ------- Warning FailedCreate 15m (x41 over 5h49m) replicaset-controller Error creating: Internal error occurred: failed calling webhook  vault. hashicorp. com : Post  https://vault-agent-injector-svc. vault. svc:443/mutate?timeout=10s : no endpoints available for service  vault-agent-injector-svc \u276f kubectl describe ds netd -n kube-system. . . Events: Type   Reason    Age           From         Message ----   ------    ----          ----         ------- Warning FailedCreate 7m18s (x42 over 4h39m) daemonset-controller Error creating: Internal error occurred: failed calling webhook  vault. hashicorp. com : Post  https://vault-agent-injector-svc. vault. svc:443/mutate?timeout=10s : no endpoints available for service  vault-agent-injector-svc "},{id:14,url:"/wiki/go/production/",title:"Production-ready Go",body:"Logging: Abort through log. Fatal instead of os. Exit to allow logger to flush bufferedlogs. Use structured JSON logging in production. I recommend uber-go/zap orrs/zerolog, both of which are performant with zero allocation. Use Error, Don\u2019t Panic, But Always Recover: A panic is delivered continuously up the stack until all functions in thecurrent goroutine have returned, at which point the entire program, not just thegoroutine, crashes. We don\u2019t want our server to crash and drop all requests and connections becauseof some rare bug for \u201cindex out of range\u201d. And this bug may be from vendoredcode. As much as possible, we prefer failing the request instead of outages. Hence, I recommend recover() even if you already use error everywhere. 123456789func main() {\tdefer func() {\t\tif r := recover(); r != nil {\t\t\tfmt. Println( Recover main.  , r)\t\t}\t}()\tstartServerThreads()}Use Strict yaml unmarshal: Strict yaml unmarshal can catch typos in the runtime config file. Fail fast isbetter than quiet undefined behaviors. 12345678910import  gopkg. in/yaml. v3 dec := yaml. NewDecoder(f)// KnownFields ensures that the keys in decoded mappings to// exist as fields in the struct being decoded into. dec. KnownFields(true)var conf Configif err := dec. Decode(&amp;conf); err != nil {\t// . . . }"},{id:15,url:"/wiki/infra-as-code/pulumi/",title:"Pulumi",body:"Pulumi: Render go templates as pulumi output futures: It is error-prone and counter-productive to call Sprintf() with many %ss. Imagine counting the i-th %s and ensuring it matches the provided args. Templating is a better solution is this case. However, it is a challenge if your template rendering requires pulumi outputsonly known after apply (i. e. futures), because when the template is rendered,the input values are empty. The way to solve this is using pulumi\u2019s ApplyT() transformation, everyone\u2019sfavorite duck tape. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566{% raw %}import (\t github. com/pulumi/pulumi/sdk/v3/go/pulumi )// OIDCUrn is the output of some other pulumi resource. Its value is only known// after pulumi up, hence it is a future. var OIDCUrn pulumi. StringInputfunc main() {  pulumi. Run(runFn)}func runFn(ctx *pulumi. Context) error {  myPolicy := pulumi. All(OIDCUrn). ApplyT(    func(args []interface{}) string {      arn := args[0]. (string)      t := template. Must(template. New(  ). Parse(`{     Version :  2012-10-17 ,     Statement : [      {         Effect :  Allow ,         Principal : {           Federated : [ {{. ArnPrefix}}:iam::{{. AccountID}}:oidc-provider/{{. OIDCUrn}} ]        },         Action : [           elasticloadbalancing:AddTags ,           elasticloadbalancing:RemoveTags         ],         Resource : [           {{. ArnPrefix}}:elasticloadbalancing:{{. Region}}:{{. AccountID}}:targetgroup/*/* ,           {{. ArnPrefix}}:elasticloadbalancing:{{. Region}}:{{. AccountID}}:loadbalancer/net/*/* ,           {{. ArnPrefix}}:elasticloadbalancing:{{. Region}}:{{. AccountID}}:loadbalancer/app/*/*         ]      }    ]  }`))    arnPrefix =  arn:aws     if onGovCloud() {      arnPrefix =  arn:aws-us-gov     }    var buf bytes. Buffer    if err := t. Execute(&amp;buf, struct {      OIDCUrn string      ArnPrefix string      AccountID string      Region  string    }{      arn,      arnPrefix,       some-account-ID ,       us-west-2 ,    }); err != nil {      return       }    return buf. String()  })  policy, err := iam. NewPolicy(o. Ctx,  my-policy , &amp;iam. PolicyArgs{    Name:    pulumi. String( my-policy ),    Policy:   myPolicy,  })}{% endraw %}"},{id:16,url:"/wiki/shell/",title:"Shell Script",body:"Robust Shell Scripts: First of all, if your script is over 100 lines, it should probably be rewrittenin Python or Go. Bash Options: Always do set -o errexit -o pipefail -o nounset at the start of the script.  -o errexit: Abort when a command exits with non-zero status (except in until, while, if) -o pipefail: Causes a pipeline to return the exit status of the last commandin the pipe that returned a non-zero return value.  -o nounset: Attempt to use undefined variable causes an error and forces an exitTrapping Deferred Calls: 123trap '{ rm -f ' ${TEMP_FILE}   ${LOCK_FILE} '}' INT TERM EXIT INT: capturing SIGINT (Interrupt). ctrl-c sends such a signal.  TERM: capturing SIGTERM (Terminate).  EXIT is a pseudo-signal triggered when the script exits, either throughreaching the end of the script, an exit command, or a failing command whenset -o errexit. Just Quote Everything: Variable expansion:  Good:  $my_var  Bad: $my_varCommand substitution:  Good:  $(cmd)  Bad: $(cmd)Linter and Formatter:  Use shellcheck, a shell script staticanalysis tool, to improve your script.  Use shfmt to format your scriptFurther Reading: Bash Pitfalls: a compilation ofcommon mistakes made by bash users. Snippets: 12345678910111213141516171819202122232425262728293031323334353637# log interpolates and writes message to stderr with timestamp. log() { echo  [$(date +'%Y-%m-%dT%H:%M:%S%z')]: $*  &gt;&amp;2}# Retry a command up to a specific numer of times until it exits successfully,# with exponential back off. ## $ retry 5 echo Hello# Hello## $ retry 5 false# Retry 1/5  false  exited 1, retrying in 1 seconds. . . # Retry 2/5  false  exited 1, retrying in 2 seconds. . . # Retry 3/5  false  exited 1, retrying in 4 seconds. . . # Retry 4/5  false  exited 1, retrying in 8 seconds. . . # Retry 5/5  false  exited 1, no more retries left. #retry() {\tlocal retries=$1\tshift\tlocal count=0\tuntil  $@ ; do\t\texit=$?\t\twait=$((2 ** count))\t\tcount=$((count + 1))\t\tif [ $count -lt  $retries  ]; then\t\t\tlog  Retry $count/$retries  $*  exited $exit, retrying in $wait seconds. . .  \t\t\tsleep $wait\t\telse\t\t\tlog  Retry $count/$retries  $*  exited $exit, no more retries left.  \t\t\treturn $exit\t\tfi\tdone\treturn 0}"},{id:17,url:"/wiki/go/snippets/",title:"Useful Go Snippets",body:"Interface: To assert that a struct implements an interface: 1var _ CoolInterface = (*CuteStruct)(nil)Network: Serve http multiplexer under subpath: 123456789import \t github. com/grpc-ecosystem/grpc-gateway/v2/runtime gwmux := runtime. NewServeMux()if err := pb. RegisterSomeServiceHandler(ctx, gwmux, conn); err != nil {  panic(err)}mux := http. NewServeMux()mux. Handle( /api/v1/ , http. StripPrefix( /api/v1 , gwmux))Test server with any available port: If you hardcode the port number, your CI system might have trouble running twotests at once that both want the same port. Instead, use whichever portavailable: 123456789101112131415grpcLis, err := net. Listen( tcp ,  localhost:0 )if err != nil {  t. Fatal(err)}server := grpc. NewServer()// . . . pb. RegisterSomeServiceServer(s, svc)go server. Serve(grpcLis)conn, err := grpc. Dial(srv. grpcLis. Addr(). String(), grpc. WithInsecure())if err != nil {  t. Fatalf( did not connect: %v , err)}defer conn. Close()clnt := pb. NewSomeServiceClient(conn)Struct: Validate Fields and Set Default Values: It gets verbose to validate configuration input or set default values, like thefollowing. It gets worse when add more fields to Config. 12345678910111213141516171819type Config struct {  KubeClientSet kubernetes. Interface  StopCh    &lt;-chan struct{}  Threadiness  int}func NewController(c *Config) (*Controller, error) {  if c. KubeClientSet == nil {    return nil, errors. New( KubeClientSet cannot be nil )  }  if c. StopCh == nil {    return nil, errors. New( StopCh cannot be nil )  }  if c. Threadiness == 0 {    c. Threadiness = 2  }  return &amp;Controller{/* . . . */}}I recommend using go-playground/validator for validation and creasty/defaults for settingdefaults. Both packages leverage the field tags in struct. 123456789101112131415161718192021import (   github. com/creasty/defaults    github. com/go-playground/validator/v10 )type Config struct {  KubeClientSet kubernetes. Interface `validate: required `  StopCh    &lt;-chan struct{}   `validate: required `  Threadiness  int         `default: 2  validate: gte=0,lte=10 `}func NewController(c *Config) (*Controller, error) {  if err := validator. New(). Struct(conf); err != nil {    return nil, fmt. Errorf( invalid config: %v , err)  }  if err := defaults. Set(conf); err != nil {    return nil, fmt. Errorf( could not set default values: %v , err)  }  return &amp;Controller{/* . . . */}}HashSet with Empty Struct: Empty struct takes exactly zero memory. 123456789strSet := make(map[string]struct{})// Add to set. strSet[ hello ] = struct{}{}// Check if the set contains an element. if _, ok := strSet[ hello ]; ok {  log. Println( set contains `hello' )}Functional Options: 12345678910111213141516171819202122232425262728293031323334353637type Foo struct {  Num int  Str string}type Option func(f *Foo)func WithNum(num int) Option { return func(f *Foo) {  f. Num = num }}func WithStr(str string) Option { return func(f *Foo) {  f. Str = str }}func New(opts . . . Option) *Foo { foo := &amp;Foo{  num: 10,  str:  hello , } for _, applyOpt := range opts {  applyOpt(foo) } return &amp;foo}func main() { foo := New() foo = New(WithNum(30)) foo = New(WithNum(20), WithStr( world ))}For more details, see Parameters with Defaults in Go: Functional Options Certificate Signing Requests with Email Address: 1234567891011121314151617181920212223242526272829303132333435363738var oidEmailAddress = asn1. ObjectIdentifier{1, 2, 840, 113549, 1, 9, 1}func NewCSR(commonName string, key crypto. PrivateKey, w io. Writer) error {\tsubj := pkix. Name{\t\tCommonName:     commonName,\t\tCountry:      []string{ US },\t\tProvince:      []string{ California },\t\tLocality:      []string{ Palo Alto },\t\tOrganization:    []string{ Pied Piper, Inc.  },\t\tOrganizationalUnit: []string{ Best team },\t\tExtraNames: []pkix. AttributeTypeAndValue{\t\t\t// Crazy dance just to set the email address in CSR. \t\t\t// https://stackoverflow. com/questions/26043321/create-a-certificate-signing-request-csr-with-an-email-address-in-go. \t\t\t{\t\t\t\tType: oidEmailAddress,\t\t\t\tValue: asn1. RawValue{\t\t\t\t\tTag:  asn1. TagIA5String,\t\t\t\t\tBytes: []byte( foo@example. com ),\t\t\t\t},\t\t\t},\t\t},\t}\t// Don't add email addresses to the CertificateRequest EmailAddresses field,\t// which sets a SubjectAltName (SAN). Those are designed more for things\t// like signed emails. In the context of TLS certificates, SANs should be\t// used for alternatively valid hostnames and IP addresses. \ttemplate := x509. CertificateRequest{\t\tSubject:      subj,\t\tSignatureAlgorithm: x509. SHA256WithRSA,\t}\tcsrBytes, err := x509. CreateCertificateRequest(rand. Reader, &amp;template, key)\tif err != nil {\t\treturn err\t}\treturn pem. Encode(w, &amp;pem. Block{Type:  CERTIFICATE REQUEST , Bytes: csrBytes})}Operators: ^ only means bit-wise XOR in Go. For logical XOR, one can use !=. 1234var a, b boolif a != b {  // only true if exactly one of {a, b} is true. }"},{id:18,url:"/wiki/infra-as-code/terraform/",title:"Terraform",body:"Terraform: No diff if you only updated outputs: To force a state change based on an output update, you need to update your codeto include a resource that gets update based on output changes such as thefollowing: 12345resource  random_uuid   tfc_output_refresh  { keepers = {  refresh =  ${filesha256(${path. module}/output. tf)}  }}Switch between Terraform versions: Use tfswitch to update your localterraform CLI version. This is useful when you need to do terraform state surgery,because terraform states are versioned. Terraform will complain when operatingon states with future versions, which happens when Terraform remote executoris using a dated version of terraform unlike yours. Select specific resources: If your team (unfortunately) applies terraform manually and planning takes along time, consider selecting only the subset of resources that youchanged. You can provide as many -targets as you wish. 123456terraform plan -target=${RESOURCE_TYPE}. ${RESOURCE_NAME}# Exampleterraform plan  -target=aws_route53_record. example-com-A  -target=aws_route53_record. www-example-com-CNAME"},{id:19,url:"/thesis",title:"Thesis",body:""},{id:20,url:"/wiki/vault/",title:"Vault",body:"Vault Agent Sidecar to Request Certificates: The vault agentsidecar canbe used to not only read secrets from Vault but also to request certificatesfrom Vault. The following example assumes the vault PKI amount is at /pki. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869apiVersion: apps/v1kind: DaemonSetmetadata: name: envoy-ingress labels:  app: enovy-ingressspec: selector:  matchLabels:   app: envoy-ingress template:  metadata:   labels:    app: envoy-ingress   annotations:    vault. hashicorp. com/agent-inject-secret-hushhush. crt:  pki/issue/hushhush_role     vault. hashicorp. com/agent-inject-template-hushhush. crt: |     {% raw %}{{- with secret  pki/issue/hushhush_role   common_name=hushhush. acme. com  -}}     {{ . Data. certificate }}     {{ index . Data. ca_chain 0 }}     {{ index . Data. ca_chain 1 }}     {{- end }}{% endraw %}    vault. hashicorp. com/agent-inject-secret-hushhush. key:  pki/issue/hushhush_role     vault. hashicorp. com/agent-inject-template-hushhush. key: |     {% raw %}{{- with secret  pki/issue/hushhush_role   common_name=hushhush. acme. com  -}}     {{ . Data. private_key }}     {{- end }}{% endraw %}    # The certificate and the key will be located in    # /vault/secrets/hushhush. {crt,key} respectively.     vault. hashicorp. com/agent-inject:  true     vault. hashicorp. com/role:  my_kube_auth_backend_role     vault. hashicorp. com/agent-pre-populate:  true     vault. hashicorp. com/service:  https://vault. internal. acme. com:8200     vault. hashicorp. com/ca-cert:  /vault/tls/ca. crt     vault. hashicorp. com/tls-secret:  acmecorp-root-ca-cert     vault. hashicorp. com/tls-server-name:  vault. internal. acme. com. com   spec:   containers:    - name: envoy     image: envoyproxy/envoy:v1. 22     imagePullPolicy: IfNotPresent     volumeMounts:      - name: config-volume       mountPath: /etc/envoy       readOnly: true      - name: acmecorp-root-ca-cert-volume       mountPath: /etc/acmecorp-root-ca       readOnly: true   volumes:    - name: config-volume     configMap:      name: envoy-config    - name: acmecorp-root-ca-cert-volume     secret:      secretName: acmecorp-root-ca-cert# The CA cert is technically not a secret, but vault-agent init container# can only reference a Secret object. ---apiVersion: v1kind: Secretmetadata: name: acmecorp-root-ca-cert namespace: envoy-ingresstype: OpaquestringData: ca. crt: |  -----BEGIN CERTIFICATE-----  . . .   -----END CERTIFICATE-----"},{id:21,url:"/robots.txt",title:"",body:"      Sitemap: {{ \u201csitemap. xml\u201d   absolute_url }}   "},{id:22,url:"/page2/",title:"Home",body:"{% if page. url == \u201c/\u201d %}  12345678910111213141516&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;Popular&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row &gt;{% for post in site. posts %}  {% if post. featured == true %}    {% include featuredbox. html %}  {% endif %}{% endfor %}&lt;/div&gt; {% endif %}  123456789101112131415&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;New Posts&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row listrecent &gt;  {% for post in paginator. posts %}  {% include postbox. html %}  {% endfor %}&lt;/div&gt;    &lt;/div&gt;  {% include pagination. html %}  "},{id:23,url:"/page3/",title:"Home",body:"{% if page. url == \u201c/\u201d %}  12345678910111213141516&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;Popular&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row &gt;{% for post in site. posts %}  {% if post. featured == true %}    {% include featuredbox. html %}  {% endif %}{% endfor %}&lt;/div&gt; {% endif %}  123456789101112131415&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;New Posts&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row listrecent &gt;  {% for post in paginator. posts %}  {% include postbox. html %}  {% endfor %}&lt;/div&gt;    &lt;/div&gt;  {% include pagination. html %}  "},{id:24,url:"/page4/",title:"Home",body:"{% if page. url == \u201c/\u201d %}  12345678910111213141516&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;Popular&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row &gt;{% for post in site. posts %}  {% if post. featured == true %}    {% include featuredbox. html %}  {% endif %}{% endfor %}&lt;/div&gt; {% endif %}  123456789101112131415&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;New Posts&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row listrecent &gt;  {% for post in paginator. posts %}  {% include postbox. html %}  {% endfor %}&lt;/div&gt;    &lt;/div&gt;  {% include pagination. html %}  "},{id:25,url:"/page5/",title:"Home",body:"{% if page. url == \u201c/\u201d %}  12345678910111213141516&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;Popular&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row &gt;{% for post in site. posts %}  {% if post. featured == true %}    {% include featuredbox. html %}  {% endif %}{% endfor %}&lt;/div&gt; {% endif %}  123456789101112131415&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;New Posts&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row listrecent &gt;  {% for post in paginator. posts %}  {% include postbox. html %}  {% endfor %}&lt;/div&gt;    &lt;/div&gt;  {% include pagination. html %}  "},{id:26,url:"/page6/",title:"Home",body:"{% if page. url == \u201c/\u201d %}  12345678910111213141516&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;Popular&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row &gt;{% for post in site. posts %}  {% if post. featured == true %}    {% include featuredbox. html %}  {% endif %}{% endfor %}&lt;/div&gt; {% endif %}  123456789101112131415&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;New Posts&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row listrecent &gt;  {% for post in paginator. posts %}  {% include postbox. html %}  {% endfor %}&lt;/div&gt;    &lt;/div&gt;  {% include pagination. html %}  "},{id:27,url:"/speculative-decoding/",title:"Accelerate LLM Inference with Speculative Decoding",
body:"2025/03/11 - Many inference speedup techniques mirror the classic systems regime\u2014such ascaching, paging, tiling, pipelining, and speculative execution (e. g. branchprediction and cache prefetch). Speculative decoding, generalizing speculativeexecution to stochastic settings, produces several tokens in each forward pass,without changing the output distribution (model quality) or model parameters. This post discusses two approaches of Speculative Decoding:  Speculative sampling with a draft model (2023 paper) Multiple decoding heads with Tree Attention (2024 paper)Speculative Sampling with a Draft Model: Speculative sampling, or rejection sampling, uses an approximation/draftmodel\u2014smaller and faster than the model you want to accelerate\u2014to generate $k$tokens autoregressively and then uses the larger model to verify the $k$ tokensin one pass. How to verify each speculative token: Let $M_p$ be the target model and $M_q$ be the approximation model. Let $p(x)$ be the probability of $x$ under $M_p$ (a shorthand for $p(x_t | x_1, ldots, x_{t-1})$). Let $q(x)$ be the probability of $x$ under $M_q$. Sample a token $x$ from $M_q$. Don\u2019t wait for the big model to verify $x$. Continue to sample the next $k$ tokens from $M_q$. Then, for each speculative token $x$:If $p(x) geq q(x)$, accept $x$. If $p(x) &lt; q(x)$, accept $x$ with probability $p(x) / q(x)$; if $x$ notaccepted, sample again from an adjusted distribution of $M_p$, where $p\u2019(x) =norm(max(0, p(x) - q(x)))$. It is proven that $x$ sampled this way ensures $x sim p(x)$. Verify in one pass: The key insight is that the big model can verify the $k$ speculative tokens inone pass. The reason is that in the self-attention mechanism, the big modelcomputes contextualized representations of all prefixes in parallel, i. e. the modeloutputs       [p(x_1   \text{prefix}), quad p(x_2   \text{prefix} + x_1), quad ldots, quad p(x_k   \text{prefix} + x_1 + x_2 + ldots + x_{k-1})]   simultaneously. Multiple Decoding Heads with Tree Attention: Choosing the right draft model is hard. What if we just reuse the same model? Medusaproposes attaching multiple decoding heads to the same model. In the caseof 2 heads, head 1 predicts the immediate next token, and head 2 predicts thesecond token after the prefix. Two heads output at the same time.   (image source) Training multiple decoding heads: The standard model only has one decoding head tasked to predict the next token. Thus, to use multiple decoding heads, we need to train the extra heads. Thetraining needs:  loss function tree attention &amp; adjusted positional encodingLoss function: Use the cross-entropy loss between the prediction of extra heads and the groundtruth. To quote the paper:  Given the ground truth token $y_{t+k+1}$ at position $t+k+1$, the loss for the$k$-th head is $L_k = - log p_t^{(k)} (y_{t+k+1})$, where $p_t^{(k)} (y)$denotes the probability of token $y$ predicted by the $k$-th head.  We also observe that $L_k$ is larger when $k$ is larger, which is reasonablesince the prediction of the $k$-th head is more uncertain when $k$ is larger. Therefore, we can add a weight $lambda_k$ to $L_k$ to balance the loss ofdifferent heads. The final loss is:  [mathcal{L}{\text{MEDUSA-1}} = sum{k=1}^{K} -lambda_k log p_t^{(k)}(y_{t+k+1}). ] Tree attention &amp; adjusted positional encoding: Suppose the first decoding head predicts 2 candidates for the next token, andthe second decoding head predicts 3 candidates for the next next token. We havea tree of $2 \times 3 = 6$ branches. The tree structure creates two challenges:    We need to adjust attention mask such that a token generated from a specificcandidate path can only attend to previous tokens within that same path andshould ignore other branches.     We need to adjust positional encoding because there are multiple candidatesfor the same position (as in depth in the tree).  The solution is Tree Attention shown below. Note that the attention maskexclusively permits attention flow from the current token back to its antecedenttokens.   (image source) "},{id:28,url:"/multi-head-attention/",title:"Parallelizing Multi-Head Attention",body:"2025/02/28 - In the multi-head attention mechanism, why after reshaping the projectionmatrices for Q/K/V from 3 dimensions to 4, we need to transpose the tokensdimension with the heads dimension? Using the canonical example code for Attention Heads below as an example, why dowe need Q = self. W_q(x). reshape(B, T, self. num_heads,self. head_dim). transpose(1, 2)?    123456789101112131415161718192021222324252627282930313233343536import torchimport torch. nn as nnclass MultiHeadSelfAttention(nn. Module):  def __init__(self, embed_size, num_heads):    super(). __init__()    self. num_heads = num_heads    self. head_dim = embed_size // num_heads # Split embeddings across heads    # Linear layers for Q, K, V    self. W_q = nn. Linear(embed_size, embed_size)    self. W_k = nn. Linear(embed_size, embed_size)    self. W_v = nn. Linear(embed_size, embed_size)    # Fully connected output layer, i. e. $W^O$    self. fc_out = nn. Linear(embed_size, embed_size)  def forward(self, x):    B, T, D = x. shape # Batch, Seq_len, Embedding_dim    Q = self. W_q(x). reshape(B, T, self. num_heads, self. head_dim). transpose(1, 2)    K = self. W_k(x). reshape(B, T, self. num_heads, self. head_dim). transpose(1, 2)    V = self. W_v(x). reshape(B, T, self. num_heads, self. head_dim). transpose(1, 2)    # Compute attention scores (scaled dot-product attention)    scores = torch. matmul(Q, K. transpose(-1, -2)) / (self. head_dim ** 0. 5)    attn = torch. softmax(scores, dim=-1) # Normalize scores    output = torch. matmul(attn, V) # Apply attention to values    # Concatenate and project back to embedding size    output = output. transpose(1, 2). contiguous(). reshape(B, T, D)    return self. fc_out(output)# Example usagex = torch. randn(1, 10, 512) # Batch size 1, Sequence length 10, Embedding size 512attention_layer = MultiHeadSelfAttention(embed_size=512, num_heads=8)output = attention_layer(x)print(output. shape) # Should be [1, 10, 512] Review Multi-head Attention: Instead of using a single attention head, transformers use multiple heads. Eachattention head has its own set of projection matrices for Q, K, and V. Each headlearns to focus on different types of relationships. For example:  One head might focus on long-range dependencies, such as linking a subject toits verb.  Another might focus on local context, such as detecting adjective-noun pairs.  Another might specialize in syntax or semantics. Each head has independent learnable weight/projection matrices $W_i^Q, W_i^K,W_i^V$, where $i$ is the head index. Each weight matrix has shape $(D, D/h)$where $D$ is the embedding dimension and $h$ is the number of heads. For input embeddings $X$ of shape $(B, T, D)$, the head $i$ computes: [Q_i = X W_i^Q, quad K_i = X W_i^K, quad V_i = X W_i^V] The multi-head attention then concatenates the per-head attention outputs andlinearly mixes them: [\text{MultiHeadAttention}(Q, K, V) = \text{Concat}left(\text{head}_1, \text{head}_2, ldots, \text{head}_h\right)W^O] Each head has output shape $(B, T, D/h)$. The concatenated output has shape $(B,T, D)$. $W^O$ is trainable and has shape $(D, D)$. $W^O$ mixes informationacross heads and refines the final representation before passing it to the nextlayer. Reshaping for Parallelization: Let\u2019s break down Q = self. W_q(x). reshape(B, T, self. num_heads,self. head_dim). transpose(1, 2): self. W_q is a linear layer (nn. Linear(embed_size, embed_size)) that projectsx into a new representation, specifically the query (Q) in multi-headattention. This linear transformation does not change the shape of x, whichremains (B, T, D). In . reshape(B, T, self. num_heads, self. head_dim), we split the embeddingdimension D into self. num_heads and self. head_dim (where self. head_dim =D / self. num_heads). For example, if D = 512 and self. num_heads = 8, thenself. head_dim = 512 / 8 = 64. The shape becomes: (B, T, 8, 64). In . transpose(1, 2), we swap the sequence length dimension (T) and thenumber of heads dimension (num_heads). Note that index 1 is the second column. This changes the shape: Before: (B, T, num_heads, head_dim)After: (B, num_heads, T, head_dim) This rearrangement makes attention more efficient, because matrixmultiplications can now parallelize across heads (each head operatesindependently on different parts of the embedding). The reason is the following. In the code scores = torch. matmul(Q, K. transpose(-1, -2)) / (self. head_dim **0. 5), K. transpose(-1, -2) swaps the last two dimensions so that K has theshape K^T: (B, num_heads, head_dim, T). Then, the matrix multiplication Q @ K^T is (B, num_heads, T, head_dim) @ (B,num_heads, head_dim, T), which results in (B, num_heads, T, T). Thus, Q @ K^T becomes more efficient after transpose(1, 2) because:  Parallel Computation for Each Head: By keeping num_heads as the seconddimension, each head\u2019s computation happens independently but in parallelacross the batch, using optimized GPU kernels.  Better Memory Access Patterns: GPUs are highly optimized for contiguousmemory access. The transpose(1, 2) operation ensures that each head\u2019s data isgrouped together, improving cache efficiency during matrix multiplication. "},{id:29,url:"/lean-startup/",title:"Notes: The Lean Startup",body:"2024/01/21 - Careful planning and execution work for general management but not for startups. Perfect execution is futile if you end up building something nobody wants (waste). The real progress for startups is not how many JIRA tickets we closed but how fast we gain validated learnings\u2014what creates value for customers and their willingness to pay\u2014while minimizing waste. Systematically break down a business plan and test each part\u2014define a clear hypothesis (predictions about what is supposed to happen) then A/B test to prove the predictions. Two leap-of-faith assumptions:  Value hypothesis: do customers find value Growth hypothesis: how new customers discover the productTest assumptions with MVP that targets early adopters, in the Build-Measure-Learn loop. Any work beyond what was required to learn is waste, no matter how important it seemed. If we do not know who the customer is, we do not know what quality is. Even a \u201clow-quality\u201d MVP can act in service of building a great high-quality product. Plan the Build-Measure-Learn loop in reverse order: decide what we need to learn, then figure out what we need to measure, then see what product we need to build to experiment. Launch MVP under a different brand name if you worry about branding risk. Commit to iteration: don\u2019t give up hope due to bad news from MVPs, but experiment more, learn more, and maybe pivot. Do not blindly copy successful startups\u2019 techniques. Charging customers from day one works for Stripe but not Facebook. Low-quality early prototypes works for Facebook but not mission-critical industries. Always experiment to see what works in our unique circumstances. Eventually a successful startup will compete with fast followers. A head start is rarely large enough to matter. The only way to win is to learn faster than everyone else. Vanity metrics, such as gross number of customers, are not actionable. We cannot conclude whether metrics growth is due to  latest product development, or driven by decisions the team had made and that current initiatives have no impact. Use cohort-based metrics (e. g. among users who signed up in June, what percentage exhibits behaviors we want), and use A/B test to conclude causality. Measure team\u2019s productivity in units of validated learning, not the production of new features. Pivot: test a new fundamental hypothesis in business model, product road map, partnership, customer segments, engine of growth. The decision to pivot depends on data &amp; intuition. Misguided decision to persevere is value destructive. Signs of time to pivot: the decreasing effectiveness of product experiments and the general feeling that product development should be more productive. Startup\u2019s runway is the number of pivots it can still make. To extend runway, achieve the same amount of validated learning at lower cost in shorter time. Schedule in advance regular \u201cpivot or persevere\u201d meetings. In pivots, don\u2019t throw out everything and start over. Repurpose what has been built and learned.  My personal thought:The search for PMF is like gradiant descent, a combination of intuition and data. Gradiant descent is an optimization algorithm: start at a point (your best guess), then iteratively descent in the direction of \u201cslope\u201d. The process is almost mechanical, but you need to start with an intuitive guess. Pivot means to find a new starting point. You need to pivot when1) the \u201cslope\u201d around the current point is flat in all directions. Nothing you do seems to improve the metrics, or2) you are trapped in a local minimum (saturating early adoptors) and want to find a better local minimum (mainstream customers). Once you have found success with early adopters, you want to sell to mainstream customers. Early adopter market is saturated quickly despite prior \u201cup and to the right\u201d results. Mainstream customers have different and more demanding requirements. This is a customer segment pivot. The actions we need to win mainstream customers is different from how we won early adopters. Pivot requires new MVP. Just as lean manufacturing uses just-in-time production to reduce in-process inventory, Lean Startups practice just-in-time scalability, conducting product experiments with small batch size. Imagine that the letters didn\u2019t fit in the envelopes. With the large-batch approach, we wouldn\u2019t find that out until nearly the end. With small batches, we\u2019d know almost immediately. Smaller batch size (small diff in product code change) means shorter Build-Measure-Learn cycle and less WIP waste. Sustainable growth: new customers come from the actions of past customers:word of mouth, product usage (wearing designer cloths), funded advertising, repeat purchase. Sticky Engine of Growth: repeat usage; use customer retention rate to test growth hypothesis. Other metrics like activation rate and revenue per customer can test value hypothesis but has little impact on growth. If the rate of new customer acquisition exceeds the churn rate, the product will grow. Viral Engine of Growth: focus on increasing the viral coefficient. Many viral products do not charge customers but advertisers, because viral productscannot afford to have any friction impede the process of signing customers up. Paid Engine of Growth: advertising, outbound sales, foot traffic. Use LTV/CAC to test growth hypothesis. Over time, CAC is bid up by competition. A startup can evaluate PMF by evaluating each Build-Measure-Learn iteration using innovation accounting. Every engine is tied to a set of customers and their habits, preferences, channels, and interconnections and thus eventually runs out of gas. If the boss tends to split the difference, the best way to influence the boss is to take the most extreme position. Your opponants will do the same. Over time, everyone will take the most polarized positions. Don\u2019t split the difference. Instead create a sandbox for innovation that will contain the impact but not methods of the new innovation. It works as follows: Any team can create a true split-test experiment that affects only the sandboxed parts of the product or service (for a multipart product) or only certain customer segments or territories (for a new product). One team must see the whole experiment through from end to end. No experiment can run longer than a specified amount of time. No experiment can affect more than a specified percentage of customers. Every experiment has to be evaluated on the basis of a single standard report of five to ten actionable metrics. Every team that works inside the sandbox use the same metrics to evaluate success. Any team that creates an experiment must monitor the metrics and customer reactions (support calls, social media reaction, forum threads, etc. ) while the experiment is in progress and abort it if something catastrophic happens. If you like notes like this, check out my bookshelf. "},{id:30,url:"/venture-deals/",title:"Notes: Venture Deals",body:"2024/01/19 - Before Fundraise:Allow minimum three to six months to raise money. Have a clean cut from last job to avoid IP disputes. Prepare data site (Certificate of Incorporation, Bylaws, board minutes, cap table, customer list, product roadmap, org chart, employment agreements, budgets, financial statements). Some VC deals fail to close because of one missing IP assignment agreement. If you want money, ask for advice. Develop relationships with VC before fundraising. Research &amp; make the outreach personal. Find mentors, not fundraising advisors asking for a cut. During Fundraise:Do not email a teaser, hoping to share more in meeting. Whatever you send a VC may be your last, so send the full yet concise pitch. The presentation is to communicate the same info in the executive summary but with more examples and visuals. Aim for 10 slides or fewer. Offer a prototype or demo that VC can interact. Demo is the best way to show your vision. Watch VC reactions during demo. Did their eyes light up? Do they understand the domain?Raise enough to get to the next milestone, plus buffer. Hire experienced startup lawyer with fee cap. Don\u2019t let VC talk you out of your lawyer choice. After you\u2019ve had a second meeting, ask what the process is going forward. Get multiple term sheets to create competition. If a VC passes, insist on feedback, which improves your next pitch. Decision Maker:Have a lead investor representing the entire syndicate. If party round, set up a special-purpose limited partnership, not to chase down 75 signatures. At every VC, find out &amp; talk to the decision makers. Reference check the VC: founders who went through hard times, like fired as CEO, learn how the VC handled tough situations. Add more to data site, but watch out for busywork that associates assigns, and the risk of leaking to a competing portco. Again, make sure decision makers are involved. Option Pool: Typical size of early-stage company option pool is 10% to 20%. Smaller pools for later stage. \u201cWe have enough options to cover our needs. If we need to expand the pool before the next financing, we will provide full antidilution protection for you. \u201d VCs want to minimize future dilution by enlarging the option pool up front. Founders should push back with an option budget that lists out futures hires until next financing and the option grant to land each hire. Liquidation Preference: In early stage, it\u2019s in the best interest of both VC and founder to have a simple liquidation preference and no participation. In future rounds, the terms are often inherited from the early stage terms. If the seed investor doesn\u2019t invest in future rounds, his economics in many outcomes could be worse with participating and end up looking like the common holders (in terms of returns), since their preference amounts are so small. Pay to Play: Investors must invest pro-ratably in future financings (paying) not to have their preferred stock converted to common (playing) or lose anti-dilution rights. Not a lifetime guarantee but an incentive to follow on, if other investors decide to invest in next round. Pay to play reduces liquidation preferences for the nonparticipating investors and ensures only committed investors have preferred stock. If VC pushes back, \u201cWhy? Are you not going to fund the company in the future if other investors agree to?\u201d Avoid the pay-to-play scenario where VC has the right to force a recapitalization (e. g. financing at a $0 pre-money valuation) if fellow investors don\u2019t play in the new round. Founder Vesting: Negociate to treat vesting as a clawback with an 83(b) election. Single-trigger: accelerated vesting upon M&amp;A. Double-trigger: accelerated vesting upon M&amp;A and being fired. Balanced approach: double trigger with one-year acceleration. Anti-Dilution: protect prior investors in a down round (equity issued at lower price). Flavors: weighted average (normal), or full ratchet (rare; reduce earlier round price to new price). Antidilution is often implemented as a price reduction for conversion to common. More exceptions in antidilution carve-outs, the better for founders. Board: Be wary of observers. Question what values they bring. Often they are associates. Sometimes they disclose board topics to brag to their friends. Get a small board. Independent board members usually get stock options 0. 25% to 0. 5% vest over 2 to 4 years. Observers don\u2019t get options. Instead of controlling the board, VC uses protective provisions (veto rights on certain actions). Next-round investors want protective provisions too, but founders should push for all Preferred voting as a single class, instead of each Series voting separately. VCs charge all expenses associated with board meetings to the company. Mandate frugality. Place a cap early on the percentage of directors who can be VCs (not independent). Preemptively offer observer rights to dethroned director, or establish an executive committee of the board that can meet without everyone else. Drag Along: A compromise is to grant drag-along rights to the majority of the common stock, not the preferred. Preferred can convert some to common to force a majority at the cost of lowering the overall liquidation preference. IPO: preferred convert to common. Never give different automatic conversion terms for different series of preferred. Push for low threshold to conversion. Redemption: Ensure dividends require board majority approval. Allow investors to sell shares back to the company for a guaranteed return. Never agree to \u201cAdverse Change Redemption\u201d because it is vague, punitive, and investors can act on arbitrary judgments. No-shop: Do not agree to pay for legal fees until deal done. Avoid pre-financing contingence: 1. \u201cApproval by Investors\u2019 partnerships\u201d means the term sheet has not been approved. 2. \u201cEmployment Agreements acceptable to investors\u201d: review &amp; negotiate full terms (e. g. what happens on termination?) before signing term sheet. Limit the no-shop period to 30 days (worst case 60 days), automatically canceled if VC terminates. Commitment should be bidirectional. You agree not to shop deals, VC agrees to close timely. Ask for exception for acquisitions. Frequently financings and acquisitions follow each other. Registration Rights: Always offered to investors. Lawyers often make innocuous edits on this section. Unnecessary. Upon IPO when the rights apply,investment bankers will restructure the deal. Right of First Refusal: Define \u201cmajor investor\u201d, only give such right to them and only if they play in subsequent rounds. Enforce stock sale also transfers obligations the original owner signed up for. Co-sale Agreement: This right says if a founder sells shares, investors can sell too. Hard to remove this right, but founders should ask for a floor. Why should VC hold it up if a founder just sells a small amount to pay off mortgage? SAFE: Some VCs consider valuation cap as a price ceiling to the next round, so do not disclose seed-round terms until you have negociated new price. Legal fee for priced equity round has dropped, no more than SAFE. Zombie VC: VC who past their investment period (usually 5 years) and did not raise a new fund. They can\u2019t invest but still meet you. Waste time. Ask \u201cwhen was your last investment\u201d (more than a year = zombie). \u201cHow many more investments will you make out of the current fund?\u201d Reserve: Fund approaching end of life creates pressure for liquidity. Underreserved VC may resist new financings, limit round size, or push for sale of company to limit dilution, even when more funding is right for commons. Pay-to-play creates more resistance in this case. Follow-on might come from new fund, or a different fund vehicle (\u201cOpportunity\u201d funds) Corporate VC: They look for more control, such as right of first refusal on acquisition, which you should never give. Negociation: Goals: achieving a good and fair result, not killing your personal relationship. Preparation is key. Focus on valuation, option pool, liquidation preferences, board, and voting controls. Know what concetions you are ok with and when to walk away. Get to know the other side ahead of time, play to their strengths, weaknesses, biases, curiosities, and insecurities. First-time founder has one advantage over seasoned VC: time. They got family, LPs, portcos to deal with. You got one company and this negocitation. Ask what the 3 most important terms are for VC. Explain yours too. Call them out if they pound hard on minor points. Don\u2019t make threat you can\u2019t deliver. Don\u2019t say who else you are pitching to. Never provide term sheet from other VC. Don\u2019t address deal points in order but focus on whole picture. \u201cThat\u2019s the way it is because it\u2019s market. \u201d probe on why the market condition applies to you. Talk to other founders to get market intelligence. Push back with \u201cWait a minute, this term creates incentive misalignment. Let\u2019s avoid a divisive relationship. \u201d If stuck with terms you don\u2019t love, next-round investors may fix them because they want your team happy and motivated. After big wins and some time together, renegociate with existing investors for founder-friend terms. Price: High valuation is risky because1) VCs hold out for a higher exit (by big perf stack, or forbid sales below $X), then founders can\u2019t sell at a price they would have been happy with. 2) At higher price, sophisticated investors demand more structure, resulting in significant outcome misalignment between early and late stage investors. Investment Banks: Avoid them at early stage. Hire them in acquisition. They maximize exit value. Best source of bankers: your board members, investors, colleagues, and other senior executives you trust. Hire a banker who knows your sector, like \u201centerprise SaaS\u201d. Daily Operation: Hire an employment lawyer when a founder or exec leaves. Make sure equity &amp; IP are settled to protect future fundraising and acquisition. If a company used a professional valuation firm, the valuation would be assumed to be correct unless the IRS could prove otherwise. File 83(b) election to start earlier the clock for long-term capital gains. Pay at least minimum-wage cash comp to full-time founders &amp; execs. If you like notes like this, check out my bookshelf. "},{id:31,url:"/scaling-istio/",title:"Scaling Istio",
body:"2023/10/22 - In a large, busy cluster, how do you scale Istio to address Istio-proxyContainer being OOM-Killed and Istiod crashes if too many connectedistio-proxies? Istio-proxy Container OOM-Killed: Problem: If istio-proxy dies, Pod disconnects from the world, because istio routes thePod\u2019s ingress and egress through the istio-proxy container. Thus, the mainapplication container cannot communicate with other services, and clients cannotreach the application either. This disrupts existing connections and riskscascading failure when loads shift to other replicas. Out-of-memory kill is #1 reason for the istio-proxy death. The istio-proxy isconfigured with resource limits for CPU and memory, to avoid starving otherworkloads sharing the k8s Node. The istio-proxy is killed once it exceeds thememory limit. Restarting istio-proxy won\u2019t help: By default, Kubernetes uses the restartpolicy \u201cAlways\u201d for Pods. Thus, if the istio-proxy container is OOM killed,Kubernetes will restart it. However, because the usage pattern has not changed,istio-proxy will enter OOMKilled again. This forms a crash loop and continueddisruption to applications. To keep bumping the memory limit is expensive and whack-a-mole. Overtime, youhave increased the memory limit from 256Mi to 2Gi, which is per istio-proxycontainer. Given tens of thousands of Pods in istio mesh cross the hundreds ofclusters, it is expensive to keep raising the limit. Furthermore, many peopleonly increase the limit is when the oncall got paged about crash-looping Pods,which already impact customer traffic. Solution: Use Sidecar object to trim unused xDS configBy default, Istio programs all sidecar proxies with the configuration to reachevery workload in the mesh, as well as accept traffic on all the portsassociated with the workload. But if you have a locked down Istio mesh, and if a tenant must request forallow-listing such source namespace using some onboarding config, then theistio-proxy container does not need the full mesh config. The Sidecar API object can restrict the set of services that the proxy canreach. Adopting the Sidecar objects will reduce the number of xDS pushes andoverall xDS config size. You could templatize the Sidecar objects and renderthem based on the per-namespace onboarding configs. Below is an example Sidecar, which allows istio-proxies in the namespace\u201cobservability-cortex\u201d to egress to four other namespaces.    123456789101112apiVersion: networking. istio. io/v1beta1kind: Sidecarmetadata: name: default namespace: myappspec: egress: - hosts:  -  istio-system/*   -  my-upstream-ns/*   -  kube-system/*   -  observability/*  Use Telemetry object to reduce metrics generationIstio collects and exports a wide range of Prometheus metrics. Metricscollection impacts memory usage. Istio-proxy doesn\u2019t need to generate allmetrics but only those we use. Consider customizing the metrics that Istiocollects and exports.    1234567891011121314151617181920212223242526272829303132333435363738---apiVersion: telemetry. istio. io/v1alpha1kind: Telemetrymetadata: name: drop-unused-metrics-and-tags namespace: istio-systemspec: # no selector specified, applies to all workloads in the namespace metrics:  - providers:    - name: prometheus   overrides:    - match:      metric: ALL_METRICS     tagOverrides:      connection_security_policy:       operation: REMOVE      destination_app:       operation: REMOVE      destination_canonical_service:       operation: REMOVE      destination_canonical_revision:       operation: REMOVE      destination_principal:       operation: REMOVE      . . .     - match:      metric: REQUEST_DURATION     disabled: true    - match:      metric: REQUEST_SIZE     disabled: true    - match:      metric: RESPONSE_SIZE     disabled: true    - match:      metric: TCP_CLOSED_CONNECTIONS     disabled: true Istio Ambient MeshWe can solve sidecar problems if we don\u2019t run sidecar at all. Istio ambientmesh is asidecar-less approach to service mesh, replacing sidecar proxies with per-nodeand (not always necessary) per-namespace proxies. With fewer proxies, it willsave us lots of money in CPU/Memory and provide shorter latency. The general problems with sidecars and benefits of ambient mesh:  Kubernetes does not have first-class support for sidecars (until k8s1. 28). Appcontainer might start before proxy ready, decide itself is unhealthy, and bein a restart loop. Short-lived Pods (Job) need to explicitly kill proxy forPod to complete.  Istio upgrade requires restarting every pod to inject newer-version Istioproxies Sidecar resources are underutilized Difficult to calculate namespace quotas (ResourceQuotas) because sidecars aretransparent to tenants but consume namespace quotas. If you use Calico to enforce L4 NetworkPolicy for Pods, you might face a blockerto adopting ambient mesh because of conflicting IPTables rules that Calico owned(GitHub issue still open). But Iencourage you to do another proof of concept, because someone (GitHubissue) used eBPF instead ofIPTables to redirect traffic to ambient-mode proxies, thus working around theconflicting Calico IPTables rules. Istiod crash if too many connected istio-proxies: Problem: Istiod is the control plane of istio. All istio-proxies connect to istiod. Istiod may crash when there were too many connected istio-proxies, specificallyif they all were added at the same time by a tenant workload scaling out. Most people run Istiod as a Deployment with a HorizontalPodAutoscaler (HPA). You could mitigate the scaling issue by setting a high minimum for HPA, butdoing so leads to low resource utilization at night and weekends, at odds withthe very purpose of autoscaling. Moreover, istiod is still at risk when thetenants scale out aggressively. Solution: Use discoverySelectors to watch in-mesh Namespaces onlyThe discoverySelectors configuration enables us to dynamically restrict theset of namespaces that are part of the mesh. The discoverySelectorsconfiguration declares what Istio control plane watches and processes. Not alltenant namespaces enable istio, so istiod could benefit from having to processless k8s events. Fine-tune HPAThe default scale-up stabilization window is 300 seconds. We should reduce it to10 seconds to be more responsive, but keep the scale-down stabilization windowat 300s to avoid threshing.    12345678910111213141516171819202122232425262728293031 apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata:  name: istiod  namespace: istio-system  labels:   app: istiod   release: istio   istio. io/rev: system   install. operator. istio. io/owning-resource: unknown   operator. istio. io/component:  Pilot  spec:  maxReplicas: 48- minReplicas: 32+ minReplicas: 3  scaleTargetRef:   apiVersion: apps/v1   kind: Deployment   name: istiod+ behavior:+  scaleUp:+   stabilizationWindowSeconds: 10s # default is 300s+  scaleDown:+   stabilizationWindowSeconds: 300s  metrics:  - type: Resource   resource:    name: cpu    target:     type: Utilization     averageUtilization: 65 Distribute istio-proxy connections across Istiod PodsIstio doesn\u2019t explicitly set a default maximum connection time betweenistio-proxy sidecars and istiod. Typically, the connections from the sidecars toistiod are long-lived gRPC connections used for service discovery, configurationupdates, and certificate rotation, and they are expected to be maintained aslong as istiod and the sidecars are running. This creates uneven distribution ofloads on istiod Pods over time. One idea is to set a max connection idle timeout for the istio-proxy to istiodconnections, so the proxy will reconnect over time, hopefully landing on a newistiod Pods.    12345678910111213141516171819202122apiVersion: networking. istio. io/v1alpha3kind: EnvoyFiltermetadata: name: istio-proxy-to-istiod-timeouts namespace: istio-systemspec: workloadSelector:  labels: {} configPatches:  - applyTo: HTTP_ROUTE   match:    context: SIDECAR_OUTBOUND    routeConfiguration:     vhost:      name: istiod. istio-system. svc. cluster. local:443   patch:    operation: MERGE    value:     typed_config:      '@type': type. googleapis. com/envoy. extensions. filters. network. http_connection_manager. v3. HttpConnectionManager      common_http_protocol_options:       idle_timeout: 300s "},{id:32,url:"/eks-sg/",title:"Work Around Max Count of Security Group Rules on EKS",body:"2023/09/26 - AWS EKS on VPC networks need AWS Security Group Rules (SG) to receipt ingresstraffic. But what if you reach the max rules count in your SG? Background: LoadBalancer-type Service and Security Group Rules: Kubernetes users can expose a Service in two ways:  Register with the Istio ingress gateways\u2014the golden path for most tenants Create a dedicated LoadBalancer-type Service object, which tells the cloudprovider to create a load balancer and set up health checks. EKS recommendsaws-load-balancer-controllerto react to updates to LoadBalancer-type Service objects and set up NLBaccordingly. For example, suppose a Service object exposesports 80 and 443, the controller will create five Security Group (SG) Rules onEKS worker Nodes:  allow ingress source 0. 0. 0. 0/0 to the correspondingNodePortfor port 80 allow ingress source 0. 0. 0. 0/0 to the corresponding NodePort forport 443 allow EKS zonal subnet in us-west-2a to ingress to the health-checkNodePort.  allow EKS zonal subnet in us-west-2b to ingress to thehealth-check NodePort allow EKS zonal subnet in us-west-2c to ingress to thehealth-check NodePortNote: health-check will fail if a) the Node does not host any targetPods or b) none of the target Pods on this Node is ready, determined by thePod\u2019s readiness probe The SG Rules are added to an SG attached to all worker Nodes in the given EKS. Security Group Limits: For each AWS account, there are two quota limits on Security Groups:  Max number of inbound rules per SG Max number of SGs per network interfaceThese limits can be adjusted subject to the constraint that the product of thetwo quotas cannot exceed 1000 (AWSdoc). It means a network interface can not have more than 1000 SG rules. Problem: Once your EKS cluster approaches the limit of SG rules, it restricts yourability to create new load balancers. It means you won\u2019t be able to performblue-green upgrade of the load balancer, because you need to provision two setsof load balancers simultaneously. The lack of headroom also means you can nolonger onboard more applications that requires a dedicated load balancer. Solutions: The following solutions are not mutually exclusive. They can be used together. Second dedicated SG for each node pools: Suppose your current setup is that all worker Nodes, regardless node pool, has ashared SG attached named \u201cworker\u201d. The aws-load-balancer-controller adds newrules to the \u201cworker\u201d SG. You can keep the shared \u201cworker\u201d SG to store common rules but create a new SG foreach node pool, and use the new SG for NLBs ingress. You need to change the nodepool launch template to attach the new SG. If you decide to continue letting the AWS LB controller manage SG rules for us,you should tag the new SG with kubernetes. io/cluster/{{ . ClusterName }}:shared. This is necessary when there are multiple security groups attached toan ENI, so that the controller knows which SG to add new rules to. Because theexisting \u201cworker\u201d SG has this tag already, we need to create a duplicate SG, say\u201cworker2\u201d, which does NOT have the SG tag for NLB. Then, we will attach to thenode pool the \u201cworker2\u201d SG and the per-pool SG. Optimize SG rules outside of aws LB controller: Recall the aws-load-balancer-controller implementation creates 5 inbound SGrules per envoy-ingress Service. We can optimize this by managing the SG rulesourselves and asking the controller to skip SG rules creation. We can reduce theneed to 2 inbound SG rules per envoy-ingress Service. Add theservice. beta. kubernetes. io/aws-load-balancer-manage-backend-security-group-rules:false` annotation to the LoadBalancer-type Service object. Documentation aboutthis annotation ishere. Reserve 3 static NodePorts for each Service. One for NLB to health check the EKS nodes. One for frontend port 80. One for frontend port 443. You canchoosea static healthCheckNodePort if you set externalTrafficPolicy: Local (whichcomes with the benefits to preserve source IP address). The two regularNodePorts can be static regardless. The two regular NodePorts should be consecutive, so one SG rule can coverboth. The healthCheckNodePort does not need to be consecutive, because thesource IP range in the SG rule is different (i. e. only allow NLB to healthcheckthe nodes). Consider the following example:    12345678910111213141516171819202122232425262728 apiVersion: v1 kind: Service metadata:  annotations:   external-dns. alpha. kubernetes. io/hostname: acmecorp. com   service. beta. kubernetes. io/aws-load-balancer-nlb-target-type: instance   service. beta. kubernetes. io/aws-load-balancer-scheme: internet-facing   service. beta. kubernetes. io/aws-load-balancer-type: external+  service. beta. kubernetes. io/aws-load-balancer-manage-backend-security-group-rules: false  name: myapp  namespace: myapp spec:  externalTrafficPolicy: Local+ healthCheckNodePort: 30218  ports:   - name: https+   nodePort: 30212    port: 443    protocol: TCP    targetPort: 8095   - name: http+   nodePort: 30213    port: 80    protocol: TCP    targetPort: 8089  selector:   app: myapp  type: LoadBalancer The optimized SG rules would be:  allow ingress source 0. 0. 0. 0/0 to the corresponding NodePort for port 80 allow ingress source 0. 0. 0. 0/0 to the corresponding NodePort for port 443   allow source 0. 0. 0. 0/0 to ingress to NodePort range from 30212 to 30213   allow EKS zonal subnet in us-west-2a to ingress to the health-check NodePort allow EKS zonal subnet in us-west-2b to ingress to the health-check NodePort allow EKS zonal subnet in us-west-2c to ingress to the health-check NodePort allow EKS VPC network in region us-west-2 to ingress to the health-check NodePortRaise max inbound rules per SG by reducing SG count per ENI: The solution picks a different point on the trade-off spectrum between #Inboundrules per SG and #SG per ENI. SG quota is set for each and whole AWS account, so any adjustment will affectother workloads in the same account. Thus, we need to verify whether theexisting AWS account has ENI with max number of SGs attached already. Build EKS clusters in a separate AWS account: Building new clusters and shifting tenants over are expensive. Try other solutions first. "},{id:33,url:"/source-ip-autoscale/",title:"Layer-4 Load Balancer & Zero-downtime Autoscaling and Upgrade",body:"2023/08/06 - Your Kubernetes cluster probably has a shared ingress for north-south traffic,coming from a cloud load balancer and lands on your favorite proxies like Envoy,or Istio gateways, or Nginx. If you  use a LoadBalancer-type Service to create a Layer-4 Load Balancerfronting your Kubernetes ingress retain source IP address by setting externalTrafficPolicy: LocalThen horizontal autoscaling (scale-in) and rolling upgrade will incur somedowntime for you. This post  explains why there is partial disruption, and how much disruption to expect discusses several options to achieve zero downtime upgrade and autoscalingFor simplicity, the rest of the doc assumes Envoy as the ingress gateway. Background: Layer-4 cloud load balancer: The routing of traffic to Envoy is facilitated by a layer-4 (L4) cloud loadbalancer, known as Network Load Balancer (NLB) in AWS terminology. Theaws-load-balancer-controllerprovisions such load balancer (LB) by watching LoadBalancer-type Service objectsin Kubernetes. Each Service object opens dedicated NodePort on all Nodes inselected Envoy node pools. Traffic to Envoy will first be routed to NodePort onthe Node hosting Envoy Pod, then DNAT-ed (iptables) to the Pod on the same Node,as shown in the following diagram.   (image source) The LB periodically check the HealthCheck NodePort. The HealthCheck NodePortwill fail if  the Node does not host any target Pods, or none of the target Pods on this Node is ready, determined by the Pod\u2019sreadiness probeexternalTrafficPolicy: Local: Suppose the Kubernetes Service object is configured with externalTrafficPolicy: Local. Then, the kube-proxy directs packets exclusively to Envoy Pods residingon the same Node, even if there are other Nodes running Envoy. This setup hastwo benefits: one less hop (lower latency) and preserving source IP address (forallowlist or rate limiting). But externalTrafficPolicy: Local is problematic during rolling upgrades orscale-in. The reason is that traffic arriving at NodePort will be dropped bykube-proxy if node has no ready Envoy Pods. LB will keep forwarding traffic tothis Node until LB detects the HealthCheck NodePort is failing. Then, LB willmark the Node as unhealthy. There is a certain delay between two key events in this setup:  An Envoy Pod becoming NotReady (for example, if it enters the \u201cTerminating\u201d state during a rolling upgrade).  The subsequent periodic health check carried out by the load balancer. During such delay, client traffic to this Node is blackholed.   (image source) Partial downtime during upgrade and autoscale-in: Why is there some downtime: As discussed in the previous section, client traffic to an Envoy Node isblackholed during the time between the envoy Pod on such Node enters theTerminating state and the LB performs the next health check. Kube-proxy will remove forwarding rules from NodePort to the Pod once the Podenters the Terminating state. Kubernetes 1. 24 and 1. 25 considers theTerminating state as not ready. For the same reason, horizontal scale-in will also cause downtime. For a while,I was just running Envoy as a DaemonSet on a node pool that does not autoscale. Why is the downtime partial: This downtime only affects one Node at a time, because currently, EnvoyDaemonSet has the following upgrade strategy:    12345 updateStrategy:  type: RollingUpdate  rollingUpdate:   maxUnavailable: 1   maxSurge: 0 Thus, Kubernetes will terminate one Pod at a time, then create new Pod on thesame Node. There are 6 Pods in each DaemonSet, so not all envoy Pods are down atthe same time. The reason for maxSurge: 0 is that envoy-ingress Pods run on host networking. It means we cannot have 2 envoy Pods running on the same Node, because they bothbind to the same ports. Thus, the current update strategy is to kill a Pod, thenstart a new one. Why host networking: Running Envoy in host networking means traffic bypasses the Pod overlay network(Normally, each Pod runs in separate network namespaces). Thus, host networkingreduces the overhead of network hops and encapsulation due to overlays. Thisresults in lower latency and higher throughput. But how much performance gain exactly? It depends on many factors like hardwareand bandwidth. Cilium did somebenchmark\u2014take this marketingwith a grain of salt\u2014that suggests host networking could improve throughput by20% and latency by 25%. They didn\u2019t say how many iptables rules (which scalelinearly) are on the given hosts. How much downtime: After NLB detects in its target group an unhealthy instance, NLB will stopcreating new connections to that target. However, existing connections are notimmediately terminated until a default 300s of draining timeout, or RST byclients or Envoy. Thus, in the worst case, the blackhole period per Pod is 310seconds. In practice, the startup time of a new Envoy Pod on the same Node will beshorter than 300s. NLB continues health-checking the unhealthy Node, and willmark the Node as healthy as gain once the new Pod is ready. But for theworst-case analysis, let\u2019s assume the blackhole period per Node is 310 seconds. Given 6 Nodes, Envoy DaemonSet will exhibit a 16. 7% error rate for a totalof 310 * 6 seconds, which is 1860 seconds, or 31 minutes in the worst case. The 16. 7% error rate comes from the fact that 1 of the 6 Pods are in theTerminating state. Still, 16. 7% is an appropriation, because another downsideof externalTrafficPolicy: Local is that connections may not distribute evenly,especially if there are long-running connections on the Terminating Pod. NLBdoes not support the least-connections load balancing scheme. Solutions: Use Pod IPs as LB backends: In this case, NLB sends traffic directly to the Pods selected by k8s Service. The benefits are:  Eliminate the extra network hop (NodePort) through the worker Nodes Allow NLB to keep sending traffic to Pods in the Terminating state but markthe target as DrainingAWS load balancer controller supports this feature natively with \u201cNLB IP-mode\u201d. On other cloud, you can implement such controller yourself, watching Pod eventsand reconcile with L4 LB target groups. To enable IP-mode, we just need to update the Service annotations    1234567891011-service. beta. kubernetes. io/aws-load-balancer-nlb-target-type: instance+service. beta. kubernetes. io/aws-load-balancer-nlb-target-type: ip# Health check the Pods directly+service. beta. kubernetes. io/aws-load-balancer-healthcheck-protocol: http+service. beta. kubernetes. io/aws-load-balancer-healthcheck-port:  9901 +service. beta. kubernetes. io/aws-load-balancer-healthcheck-path: /ready# NLB with IP targets by default does not pass the client source IP address,# unless we specifically configure the target group attributes. +service. beta. kubernetes. io/aws-load-balancer-target-group-attributes: preserve_client_ip. enabled=true To achieve zero-downtime upgrade, we need to additionally configure on the envoyPod a preStop hook like below. When Pod enters the Terminating state, k8s willexecute the preStop hook and keep the Pod in Terminating until the preS`tophook completes.    12345678910111213141516# We must define a longer terminationGracePeriodSeconds, which by default# is 30s, upon which the Pod is killed even if preStop has not completed. terminationGracePeriodSeconds: 305containers: - name: envoy  lifecycle:   preStop:    exec:     # The default target group attribute     # \u201cderegistration_delay. timeout_seconds\u201d is 300s, configurable     # through Service annotation.      command:      - /bin/sh      - -c      - curl -X POST http://localhost:9901/healthcheck/fail &amp;&amp; sleep 300 By failing the envoy health check but keeping envoy running in the Terminatingstate, envoy can still process traffic. Once NLB deems the Envoy Pod unhealthy,it halts new request routing to the Pod but maintains existing connections. Consequently, active TCP connections persist, with client requests continuing tothe now-unhealthy NLB target (Envoy Pod) until either client or envoy closes theconnection or idle timeout expiry, defaulting to 300 seconds for NLB. ProxyTerminatingEndpoints: ProxyTerminatingEndpointsis a new beta feature in Kubernetes version 1. 26. It is enabled by default. When there is a rolling update and a Node only contains terminating Pods,kube-proxy will route traffic to the terminating Pods based on their readiness. At the same time, kube-proxy will actively fail the health check NodePort ifthere are only terminating Pods available. By doing so, kube-proxy alerts theexternal load balancer that new connections should not be sent to that Node butwill gracefully handle requests for existing connections.    12345678910111213141516# We must define a longer terminationGracePeriodSeconds, which by default# is 30s, upon which the Pod is killed even if preStop has not completed. terminationGracePeriodSeconds: 305containers: - name: envoy  lifecycle:   preStop:    exec:     # The default target group attribute     # \u201cderegistration_delay. timeout_seconds\u201d is 300s, configurable     # through Service annotation.      command:      - /bin/sh      - -c      - sleep 300 Note that here we must NOT call POST http://localhost:9901/healthcheck/fail onEnvoy, different from what NLB IP-mode needs. The reason is that TerminatingPods need to pass the readiness probe to continue receiving traffic, so wecannot fail the envoy health check. Since kube-proxy will actively fail thehealth check NodePort if there are only terminating Pods available on the Node,NLB will start the draining process. Customize NLB, keep host networking: Forget about the NodePort and HealthCheck NodePort opened by kube-proxy. We cancreate the NLB not through k8s Service object, but using infra-as-code toolssuch as pulumi. This bypasses the kube-proxy. The NLB will look like this       NLB frontend port   Target port (=NodePort =Pod port because of host networking)         443   8443       80   8080   The NLB will find all Nodes running envoy Pods using the autoscaling group forthe envoy-ingress node pool. Yes, we can autoscale with solution 4. 3. This setupis similar to Section 4. 1. 1 NLB IP-mode, except the NLB is not created byKubernetes. We need the following Pod spec change.    12345678910111213141516# We must define a longer terminationGracePeriodSeconds, which by default# is 30s, upon which the Pod is killed even if preStop has not completed. terminationGracePeriodSeconds: 305containers: - name: envoy  lifecycle:   preStop:    exec:     # The default target group attribute     # \u201cderegistration_delay. timeout_seconds\u201d is 300s, configurable     # through Service annotation.      command:      - /bin/sh      - -c      - curl -X POST http://localhost:9901/healthcheck/fail &amp;&amp; sleep 300 We also need to expose the \u201c/ready\u201d endpoint from envoy to the host. Then, we need to update the Service annotations like the following.    1234# Health check the Pods directly through NodePort 9901+service. beta. kubernetes. io/aws-load-balancer-healthcheck-protocol: http+service. beta. kubernetes. io/aws-load-balancer-healthcheck-port:  9901 +service. beta. kubernetes. io/aws-load-balancer-healthcheck-path: /ready "},{id:34,url:"/sales-lessons/",title:"Enterprise Sales",body:"2023/01/01 - How to do product-led growth and hands-on outbound sales at the same time?:  Every PLG company eventually has to embrace enterprise.  \u2013 Annie Pearl, Chef Product Officer at Calendly The upper limit of PLG seems to be $100M to $200M ARR (e. g. DataDog around IPO). Beyond triple-digit million ARR, you quickly saturate the market of users whobuy things by pulling out their credit cards. The growth of the PLG channelnaturally slows at some point. Most companies start with self-serve PLG and then layer in enterprise sales. This may not work well. Your entry-level product could cannibalize yourenterprise product. Think hard about differentiation that justifies the premium. You cannot just add single-sign-on (SSO) and call it enterprise edition. Some common enterprise features are:       Feature   Description         Customer supports   Support is what makes or breaks the enterprise product. Your company success is defined solely by customer success. 24/7 or business hours? Email, chat, or phone support? SLA on response time?       Dedicated account manager   Evangelize best practices and champion support.        Fine-grain authorization policy   Roles and permissions.        Audit logs   User and workload identity, actions, timestamp, easy export.        Compliance   For example, FedRAMP, HIPAA, GDPR, SOC2.        High availability   For example, multi-region disaster recovery, cross-region replication, four-nine uptime SLA.        Private network   Private link, VPC peering, single-tenant deployment.        Self-host   User-managed private deployment       Data retention period   For example, DataDog allows free accounts to view metrics from last 24h       Performance   For example, run the same pytorch model 100x faster.    Another pro tip: within minutes of self-serve signups, directly reach outto that new user, assuming they pass certain criteria about company sizeand title. The reason is, your product is top of mind for the user right now,who clearly has been doing research about this space. If they give you the phonenumber, call them directly. When to layer in enterprise sales?: Similarly, when to make the first sales hire? You need product-market fit (PMF) and annual recurring revenue (ARR). PMF isshown through retention, exponential and organic customer growth, and netpromoter score (NPS, or how upset would you be if this product disappears? Howlikely would you recommend this product to your friends?). To learn more aboutPMF, I love thispostby Lenny Rachitsky. Rule of thumb: you need at least $1M ARR to show you can sell outside of yournetwork (friends and family). Should your first sales hire be head of sales or junior sales rep?: You probably don\u2019t yet need a head of sales because  The target customers and the deal size remain uncertain, so it is hard to knowif a candidate is the right person.  Head of sales is expensive and often commands a structure in marketing,finance, legal, etc. Big overhead.  Head of sales probably has not been down in the weeds for a while. You should not hire anyone junior because it takes your time to train them, suchas how to write emails and pricing proposal. An ideal candidate would be an account executive (AE) at a hypergrowth companywith great track record, or someone recently moved into front-line managementwho used to be an AE and who desires the opportunity to grow into the head ofsales role. Some revealing questions to ask:  Tell me about a deal that you lost.  Let\u2019s do a mock discovery call together.  Out of the 8 SDRs on your team, where did you come up on the leaderboard?What are the folks front-running you doing differently?When do you start outbound?: According to Maggie Hott, Director of Sales at Webflow, the answer isalways now. The goal of cold outbound is not to close deals but to build brandawareness and educate potential customers. You almost never hit theprospects at the right time, but when they are ready to buy, prior outboundsmake you part of the evaluation. Whereas marketing targets a market segment,outbounds are personalized to each prospect. Maggie recommends a 10-80-10 outbound strategy:  First 10% is personalized and explains how the prospect could relate to yourproduct.      For example, \u201cHey John, amazing talk at KubeCon last week with such greatlessons for multi-cloud adoption. I am curious how your team approachesmulti-cloud observability and cost attribution. All the folks I talked toare struggling with this. I think our product can really help. \u201d    Mid 80% is repeatable product marketing.  Last 10% is to ask for that call or meeting, and end with a playful closing. Lauren Schwartz, VP of Sales at Fivetran, stressed the importance of havingmultiple sponsors in the prospect\u2019s organization. People change jobs and havedifferent types of influence. Should we give discounts for testimonials?: In the early days, worry less about the contract size but focus on gettingthose logos. The bulk of the enterprise market is the early majority andlate majority. They are almost never early adopters. They need testimony andsuccess stories. Maybe you could get more testimonials through discounts, but be judicious aboutdiscounts, because people talk, and soon more customers will ask for discounts. The best reason to give discounts is to control the close date and paymentstructure of the deal. You can always bring up the ask for testimonialsif the prospect comes back asking for more discounts. "},{id:35,url:"/more-advices/",title:"More Career Advices",
body:"2022/12/06 - Make sure to check out the previous post: Advices I wish I got at thestart of my career. Ask for help:  Son, your ego is writing checks your body can\u2019t cash.  \u2013 Captain Tom \u201cStinger\u201d Jordan, Top Gun The number one reason why senior people fail is that they do not ask for help. We are all shareholders. You do whatever you can to unblock yourself. It isabout time to market and showing results. Promote thought leadership:  If you want to build a ship, don\u2019t drum up the men to gather wood, divide thework and give orders. Instead, teach them to yearn for the vast and endlesssea.  Antoine de Saint-Exupery, author of The Little Prince In tech, most decisions are based on influence, not hierarchy. Thoughtleadership is a great way to gain influence. To be a leader, you should start byacting like one. Be confident, give tech talks, and voice your opinions inplanning, design review, and postmortems. Doing so makes you the obvious choicefor the next big project or the next leadership roles. The outcome and how you drove that outcome are both important. Be careful withyour reputation. Befriend Jeff Bezos before he gets rich: The best time to become friends with Jeff Bezos is before he becomes rich andfamous. Networking does not mean you must reach upward. Invest in your peers,who are more receptive to getting to know you. Imagine you are Stripe in 2012. Rather than running into walls with F500 enterprises, you should onboardhundreds of startups, because among them are the next Airbnb and Lyft. Relationships compound. Start early. Do informational interviews. Ask people at other firms what they like and don\u2019tlike about their job. Always follow up to maintain weak ties, such as  Saying hello to someone you met at a conference last year. Or asking ifthey\u2019ll be attending after this year\u2019s agenda is published.  Share interesting news about your old company with a former colleague.  Send them news, event, commentary related to their interests. Examples:     \u201cTom, I just read this great white paper on blockchain. I know you\u2019d get alot out of this. \u201d   \u201cAlice, congrats on the new job. Enclosed please find a copy of the bestbook I\u2019ve read on starting a new job,\xa0The First 90 Days. Call me if you wantto compare notes. \u201d   \u201cJames, I just got an invite to a private class at this new gym but can\u2019tgo. You mentioned you love Crossfit \u2014 want my ticket?\u201d   Stop productivity porn. Bias towards action. : Watching others lifting weights is not going to make you fit. Many of us spendso much time collecting books we want to read but haven\u2019t, or studying how othersslice up their days to get more done. You get more done by doing and by startingnow. Develope relationship skills:  A major reason change efforts so often fail is that successful implementationeventually requires people to have difficult conversations \u2026 With everyonetaking for granted that their own view is right, and readily assuming thatothers\u2019 opposition is self-interested, progress quickly grinds to a halt. Decisions are delayed, and when finally made they are often imposed withoutbuy-in from those who have to implement them. Relationships sour. Eventuallypeople give up in frustration, and those driving the effort get distracted bynew challenges or the next next big thing. The ability to manage difficultconversations effectively is foundational, then, to achieving almost anysignificant change.  \u2013 Douglas Stone, Author of \u201cDifficult Conversations: How to Discuss What Matters\u201d Relationship problems are at the heart of every organization. Take productmanagers (PM) and engineering managers (EM) for example. PM &amp; EM haveoverlapping scopes by definition. When you seek better scoping, you don\u2019t have ascoping problem, you have a relationship problem. Here are some tips: Be a good listener. People rarely change. People just want to be understood. Listening is more persuasive than talking. Listening fosters a reciprocalrelationship. Listening is not just about paraphrasing back. Ask deep andrelevant questions, take notes, and maintain eye contact. For many, listeningneeds to be a trained response. When you are frustrated, you are the leastcurious\u2014you have so much noise in your internal head that left little space toworry about what\u2019s on the other person\u2019s head. Learn to lean into the conflicts,just like firefighters learn to run towards the fire. Lean into conflicts. Avoiding conflict is the worst kind of measure ofrelationship health. Staying quiet creates resentment. You must confront, butdo so with skills and preparation. It is mature to share your feeling andinquire about others\u2019. Doing so builds trust. For example: \u201cBob, I feelfrustrated. It seems this conversation is not getting anywhere, and I want tounderstand why. \u201d Acknowledge the differences between the two parties, not who isright or better. I love this example \u201cJill, you and I seem to have differentpreferences about when code reviews should be done. I wonder if that\u2019s somethingwe could talk about?\u201d Resolve email conflicts in person. If a conflict starts on email, it is hardto solve on email. In this case, just meet in person or pick up the phone. It ishard to communicate emotions through email\u2014no tone, no voice, no facialexpression, no body language. Apology diffuses the tension. In most conflicts, blaming does not help. Mostof the time, you share part of the blame, even just 5%. Apologizing andacknowledging the fault on your side could really diffuse the tension. Apologyis an underrated and underutilized skill. Apology needs to be genuine. Saying \u201cIam sorry that you feel that way\u201d is not genuine. Here is how to make a goodapology:  Acknowledge the harm. \u201cI am sorry that I interrupted you in the meeting. \u201d Say why it is wrong. \u201cIt was disrespectful and discourage the full exchange of ideas. \u201d Say what you will do next time, not what you won\u2019t do. \u201cI will make sure tolet you finish before I chime in. \u201d Ask for forgiveness. Bring cupcakes. Go straight to the job you want: Don\u2019t let inertia drive you. Take some risks when you are young. No one in their40s said they took too much risk. If you are unhappy about your job, move on. Every job change you make, youalways wish you make it 6 months earlier. Life is so short. Do not spend time onjobs that you do not like. You will be so productive in jobs you like. Don\u2019tassume that you have to do this job, then get that job and then that job, andthen you can do what you really want. Go direct. Know your alternative. Negotiate hard on your second best offer, then negotiatewith your first choice, knowing what you can walk away towards. Pay attention todetails. Be specific about equity grant date, vesting schedule, etc. What to look for in the next job: Growth: With the right opportunities, you can 10x your impact every decade. Because ofcompounding, what seems to be golden handcuffs today is dwarfed by theopportunity to accelerate growth. The exponential curve actually consists ofmany little S-curves. If you find yourself approaching the flattening end of theS-curve, it is time for a change. "},{id:36,url:"/streamlit-interview/",title:"Interviewing Adrien Treuille, Founder CEO of Streamlit",body:"2022/11/21 - Streamlit, about to raise its Series-C, was acquired by Snowflake for $800M inMarch 2022. In this conversation with Adrien, we chatted about OSSmetrics, licenses, open-core vs freemium vs free trial, PLG vs sales motion,third party contributions, and lessons from building Streamlit. Insights belongto Adrien. Errors and omissions are my own. Given Streamlit is an open-source product, what are the most important metrics you watch for while you build this product? Why?: Open-source telemetry is a gray area in the open-source world. Because thethings that you\u2019d like to track are typically not the things that likeopen-source projects are supposed to track like utilization. There are two kindsof utilization metrics: Indirect measure of utilization: downloads, GitHub stars, and engagement metricson forums (slack, stackoverflow). Direct measure of utilization: which featureswere used when. This is a SaaS-like approach, people don\u2019t always like this Streamlit did the latter. We made it very clear when you install Streamlit thatwe\u2019re going to collect the statistics, and here\u2019s how you turn off the datacollection. We wanted to be good citizens in that regard. This opt-out featuremeans we may not be aware of all utilization patterns. Conversely, we were ableto better visibility into the Streamlit community, such as the monthly activedevelopers and viewers. Active users are trailing metrics, not leading metrics. They don\u2019t really informproduct decisions but are a overall score. You brought up a good point aboutthese metrics are more common in consumer software. Streamlit may consideritself as a B2D company, D as in developers. B2D is not too different from B2C,so I want to optimize for virality and engagement. Taking some members of thecommunity and making them famous is a really key strategy. We were doing allthat stuff like crazy. Have you considered freemium or free trial? What makes open source a better fit for Streamlit?: If you target an existing workload at companies, focus on exactly that customerset, make them as happy as possible, do better than the competition, and youmight not have to open source. HEX is an example, which is saying like, hey,we\u2019re gonna make this about notebook, but it\u2019s like super annoying, so we\u2019regoing to improve on it in like, six, seven ways. However, for Streamlit, it was clear that we were inventing a new workload. Early adopters were usually groups working on super high-tech things that likethey themselves, their processes were so wide open that they could determineeverything we fashion and instrument to work perfectly. These early adoptersconvinced me to start a company. For example, Uber was using Streamlit to figureout where to put chargers for the electric bikes. If you\u2019re inventing newworkloads, then the strategy is you have to become universal, we just had toopen source. Charles: It\u2019s a very similar approach, especially in the infrastructure world,where you really have to be the de facto standard. Thus, you need to earn users\u2019trust so they\u2019re willing to invest in this platform to get the kind of snowballeffect rolling. Adrien: Exactly. It\u2019s all like famous for being famous. Streamlit uses an Apache2 license. Have you considered MongoDB and Elastic\u2019s licensing model? Why not?: We could always transition to a model like that. Mongo is an example of acompany that changed licenses. But the truth is that we never really got to apoint where the Apache2 license was an issue, as we were out there trying to winthe community. We did in some ways pull away from the pack of people who weredoing similar things two or three years ago. Our next big challenge was to monetize. We had a theory for how to do so, thoughit was certainly not proven. We were literally onboarding our first payingcustomers, when snowflake approached us for acquisition. And we said noactually, because we had great term sheets from amazing investors and we had therevenue. Snowflake said, we don\u2019t want you to figure out how to make money,because if you do, you\u2019re gonna get way too expensive. Snowflake matched ourterm sheet valuation and went over a bit to catch the projected revenue. Theterm sheet we had was to raise $95 million, which buys years of runways, so wewould have figured out the business problems along the way. Is it possible to do both PLG and sales motion at the same time?: Charles: Integration is always a challenge with any acquisition. Specifically,Streamlit started as an open-source project, and it\u2019s about to get intomonetization with product lead growth, which is different from snowflake\u2019ssales-driven model. How do we best integrate the two products together? Adrien: The quick answer is yes. The question is, what does that actually look like? I think what it looks like is perhaps less PLG. For me, true PLG looks likethis: we\u2019re gonna convince you to pay up like $1,000 a year for us, and thenbefore you know it, you\u2019ll be paying like a million dollars a year for usbecause we\u2019re just gonna prove our worth to the entire organization and theadoption growth is bottom-up. Our ambition at snowflake is not to turn Snowflake sales motion into a PLGmotion but to piggyback on snowflake\u2019s unbelievably successful sales motion. What we can be is beloved by developers and be a reason why a deal cuts inSnowflake direction. This is the lower ambition. The higher ambition is theabove plus driving a ton of credit consumption and indulgence. Snowflake andStreamlit have a lot of joint users. If the next prospect, who is doingdiligence on Snowflake, asks internally that Snowflake comes with this Streamlitthing, who has heard of it? And the data science teams all say that would beawesome. This totally goes for snowflake, right? And then all of a sudden, amassive workload moves over to snowflake. That is success. Whether you call itPLG or not, I think it\u2019s completely compatible with Snowflake\u2019s sales motion. How do you prioritize community feature requests vs your product roadmap? What to do with voluntary and unsolicited contributions?: Charles: I have this question because I get common feedback from open-sourcemaintainers that contributions from individual community members are great, butonce we accept their contributions, we have to maintain the features goingforward in all future releases. By then the original contributors are gone. However, rejecting their contributions would be such a blow to their love ofyour products. Would you prioritize differently based on the feedback andcontributions you got? Adrien: The good news about your story about the contributor release is that inpractice, it never works that way. For all the serious open-source projects thatI know of, there are no nontrivial yet drive-by contributions. The actualso-called community contributions are more things like adding a comma in README. For Streamlit, if someone wants to merge a fundamentally new feature that letsyou, for example, parse URL parameters and do whatever, we would just take alook at it and say no, because it was not in our roadmap, and it wasn\u2019t the waywe would have done it. We are not gonna let people check random things intoStreamlit. But, the thing that the community does do extremely well, which is kind ofevergreen in its own way, is to provide a ton of IP around the project. Forexample, every StackOverflow answer and every example code in the public repos. GitHub Copilot writes fantastic Streamlit code, which is amazing. You canliterally add a comment like show Yahoo stock pricing stream, and Copilot poppedout a beautiful app. All those are community contributions. What remains the biggest challenge in data infra?: Streamlit is just like one piece of a huge ecosystem of data infrastructure, allof which is changing really quickly. Whether snowflake keeps up with and leavesthe pack is a question that far transcends Streamlit. We are just going to playa role in a positive direction. In many ways, the promise is still ahead of us,in the sense that the actual number of companies\u2014that are really committed tousing us and have like amazing results\u2014wasn\u2019t that big, but those that did gotreally solid results. The challenge is replicating that experience, like 10x100x 1000x. If you were to start Streamlit again, what would you do differently?: The biggest mistake I had was not hiring leaders fast enough and growing theorganization\u2019s maturity. I was worried that a hiring mistake could backfire forus, but in reality, the executives we hired worked out extremely well. When Iinterviewed them, I wished I had met them sooner because they could truly bringlots of value to the team. They really increased the execution velocity bymaking the organization scalable. "},{id:37,url:"/k8s-net/",title:"Kubernetes Networking From the First Principles",body:"2022/03/01 - We go from containers and network namespace to Pod-to-Pod, Pod-to-Service, andexternal-client-to-Service networking. Pods: Containers of the same Pod share the same Linux network namespace, isolated fromthe host network namespace. Each Pod gets a separate network namespace and is assigned a cluster-wide uniqueIP address from the cluster\u2019s Pod CIDR range. Many managed Kubernetes offeringsuse Host-local IPAM (IP Address Management), so that each Node is fist assigneda subnet of Pod CIDR. Then, each Pod gets its IP address from the Node the Podis on. The Kubernetes networking model requires that a container in Pod A can reach acontainer in Pod B, crossing network namespaces, regardless of whether Pods Aand B are on the same Node or not. Pod to Pod on the same Node: For each Pod, kubelet will create a VETH (Virtual Ethernet Device) pair in thehost network namespace. Packets transmitted on one device in the pair areimmediately received on the other device. Then Kubelet will move one device ofsuch pair into the Pod\u2019s network namespace and rename this device to eth0 inthe Pod\u2019s namespace. Each VETH device remained in the host network namespacewill be assigned a Pod IP and be connected to a software bridge cbr0.   Pod to Pod on another Node:  On Node-1, IP packets (whose source and destination addresses arethe Pod IPs) sent by pod-1 will be encapsulated as Ethernet framesbeing sent to cbr0. The cbr0 switch has a match-all forwarding rule for all packets destinated toanything but Node-1\u2019s subnet of Pod CIDR. The CNI plugin in VXLAN mode willencapsulate the Ethernet frames as UDP packets. These UDP packets\u2019 source anddestination addresses are the Node IPs. ClusterIP-type Services: Even though Pod IPs are routable, Pods (and hence the Pod IPs) are ephemeral bydesign. Hence, it is more reliable and recommended to use the KubernetesService, which provides a static cluster IP and load balancing over a group ofPods. The most basic Service type is ClusterIP, which represents a serviceavailable within the cluster but not exposed to the internet. ClusterIP-type Services are implemented using kube-proxy, which is not a realproxy (data plane) but configures iptables to capture and NAT traffic to clusterIP of the Service. Below is an example of pod-1 sending a request to service-1 backed by pod-0and pod-2. Encapsulation details covered in the previous section are omittedfrom the diagram below.   Kube-proxy will choose at random one of the backing Pods to serve the request,by dNATing the destination IP from the Service\u2019s cluster IP to the IP of thechosen Pod, which in this example is pod-2. Note that the response frompod-2 will be sNATed back to the Service\u2019s cluster IP, so that kube-proxyremains transparent to workloads. Otherwise, pod-1 only has connection statesabout service-1, not pod-2, and thus will reset the connection. NodePort-type Services: To expose a service to clients outside of the cluster, use the NodePort-typeService, which reserves the same port on each Node such that the client couldaccess the service by hitting the NodePort on any Nodes. The example below assumes externalTrafficPolicy is set to Cluster, whichmeans traffic can be routed to a backing Pod on a different Node. Here, thecluster-external client send requests to the NodePort of service-1 on Node-0,and kube-proxy chooses pod-2 to serve the request.   Obviously the destination address will be masqueraded from Node-0 to pod-2,but notice that source address is also masqueraded, from the client IP toNode-0\u2019s IP. Source-NATing is necessary, because otherwise pod-2 will responddirectly to the client, who assume it is maintaining a connection to Node-0. "},{id:38,url:"/accounting/",title:"Accounting Advice for Founders",body:"2021/08/05 - Notes derived from a guest lecture by Danny Wallace, Partner at PwC\u2019s SiliconValley practice. For informational purposes only. Errors and omissions are my own. Collect sales tax from early on: Since the South Dakota v. Wayfair supremecourt case, businesses without a physical presence in a state but with more than200 transactions or $100,000 in-state sales should pay sales taxes in the state. Many startups did not collect sales tax at all, a mistake that will defer IPOand M&amp;A deals. Fixing the sales tax issue is expensive for 2 reasons:  You need to pay sales tax in arrears.  Starting to charge sales tax will hurt your growth numbers and hencevaluation. Sales tax is an issue that comes up on every M&amp;A deal. By all means, the companyneeds to get registered for sales tax and should seek professional advice. Classify operating expenses cleverly, reduce G&amp;A: Operating expenses are not created equal. We prefer R&amp;D (research anddevelopment), and maybe S&amp;M (sales and marketing), over G&amp;A (general andadministrative). Take advantage of the reporting flexibility of expenseclassification. For example, rent in the Bay Area is high, and some startups putrent under G&amp;A, but we could allocate the rent to different items. Suppose youhave one floor for G&amp;A, one floor for R&amp;D, and one floor for S&amp;M, then we canonly report a third of the rent under G&amp;A. Also, there are opportunities to capitalize the R&amp;D expenses so they do not hitthe income statement and go straight to the balance sheet and depreciate overtime. Do 409a valuation every year and proper tax withholding: You must do a 409a valuation at least once a year, which will qualify for a safeharbor for the next 12 months. Otherwise, the IRS may challenge the tax statusof the stock rewards to employees, and in turn, employees may sue the companyfor failure to perform 409a valuation properly. Grant the stock rewards at the fair market value, or there will be tax consequences. The company needs to issue 1099\u2019s to the contractors. If the company does not,IRS will treat the contractors as employees. As the business expands globally,you also need to consider international taxes and work with professionals todesign a global tax strategy. Watch out for internal fraud: Especially in the seed and early stages, startups are plagued with internalfraud and misappropriation of the company\u2019s assets. Two-thirds of all US-basedsmall businesses fall victim to employee theft. Common schemes include expensingtrips that never took place, claiming personal expenses as business ones, andfalsifying sales for commissions. Fraud is common at this stage because companies often have no formal processesin place, and operators are too busy with getting the business off the ground. The companies should minimize the number of corporate credit cards and requireapprovals for all purchases and transfers above a certain threshold. At the sametime, strengthen the hiring process with reference checks and background checks. Set up a whistleblower hotline. Always verify communication with a second means: External fraud is also prevalent, with the most common one being phishing emailsfor wire transfer. Examples of spoofed emails are the CEO asking emergency wiretransfer for a confidential deal, or a vendor requesting for updating bankinformation. Or the hackers may fake a login page to get your internal systemscredentials, risking your intellectual property and employee data. Spoofingcould also be in physical mails. You should always verify the intent with a second means, such as calling thealleged sender directly and set up multi-factor authentication. Think twice before publishing key metrics, protect data integrity: Besides items from financial statements, a company may choose to publish keymetrics specific to the business, such as monthly active users, number of payingcustomers, net retention rate, number of completed trips, etc. You need toclearly define and consistently measure the key metrics. Once you publish suchmetrics, you need to continue doing so going forward. Data integrity and historyare crucial. Investors will sue for any misrepresentation of the metrics. Similar to how you should prevent internal fraud, make sure you have internalcontrol in place. Own your revenue model: ratable or non-ratable, net or gross: There is a trade-off between the ratable and non-ratable revenue models. Ratablemeans proportional. Ratable models recognize service revenue over time asservices are provided. The revenue schedule is defined in the contract elements. Ratable models are more predictable and offer more visibility into the company. For example, SaaS companies with mouthy subscriptions can provide great guidanceto investors with a high degree of confidence. However, ratable models are lesstweakable. For example, at the end of the quarter, if you risk missing theforecast numbers, you cannot just send salespeople out to get deals done on thelast day of the quarter. Even if you do so, you recognize only one day ofrevenue. Another dimension of the revenue model is gross vs net, especially formarketplace and platform businesses. For example, Uber follows the net revenuemodel. If Uber collects $10 from a rider, pays the driver $8, pockets $2, Uberwill recognize $2 revenue instead of $10, and $0 COGS instead of $8, because itsterms and conditions say it is a platform company connecting drivers and riders. "},{id:39,url:"/ip-law/",title:"Intellectual Property and Entrepreneurship",
body:"2021/05/06 - Notes on Intellectual Property (IP) law for founders and busy professionals. Not legal advice. For informational purposes only. Laws can change, so thisarticle may contain dated information. Always consult an attorney. The big 4: Patents: Patents protect novel, useful, non-obvious inventions. With full disclosure ofthe innovation, a patent grants the right to exclude others from making, using,selling, offering to sell, or importing the invention. It does not require itsowner to use the patent. Copyright: Copyright protects original works of authorship fixed in any tangible medium ofexpression, such as source code, art (painting, photographs, acting), andsculpture. Copyright does not protect functionality but only the expression. The author mustindependently create the work (no copying) and display some level of creativity. Copyright is secured automatically when the work is created. Copyright owners have the exclusive right to create copyrightable derivativeworks, unless others obtain a licence from the owner. However, fair use isoften a defense against infringement claims. Reverse engineering of competingproducts is fair use, but you can protect against reverse engineering usingcontracts. There are 4 factors to evaluate a fair use defense. Talk to yourattorney. Trademark: A trademark is a word/phrase/logo that identifies the source of goods. Toregister as a trademark the name of a well-known political figure and celebrity,their written consent is needed. Colors can be a trademark. Think Tiffany blue. Trade secrets: Trade secrets protect confidential company information that is commerciallyvaluable and not publicly known, such as the Coca-Cola Coke formula. They do notrequire any registration with government agencies. Patent applications areconsidered trade secrets by the USPTO until their eventual publication. The trade secret owner must have demonstrable intent to maintainconfidentiality (use NDAs). Trade secrets can be licensed. Ideas are not IP: IP law only protects the embodiment (expression, implementation) of ideas. Becauseideas are not IP, they are not proprietary (i. e. cannot be owned), but they canbe confidential (i. e. secrets). However, ideas may be protected with contracts, such as nondisclosure agreements(NDA) or payment agreements. California law might imply a contract in certaincases, but a contract will not be implied if the disclosure occurs before it isknown that payment is a condition of use. Nondisclosure agreements: Without an NDA, discussion of any invention is considered public disclosure,which prevents you from filing patents and voids your trade secrets. VCs do notsign NDAs. Hence, when pitching to VCs, 1) focus on the business and market, and2) file a provisional patent application to lock in the priority date beforetalking to anyone who is not an inventor. Like all agreements, an NDA must state the parties bounded by the agreement withdate and signatures. It should identify the information discloser and receiver. It should describe with sufficient particularity the information to bedisclosed. Saying that the NDA covers \u201ceverything provided\u201d is likely to be notenforceable. The NDA should state the intended purpose and limitations of the use ofthe disclosed information (e. g. not a license to IP and cannot be used todevelop any product). Make sure to mark confidential documents as such. Definethe duration for disclosure exchange and for confidentiality preservation. Declare which state or country law governs the interpretation of the agreement. Avoid the \u201cresiduals\u201d provision, which says it is okay to use things rememberedin memory. Patents: Utility patents have a 20-year life from the application date and cannot beextended. Patents are granted to the first to file. Laws of nature (theorem,algorithm) and products of nature (animals) are not patentable. The patentowners, not the government, police their patent rights. A provisional application is recommended not only because it secures an earlierpriority date but also because filing patents is expensive (thousands of dollarsbefore lawyer fee) and time-consuming (takes about 25 months to issue), so the1-year grace period allows the inventor to evaluate the business strategy todecide if patent filing is worth it. Patent protection does not start until thepatent issues. A patent requires the invention to be novel. An invention is not novel if it isanticipated (identical to a prior art). Public disclosures\u2014talking tononinventors, publication, selling, offering to sell\u2014count as prior art. So again, NDAs and provisional applications are recommended.  That which will infringe, if later, will anticipate, if earlier.  \u2013 Peters v. Active Mfg. , 21 F. 319 Patents are always issued on Tuesday. Why? Because the USPTO said so. Patent claims must be written in one sentence. Why? Because the USPTO said so. A patent licensee may challenge the patent (e. g. that it was anticipated) whileretaining a licence to the patent. If you lose the patent challenge, you stillhave the licence. AI cannot be an inventor, who must a human being. Employment agreements: An employment agreement likely includes an IP assignment agreement, meaning IPcreated within the scope of employment is owned by the employer. The employerowns the code written by its engineers, but not the code written by janitors. Always include such assignment agreement, essentially for contractors. Do notuse the phrase \u201cwork made for hire\u201d, which in California makes a contractor anemployee. The assignment agreement will protect the company in case someemployees leave and start their own thing using the same IP. California safe harbor: The assignment agreement with the employer does not apply toinventions an employee developed entirely on their own time withoutusing the employer\u2019s equipment, supplies, facilities, or trade secret. However, exceptions are  (1) Relate at the time of conception or reduction to practice of the inventionto the employer\u2019s business, or actual or demonstrably anticipated research ordevelopment of the employer; or  (2) Result from any work performed by the employee for the employer.  \u2013 California labor code section 2870 (a) It is always a peril when someone is still employed elsewhere while also workingon the startup. This person probably also signed IP assignment agreement withthat other employer, and exception (2) above makes things really tricky. Ideally, have a clean cut. Quit former employer, start the company, thendevelop. This issue always comes up during due diligence when you raise funding orseek exits. Ownership and multiple creators: For every type of IP, the default owner is the creators:  patents: inventors copyrights: authors trademark: first user in commerce trade secrets: creatorsThe legal default is that creators are the joint owners of equal rightsregardless of the disparity in contribution. Equal ownership can only be shifted byassignment agreements. Among co-owners, licensing rights are often the most contentious. To avoiddisputes, one option is to divide up the right so that each owner will haveexclusive rights to grant licenses only in a particular industry or country. In community property states, such as California, spouses have ownership rightsin IP created by the other spouse during the term of the marriage. As shown below, there are unique ownership issues for different types of IP. Patents: Absent written agreement, all inventors can each independently license to thirdparties without any duty or notice to co-inventors, perhaps with inconsistentterms. Hence, co-owners could compete with each other. Copyrights: Absent written agreement, each owner can independently license to third partieson inconsistent terms, but contrary to patent law, all co-owners have a duty toeach other to provide financial accounting and share the proceeds fromlicensing. Such duty can be changed with written agreements. Trademarks: It is the business, not the creators that own the trademark. The artisticappearance of the trademark might be protected by copyright. Do not use the logounless the copyright is assigned or licensed to the company, especially when thelogo is produced by a contractor. Open-source software (OSS) licensing: The following 6 OSS licenses cover 99% of the use case. Copy-left licenses (free software):  GPL LGPL Eclipse Public License, CDDL, Mozilla Public License (these 3 are effectively identical)Permissive licenses:  BSD MIT Apache 2Permissive licenses allow the user to do whatever as long as the license header is kept. Copy-left licenses require that if you distribute your software in binary form,you must make the corresponding source code available to binary recipients andonly license your code on the same copy-left licensing terms. Distribution is the key here because it is a right under the US copyright law. Distribution means transferring a copy from a legal person to another. Absentdistribution, most OSS licenses impose no condition. For most licenses, SaaS isnot considered distribution, but pay attention to the recent trend of adoptionof the Server Side Public License. Copy-left licenses have different levels of restriction as shown below. GPL - strong copyleft: if any code in a program is GPL, it must all be GPL. This means no proprietarycode in a GPL codebase and no linking to proprietary code, because linking is aprocess that creates executables, as opposed to ways of inter-processcommunication. In a GPL codebase, code may come from other GPL compatiblelicenses, such as permissive licenses. Everything in kernel space is GPL. Proprietary code in the kernel space isrisky. Application code that communicates with the kernel through syscalls can beproprietary. LGPL - library copyleft: If any code in a library is LGPL, then the entire library is LGPL. However, youcan dynamically link to proprietary code, which means you need to share changesto the library but not the code using the library. Static linking to proprietarycode might be permitted but most companies decided not to after talking to theirattorneys. Note that scripting languages basically work by the equivalent ofdynamic linking for the purpose of this analysis. Mozilla, Eclipse, CDDL - weak copyleft: With no rules about linking, these licenses are friendly to proprietary code,which should be kept in a separate file. If you did not modify the licensedcode, you easily comply. If you did modify, a simple legal review can help youcomply. "},{id:40,url:"/uncertain/",title:"Life and Investment Through the Lens of Uncertainty",body:"2021/01/03 - Disclaimer: Opinions are my own. Not investment advice. Decisions are not judged by results: Decisions and results are separate. The quality of a decision should not bejudged by the quality of its result, which often depends on randomness. If I drove drunk yet got home safely, I have made a bad decision with a goodoutcome. If I drove sober and careful but was rear-ended, I have made a gooddecision with a bad outcome. If I rushed into $NIO upon open, dumped before close, and netted some decentgain, did I make a good decision? So much of one\u2019s success was attributed to theresearch and painstaking, often confusing luck with skills, and signal withnoise. In the last 20 years, $SPY has a nontrivial mean daily return of 0. 0338% but astandard deviation of 1. 2405%, which is 37 times the return. Holding it for justa day is really speculating as opposed to investing. We always make decisions under uncertainty, but our weapons are legions\u2014probability, expectation, and Monte Carlo. Vanish of randomness: Suppose we treat each daily return R1, R2, . . . , Rn as a random variable fromthe same distribution\u2014any distribution, especially not Normal. Suppose thedistribution has a mean mu and standard deviation sigma. Then, the sum ofR1, R2, . . . , Rn has mean n * mu and and standard deviation sqrt(n) * sigma. In layman\u2019s terms, the longer you hold a stock, the narrower the distribution ofits return. If you end up with a spike, it means you almost always make money. This analysis is simplified and assumes the underlying distribution does notchange. The real distribution is not observable, but with so many pension andretirement funds in the market, your investment positions are also a reflectionof your confidence in the federal government. Uncertainty has existed in the past: Uncertainty implies unknown and seems applicable only to future events. After all, I cannot predict the tomorrow price of a stock, but I know for surewhat its price was yesterday. The past can only be learned but not altered. Yet uncertainty has existed in the past. The price went up yesterday, but itcould have gone down. Be very careful with past observations. What has happenedis only one of the possibilities. Watch out for the \u201calternative\u201d histories,cried Nassim Nicholas Taleb. If a certain kind of events\u2014no one has seen a black swan, or the market hasnever gone down more than 20% in a day\u2014has never been observed before, wecannot conclude it is impossible. Stay in the game: The black swan is not scary as long as you can keep playing. Stay away fromRussian roulette. No matterhow lucrative the upside is, there is no come back if you get wiped out. Limittail risk. Always fasten the seat belt. Know your tools: Through what instruments, you asked. It is like choosing the tech stack for yourstartup: always the ones you know. Call options have the leverage and convexity that bends in your favor regardlessof the price going up or down, or so you have overheard in an anonymous webforum. Not entirely in apropos, but at what cost? What if the price remainsstationary? Hint: look intotheta risk. Being reasonable, not rational: It is never about achieving the most optimal return, but a good enough onethat allows me to sleep at night and achieves my life goals. Investing is ameans to an end. Be patient and start living.  Warren Buffett\u2019s skill is investing, but his secret is time.  \u2013 Morgan Housel, The Psychology of Money "},{id:41,url:"/shell/",title:"Navigating Shell for Productivity and Profit",body:"2020/11/20 - I hope you find inspirations from these pretty neat shell tricks and my shell setup. Array Expansion: Use Array Expansion to quickly build arguments to a command. For example, to downloadseveral HTTP resources:    1$ wget https://kubernetespodcast. com/episodes/KPfGep{001. . 062}. mp3 To rename files when you just want to change a substring.    1$ mv /path/to/same-prefix-{old,new}. txt Process Substitution and Anonymous Files: &lt;() redirects the stdout of the command in the subshell to a file descriptor openedat /dev/fd. This is known as process substitution.    1$ sh &lt;(curl -sL https://istio. io/downloadIstioctl) The above is equivalent to the following, where - means to use the stdin as a file.    1$ curl -sL https://istio. io/downloadIstioctl | sh - Note that process substitution is more powerful than pipes, since you can do things like    1$ diff &lt;(ssh somehost cat /etc/hosts) &lt;(ssh some-other-host cat /etc/hosts) Start new shell: Just updated your bashrc/zshrc file, but want to hold on to your environmentvariables, file descriptors, etc? Do this.    1$ exec -l $SHELL The password is sudo: Dealing with production outage and in dire need of mitigation?    1$ sudo su And then you are always the root master. \u201cI don\u2019t test in prod; I dev in prod. \u201d Alias for the Win: Type less. Move fast. Here are some I use everyday.    12345678mcd() { mkdir -p  $1  &amp;&amp; cd  $1  }cdl() { cd  $1  &amp;&amp; ls }alias ls='ls -F -G'alias ll='ls -lh'alias o='open'alias ping='ping -c 5'alias gauth='gcloud auth login --update-adc' (I consider it a work injury that I now quote everything in shell and has long stopped usingwhite space in file names even for personal stuff. Maybe you are like me. ) Cursor Navigation:  Command Deletion:  In-line file:    12345678910cat &lt;&lt; EOF | kubectl create -f -apiVersion: v1kind: Podmetadata:name: nginxspec: containers: - name: nginx  image: nginxEOF Search Command History with Fuzzy Find: You probably know you could use ctrl+r to retrieve a previously executed commandquickly, but it only gives you one suggestion. fzfsolves it not only for commandsbut finds files as well.   Auto-correct Command Typos: I don\u2019t endorse profanity but we all share the frustration. Thefuck will correctyour mistake.   ZSH, ohmyzsh, and powerlevel10k: ZSH, or Z shell, is an extended version of the Bourne Shell (sh), with numerousnew features, plugins, and themes. To change your shell to ZSH    1$ chsh -s $(which zsh) Ohmyzsh is a plugin manager for ZSH. I recommend at least two plugins. autosuggestions makes zsh like fish.   syntax-highlighting colorizesthe command and strings, providing visual warning for typos.   "},{id:42,url:"/promo/",title:"Software Engineering Levels and Promotion",body:"2020/08/31 - This post explains the expectation of each engineering level in the most conciseand company-agnostic way and reveals the steps towards promotion. Levels: The first step to prepare for promotion is to understand the expectation ofeach level. Having worked at several companies ranging fromFAANG to startups inthe valley, I noticed that the engineering leveling systems are awfully similar. New grad / Entry-level (L3) Deliver assigned tasks timely and independentlySoftware Engineer (L4) Design, implement, and operate scalable systems for well-scoped projectsSenior Engineer (L5) Define and scope undefined problems given defined goals Substantial impact within the team     Mentoring junior engineers   Technical oversight   Staff Engineer (L6) Propose and drive cross-functional initiatives, define goals, identify andconnect stakeholdersSenior Staff Engineer / Principal Engineer (L7+) It becomes stratospheric from here. Engineers at such levels are experts intheir domains.  Invent and evangelize new products and technologiesPromotions: There are many moving pieces when you work towards a promo. If you forgeteverything I shared in this post, try to remember this: the single mostimportant thing to do for a promo is consistently delivering projects at thenext level of scope. This is also one of the reasons (besides budgeting) why promotion is gettingslower and harder to come by at some of FAANG, because with the money-printingsuccess of the company\u2019s business model, development work gradually shiftstowards keeping the lights on. Below are generic steps towards a promotion. Step 1: Reasonable manager: The promotion process, as well as one\u2019s life, depends more or less on luck ongetting the right manager and projects. Without a supportive manager, promotionis rather difficult, because your manager is the person who represents you inthe calibration and promo committee review. At least, your manager should view people development as part of her job andbelieve that you have the potential to grow into the next level. If youunfortunately end up in sometoxicwork environment, there are ways to get promo too that I do not recommend. Instead, my suggestion in this case would be to run away screaming (change teamor company). Step 2: Identify gaps: Engage with your manager in your weekly one on one, to shareyour career aspiration and together identify the gaps between your current andthe next level. Two things to keep in mind: Seek actionable feedback. If you are L3 and your manager says the tasks youhave delivered are not technically challenging enough, this feedback probably isnot actionable for you, because you are likely not the person deciding on whichtasks to focus on. An example of actionable feedback is to \u201cbe more vocal aboutyour opinions by speaking up more in meetings and commenting on design docs\u201d. Perception is reality. If your manager thinks you have a communication problem,then you need to solve this communication problem to get promo. You may disagreewith such diagnosis, or there might be underlying causes for such a symptom, butyour manager\u2019s perception is what matters here. Step 3: Find the right projects: To repeat from the opening of this section, the key to promotion is to deliverprojects at the next level of impact. At L3/4, finding such projects are easy. Most projects permit designs that goon an extra mile in terms of feature, scalability, testability, and operationalexcellence. At L5 and beyond, it gets a bit tricky. Most teams do not have a backlog of L6projects, which is one of the reasons why many plateau at L5. So again, luck(right place and the right time) is needed for promo. Your manager and maybetech lead are responsible for prioritization and project scoping. Make sure toloop them in to strategize for these projects. Step 4: Learn and grow: Learn all the new technologies. Build healthy relationships with sister teams. Execute on the projects. Establish competencies. Step 5: Demonstrate impact: You may believe in meritocracy and think the excellence of your work warrantsattention and rewards, or perhaps you are being humble. Yet the reality is thatmost people are too busy to look if you do not point them in your direction. Bombast about yourself feels awkward and might backfire. The trick is to letother people praise you, especially those who are at high levels than yourtarget level. Relationship is important not only because it may unblock yourproject dependencies but also because many, after all, judge books by covers andbelieve trusts are transitive. Prefer written evidence if you can, such as design docs you authored, videorecording of your demo at all-hands, and thank you notes / peer bonus. Following these steps and with sustained performance at the next level,promotion is yours. If not, did I mention luck and having a reasonable manager?My run-away-screaming advice still holds. "},{id:43,url:"/one-on-one/",title:"What to Talk about in Effective 1-on-1s",body:"2020/06/21 - Unlike in school when we get grades on every assignment and in every course, we get less frequent feedback in professional life, usually once or twice per year, which is also when compensation and level adjustment happens. The infrequent feedback might be frustrating, since it introduces artificial delays in recognizing and acting on improvement opportunities. Careers management is not so different from product management\u2014fast iterations of small improvements are the proven route towards success. One-on-one meetings allow a much smaller feedback cycle. You should schedule regular one-on-ones with your direct manager, if not your skip manager as well. However, it is not trivial to run effective one-on-ones. Sometimes they become status updates or are simply skipped over (Pro tip: don\u2019t cancel but reschedule instead). Just like you own your career, you also own the one-on-ones. Here are some good questions to ask your manager or share your answers with your manager. What to talk about: In this context, you means the manager and I represents the individual contributor. Career and Performance: How am I meeting your expectation?Could you help me understand what the expectations of me are at my current level? What does \u201cExceeds Expectation\u201d rating mean in the context of our team? From what you have seen in calibration and performance review, what are the set of projects and impacts I should aim for to reach such a rating? What is my next career goal? Could you help me identify the specific gaps between where I am and my goal? What are the and action items to bridge the gap? To grow into a senior/staff engineer or an engineering manager requires different skills and experience. You want your manager to know your aspiration and together devise a plan to achieve it. How can the manager be helpful without knowing what you want? Whose job do I want in the future? Do I know what their functions are?Team and Satisfaction: Am I happy with the projects I am working on? Is it too much operation than development? Am I learning new skills? Can I accomplish my tasks?     An ideal job/project should mandate a level of expertise of which I satisfy 60%. Anything more will be trivial to solve and not enough growth for me, but anything less will be overwhelming and set me up for failure.     Are these projects meaningful or impactful? I am very excited about the XYZ project that Jane Doe is working on. She would be a great mentor for me and XYZ is a top priority of our team. How can I contribute? Would you be supportive of me to involve in that project for my career development?Am I happy on this team?Are my peers capable of delivering results? Do they have a strong personality that makes it hard to work with them sometimes? Do they write awesome reviews on my pull requests, or am I tired of their nit-picking? Am I happy with my manager?Do I get the support I need? Does my manager maintain a healthy cross-functional relationship? Do I wish my manager to lean more toward technical leadership or people management? Am I happy at this company?Are there certain policies I like or dislike? Do I enjoy the culture? Do I trust senior leadership? Projects and Processes:  Instead of providing status updates, ask: Do you feel we have a strong tracking process to keep you updated on the project status? Are we setting realistic goals? Do you like our quarterly/sprint planning process?"},{id:44,url:"/istio-short/",title:"The Good, Bad, and Ugly: Istio for Short-lived Pods",body:"2020/04/26 - Kubernetes does not differentiate sidecars and application containers in a Pod. Hence, enabling Istio for short-running workloads imposes additional challengesto the conventional approach of injecting an Envoy sidecar to the istio-enabledPod.  The proxy sidecar is long-running and prevents the Pod from finishingeven after the application container has completed.  We cannot ensurethe proxy to be running and healthy before the application container starts. Without Istio proxy, requests to downstream services may fail if serviceauthentication is enabled, and thus the app container would fail its health check,causing the Pod to be recreated. The pod may be stuck in a crash loop for a while. Straw-man attempts: At first sight, it seems the initContainers in Pod is exactly what we wanted. After all, don\u2019t initContainers always run before containers in Pod? Yet,no containers start until all initContainers have exited successfully. If werun Istio proxy in initContainers, then we have to kill it manually before theapp container could start, which requires the proxy to connect to the service mesh. Some popular mitigationI have seen is to change the entrypoint of the app containerto wait a few seconds before executing the app binary. But how long of a waitis enough? Starting too soon risks proxy not healthy, but starting too late isa waste of time and resources. Scheduling is never deterministic. Worse, onewill have to update the Pod spec themselves, which is not amenable to all theworkloads in the cluster. An Automated and Robust Solution: Start Pod: Mutating Webhook + Command Overwrites: You may build a binary (say, valet) that polls the /ready endpoint of theproxy, and then exec the app binary afterwards. We can use the mutating webhookfrom Kubernetes to automatically rewrite the container command to includevalet. The valet binary can be downloaded using a init container and copiedto the app container using a shared volume. The following Kubernetes manifestshows this approach.    123456789101112131415161718192021222324252627282930313233343536373839apiVersion: batch/v1kind: Jobmetadata: name: short-livedspec: completions: 1 parallelism: 1 template:  metadata:   name: short-lived   annotations:    sidecar. istio. io/inject:  true    labels:    app: short-lived    myapp. com/valet:  true   spec:   initContainers:   - name: valet-init    image: myapp/valet:latest    command:    - /bin/sh    args:    - -c    - cp /valet /shared/valet    volumeMounts:    - mountPath: /shared     name: shared   containers:   - name: short-lived    image: myapp:latest    command:    -  /shared/valet     -  myapp     volumeMounts:    - name: shared     mountPath: /shared   volumes:   - name: shared    emptyDir: {} Finish Pod: Pod Exec + Kill Proxy: The valet binary also supports a -stop-proxy flag, which will stop Envoyproxy by calling the /quitquitquit endpoint on the pilot-agent. In addition tothat, the valet controller will be watching all the pods that are labeled asmyapp. com/valet:  true , and it will issue a valet -stop-proxy command whenthe short-lived containers are all completed. It will essentially be the below call.    1kubectl exec -it $pod -c short-lived -- /shared/valet -stop-proxy "},{id:45,url:"/dns-udp/",title:"DNS, UDP, IP Anycast, and All That",body:"2020/04/05 - DNS prefers UDP. There are times when DNS must run on TCP (request orresponse size exceeds a single packet, perhaps due to too many responserecords), but UDP is perferred if possible. The reasons are  Constraints from IP Anycasts that favor stateless applications such as DNS.  Performance gain with UDP over TCP (2 vs 11 IP packets, no connection state management). IP Anycast: Anycast is one of the fiveaddressing methodsin IP, where multiple endpoint destinations share the same address. A request tosuch address expects only one response from any of the destinations. Routers mayselect the desired path based on costs, latency, and congestion. Anotheraddressing method, Unicast address uniquely identifies a single receiver endpoint. Anycast can be implemented by usingBorder Gateway Protocol (BGP) and Unicast. Multiple hosts (likely in different regions) are given the same unicast IP address. Different routes to the address are advertised through BGP, as if they are alternativeroutes to the same destination when in fact they actually route to differentdestinations with the same address. As usual, routers select a route by whatevermetric (cost, congestion, distance, etc). Selecting a route in this design amounts to selecting a destination. However, routing changes could break open connections to an anycast address,since IP packets could be routed to a different host that has no context on theconnection state (such as TCP sequence numbers). With a normal unicast address,a routing change is not a problem at all, as packets eventually arrive at thesame destination. Hence, anycast is often used with connection-less protocols to provide highavailability and load balancing for stateless services. DNS is a great fit. Theroot name servers need to be accessible at a well-known address (or we need anothername server to find these name servers but what is the address to that name server?). The actual backend serving the query should be as close as possible to reduce responselatency (as DNS lookup is often in the hot path before TCP, TLS, and finally HTTP). UDP: UDP is best-effort (unreliable) and packets may be delivered out-of-order. However,a DNS query usually fits in a single packet and does not require an ordered byte stream. Hence, out-of-order delivery is not an issue;checksum and retransmission are enough to ensure integrity. In comparison, queries over TCP require 11 IP packets to complete:  Three-way handshake to establish connection (SYN, SYN-ACK, ACK) Query by client, ACK by server Response by server, ACK by client Four-way handshake to close connection (FIN, ACK, FIN, ACK)Using UDP allows the name server to scale better because it does not allocate or manageconnection states, such as Receive and Send buffers, Sequence and AcknowledgeNumbers, and flow-control and congestion-control parameters, etc. "},{id:46,url:"/gke-scaling/",title:"Lessons from Scaling GKE: L4 ILB Tops at 250 Nodes",
body:"2020/03/20 - My team at Cruise operates tens of Kubernetes clusters with 10,000s coresand 100s of TB of RAM. Since migration to GCP, we have hit several interesting scaling issues. One of those caused cluster-wide ingress outage for all tenants. In this post, I will revisitthe symptom, root cause, and mitigations for this incident. We struggled so you do not have to. Architecture: Private Ingress with ILB and Nginx Controller:    1234567891011121314151617181920      Client       |       | L3/L4       |       vInternal TCP/UDP Load Balancer       |       | L7 HTTP       |--------------+------------------|       |         ||       v         ||  Nginx Ingress Controllers  ||       |         ||       | L7 HTTP     ||       |         ||       v         ||      Pods        ||GKE              |--------------------------------- Symptom: The Internal Load Balancer (ILB) used for private ingress in the cluster stoppedresponding to connections. New connection requests hung (e. g. curl &amp; traceroute)except for requests originated from within the cluster. Root Causes:    Currently, L4 ILB only supports at most 250 backends(source). The Kubernetes cloud provider that controls ILB creation sets all Kubernetes nodesin the cluster as ILB backends. As a result, the maximum number of GKE nodes that gets traffic from the ILB is 250. When more than 250 nodes present in the GKE cluster, ILB will deterministically (using node ID)select a subset of 250 backends for healthcheck.     The Nginx controllers are deployed in the cluster as well. They useexternalTrafficPolicy: local to bypass iptables and sends traffic directly toany pods located on the host. From the perspective of the ILB every node that hasthe destination pod will be healthy and ones that do not will be permanently unhealthy.     A tenant workload scale-up caused the number of nodes in the cluster toautoscale above 250. None of the nodes that Nginx resides on were selected bythe ILB. As a result, the ILB considered all nodes in the subset as unhealthy. When all ILB backends are unhealthy, no traffic through the ILB will be passedto the cluster.  Mitigations: Immediate: In order to immediately restore cluster ingress service, another ILB was manually created using the cluster\u2019s well-known ingress IP address. This ILB was configured to point to a couple instance groups such that the number of backing nodes did not exceed the limit of 250. Short-term: One could deploy a cluster with larger nodes (vertical scaling) in the hope thatthe total number of nodes will be within 250. Alternatively, one may useexternalTrafficPolicy: cluster so Nginx service will use kube-proxy/iptablesto load balance traffic to the correct pod(s) regardless of which node trafficlands on in the cluster. The downside of this solution is that  Extra latency. When the ILB is sending traffic to the 250-node subset,traffic will go through a second hop to get to the destination pod IPs.  Less secure, because it requires opening firewalls for port 80/443 on all nodes.  The client source IP is not preserved, because of the iptables hop. Long-term: Move Nginx controllers out of the cluster and into a GCE instance group, so ILBhealthcheck will be done properly. "},{id:47,url:"/go-opts/",title:"Parameters with Defaults in Go: Functional Options",body:"2020/03/01 - Unlike C++ or Python, Go does not support function parameters with default valuesif unspecified. Specifically, we want that  Passing multiple parameters is supported.  Interface remains backward-compatible when the number of type of one or more parameter changes.  Parameters have default values that can be overridden. In search of a general and elegant solution to this problem, wepresent a few straw-man approaches as motivations for thefunctional options and With*() pattern, which are presented last. Straw-man: Update function signature to accept more inputs: Suppose you have the following Foo struct with the most basic constructor New().    123456789101112type Foo struct { num int str string}func New(num int, str string) *Foo { // . . . initialization return &amp;Foo{  num: num,  str: str, }} Imagine that we want to add more fields to Foo and therefore to change theconstructor into func New(num, num2 int, str, str2 string, bar *Bar) *Foo. Notonly is this function incompatible with existing calls, it also becomes less readableas we add more parameters. Keep the old functions:    123456789type Foo struct { num, num2 int str, str2 string bar    *Bar}func New(num int, str string) *Foo { /* . . . */ }func New2(num, num2 int, str, str2 string, bar *Bar) *Foo { /* . . . */ } We could keep the old functions while adding new ones to support more fields,but it also means the total number of functions grows exponentially (power of 2) with thenumber of parameters, since each parameter could be included or excluded in afunction. Our file would soon become unmaintainable. Putting all params in a struct:    12345678910111213141516171819202122232425type Foo struct { Option}type Option struct { Num int Str string}func New(opt Option) *Foo { // Default values for Foo.  foo := &amp;Foo{  Num: 10,  Str:  hello  } // Set overwrites.  if opt. Num != 0 {  foo. Num = opt. Num } if opt. Str !=    {  foo. Str = opt. Str } return foo} Doing so addresses the compatibility issue and seems to achieve default values. However, it is impossible to determine whether the function caller explicitly setsopt. Num to zero or did not specify Num at all and therefore using the defaultvalue 10. In fact, such confusion around zero-value as input happens not just for Optionstruct but whenever and whatever parameters are passed directly to functions. Passing a struct pointer:    123456func New(opt *Option) *Foo { if opt == nil {  // Use all values in opt to create Foo.  } // Use all default values. } The nil pointer is a good way distinguish set vs unset. However,The new problem is that either all fields need to use thedefault values or none of them does. Hence, if the user only wants to overwriteone parameter and use defaults for the rest, the user must supply the default valuesfor other fields explicitly. Yikes. Make all fields pointers:    1234567891011121314type Option struct { num *int str *string}func New(opt Option) *Foo { foo := newFooWithDefaults() if option. num == nil {  foo. num = opt. num } if option. str == nil {  foo. str = opt. str }} Since the nil pointer is a good way distinguish set vs unset, making all thefields a pointer seems to do the trick. However, this is not user-friendly at all,because in Go there is no such thing as &amp;20 or &amp; hello and the call must assignthe literal value to atemporary variable and then take its address. Not pretty.    1234567num := 20str :=  hello opt := {  num: &amp;num,  str: &amp;str,}foo := New(opt) Variadic functions:    1234567func New(num int, str string, num2 . . . int) { if len(num2) == 0 {  // Did not provide num2, use default.  } else {  // Use num2[0].  }} This alternative approach only works with one optional parameter with dirty semantics,such as the behavior if len(num2) &gt; 1. Functional Options: Finally, we arrive at the functional options pattern that solves optional params(or params with defaults) nicely.    123456789101112131415161718192021222324252627282930313233343536type Foo struct {  Num int  Str string}type Option func(f *Foo)func WithNum(num int) Option { return func(f *Foo) {  f. Num = num }}func WithStr(str string) Option { return func(f *Foo) {  f. Str = str }}func New(someRequiredField string, opts . . . Option) *Foo { foo := &amp;Foo{  Num: 10,  Str:  hello , } for _, applyOpt := range opts {  applyOpt(foo) } return &amp;foo}func main() { foo := New( important , WithNum(30)) foo = New( required , WithNum(20), WithStr( hello ))} More Go Tips: Love what you are reading? My wiki pages have more battle-tested Go lessons:  Useful Go Snippets Go Common Pitfalls More Effective Go Production-ready Go"},{id:48,url:"/k8s-ha/",title:"How to Configure Applications for High Availability in Kubernetes",body:"2019/12/29 - Pods in Kubernetes are the smallest orchestration unit and are ephemeral by definition:  Deployment/StatefulSet/DaemonSet/ReplicaSet updates or patches Nodepool downscaling (compaction) or upgrades (cordoned and drained)Kubernetes simplifies scheduling and orchestration but there are extra hurdles to develop and operate applications with high availability. I list some action items for HA and explain the motivations behind them. Pod: Graceful Termination: When a Pod is evicted,  Pod containers each receive a SIGTERM.  Eviction controller waits terminationGracePeriodSeconds (default 30 seconds) Pod containers each receive a SIGKILL Eviction controller waits for all containers exitHandle the termination signal gracefully because your application may still be serving in-flight requests or need to clean up persistent storage before exit. Health Checks: Health checks are important to ensure that unhealthy\u2014irresponsive, about-to-be-killed, or initializing-and-unready\u2014Pods are not selected in the working set. Health checks can be in forms ofshell commands,HTTP probes,or TCP probes. Readiness ProbesA Pod will only start serving traffic after its Readiness probe succeeded. For example, your web app should establish a connection to your favorite database before serving its API. Liveness ProbesMany applications running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes provides Liveness probes to detect and remedy such situations. Startup ProbesStartup Probes resembles Liveness probes but are intended for slow-initialization applications. Without Startup probes, one may use longer Liveness probes for slow-start applications, but doing so compromises the fast response to deadlocks that motivate Liveness probes in the first place. Priority Class: If a Pod cannot be scheduled, the scheduler tries to preempt (evict) lower priority Pods to make scheduling of the pending Pod possible. Run your mission-critical Pods on high priority. For example, logging, monitoring, backup, and a subset of application services (depending on business logic) should be of high priority, but batched and cron jobs may be not. Resource Requirements and Quotas: Pod resources are CPU counts, memory space, and ephemeral storage size. Resource requirements include requests (minimum needed to run) and limits (maximum allowed). If you configure the same value for requests and limits, your Pods are of Guaranteed (highest) Quality of Service (QoS) Classes. If the node is under memory pressure, burstable or best effort Pods are killed and rescheduled first. However, depending on your node size, choose sensible resource requirements. If your node has 64 vCPU, asking for 60 is just not cloud-native. Replicated Pods: Load Balancing - Ingress and Service: Put your replicated Pods behind a load balancer to ensure your HTTP/TCP/UDP application remains accessible when Pod health or membership changes. Such a load balancer could be an Ingress or a Service, depending on whether you want this service to be accessible only in the cluster network or your company VPC or the public internet. Leader Election: Perhaps not all of the replicated Pods should be considered active. For example, a Kubernetes controller on CRDs should logically be just one instance. Leader election allows such an application to still be replicated while preserving the single-instance abstraction. Pod Disruption Budget (PDB): A PDB limits the number Pods of a replicated application that are down simultaneously from voluntary disruptions. For example, a quorum-based application would like to ensure that the number of replicas running is never brought below the number needed for a quorum.    123456789apiVersion: policy/v1beta1kind: PodDisruptionBudgetmetadata: name: zk-pdbspec: minAvailable: 2 selector:  matchLabels:   app: zookeeper Anti-Affinity Preference: Anti-Affinity Preference allow replicated Pods to be scheduled to different nodes. Without such preference, a single node failure could knock out multiple Pods if they happen to be scheduled on the same node, which is possible since scheduling is done by resource allocation. Autoscaling: Autoscaling allows the service backends to adjust to the request rate and to mitigate overloading. Vertical AutoscalingVertical autoscaling modifies the resources allocated to your Pods dynamically.    1234567891011apiVersion: autoscaling. k8s. io/v1beta2kind: VerticalPodAutoscalermetadata: name: my-vpaspec: targetRef:  apiVersion:  extensions/v1beta1   kind:    Deployment  name:    my-deployment updatePolicy:  updateMode: Initial If you need to limit the number of concurrent pod restarts, use a Pod Disruption Budget. Be extra careful if you are using ingress solution from your cloud vendor, such as GCLB, it may take minutes to update backend routes which can cause routing failure and downtime when combined with frequent rescheduling. Horizontal AutoscalingHorizontal autoscaling dynamically modifies the replica count of a Deployment. By default, the HPA only supports scaling based on CPU or memory usage, which are often suboptimal for request-based or queue-based workloads. HPA supports autoscaling based on custom metrics, but doing so requires elevated permissions to register a Custom Metrics Adapter to serve a cluster-level API endpoint, which may not be an option if you are running in a multi-tenant environment. Monitoring and Alerts: API Uptime and Latency: This is probably the key KPI that your customers care about. Visibility on the API uptime and latency allows quick responses to incidents. Managed solutions include Runscope, Pingdom, DataDog, UpDown. io, StackDriver, etc. It is also important to alert on these metrics to PagerDuty, Slack, or Email. Resource Usage - CPU, RAM, Disk, I/O: Eccentric resource usage is usually a precursor for outages. Usually, developers or the language runtime will embed in the application Prometheus endpoints for scraping these metrics. DataDog agents make collecting them easy. Unavailable Pods: You could have the right configurations for HA but still have unschedulable Pods\u2014image pull unsuccessful, or container stuck in crash loop, or node auto scaler has reached max, etc. "},{id:49,url:"/advices/",title:"Advices I wish I got at the start of my career",body:"2019/11/03 - When I was a kid playing chess with my dad, he sometimes would offer me hints onsome good moves. I would never make those moves. I would rather make othersub-optimal moves for the sake of not taking on his advices, believing that myeventual success\u2014much rarer than I would hope\u2014should be attributed solely tomy own volition and endeavor. I find such irrational rationale the perfect metaphor for many adventures in mylife. I preferred to learn things the hard way by myself and I did. It took me awhile to realize the story I touted that I was a self-made man could not havebeen more misleading. Despite all my trials and errors, I would have comenowhere close to where I am had I not had the help from my parents, friends,advisors, and colleagues. Hence, though I would never stop experimenting, I started to seek advices fromall sorts of venues. Many lessons need not be learned the hard way. If you sharemy goal of personal and career growth, here is a collection of wisdom I gainedover the years from either misadventures of my own or people I admire andrespect. Learning is Compounding: Prioritize learning over pay. At the start of one\u2019s career, the pay differencesamong offers are negligible after taxes. At 22, your largest investment capitalis your time. Invest early and continuously in yourselves to leverage thecompounding effects, which apply to both learning and career. Your personalgrowth and career growth is not a forcing function that only increases withtime\u2014you are not growing but just getting older if you are essentially doingthe same job for many years. Inductively, your next opportunity directly relatesto your current knowledge and experience. Optimize for growth to get the bestopportunity, and the pay always follows.  We tend to massively underestimate the compounding returns of intelligence. Ashumans, we need to solve big problems. If you graduate Stanford at 22 andGoogle recruits you, you\u2019ll work a 9-to-5. It\u2019s probably more like an 11-to-3in terms of hard work. They\u2019ll pay well. It\u2019s relaxing. But what they areactually doing is paying you to accept a much lower intellectual growth rate. When you recognize that intelligence is compounding, the cost of that missinglong-term compounding is enormous. They\u2019re not giving you the best opportunityof your life. Then a scary thing can happen: You might realize one day thatyou\u2019ve lost your competitive edge. You won\u2019t be the best anymore. You won\u2019t beable to fall in love with new stuff. Things are cushy where you are. You getcomplacent and stall.  \u2013 Stephen Cohen, Co-founder and Executive VP of Palantir Join Companies on the Breakout Trajectory: Your scope and impact will be much larger than what your title suggests. Youwill work with the smartest people on the latest technologies without servicinglegacy baggage. You will likely work with open-source software (OSS) that grantsyou directly transferable skills. Your patches back to the OSS community giveyou unparalleled external visibility. Companies on the breakout trajectoryentail less risk than do seed-round startups struggling to find product-marketfit and yet more upside than established counterparts. The company itself will be focusing on exponential growth in business andpeople. There will always be more work than people on it and fewerpolitics\u2014hardly fights over interesting projects or he-said-she-said nonsense. In the future, having this company on your resume gives you more credit than youdeserve. Being at Paypal around 2000, Google in 2005, and Facebook in 2010 isworth more than any advanced degree will buy in this market. Career takes care of itself. Your career growth is the superposition of yourpersonal growth and company growth. Companies often prefer to promote fromwithin than to hire external talent to preserve the culture and spare knowledgetransfer. With your large scope and impact and expanding headcounts of alllevels in all departments, promotion is fast. Your experience means a lotexternally as well. Everyone wants to recruit from successful companies in thehope that people carry the lessons of success with them.  If you\u2019re offered a seat on a rocket ship, don\u2019t ask what seat. Just get on.  \u2013 Sheryl Sandberg, COO of Facebook  All our advice on Silicon Valley careers is based on a simple idea: that yourchoice of company trumps everything else. It\u2019s more important than your jobtitle, your pay or your responsibilities.  \u2013 Andy Rachleff, Executive Chairman of Wealthfront, Partner at BenchmarkCapital Prestige Matters But Can Be Manufactured: At first sight, prestige seems misplaced in Silicon Valley. I have troublethinking of a more meritocratic industry than tech and engineering. Yet prestigestill matters. At virtually every company, the hiring process biases towardsrespected colleges for the promise of rigorous curricula and talented studentsand from other elite companies, sayFAANG.  Many desirable things that you want over the course of your career will begated by mechanisms that favor folks with prestige. You can be justly upset bythat fact, but upset is an insufficient catalyst for change, and ultimatelyyou\u2019ll have to develop your own prestige to gain access to those scarceopportunities and resources. Prestige makes everything more attainable: auniversal lubricant.  \u2013 Will Larson, Head of Foundation engineering at Stripe But fear not! Prestige can be manufactured. Promote yourselves with blogs,newsletters, podcasts, conference talks, and open-source contributions. Thingsget easier and compound over time. Your first few blogs may lead to columnwriting, which may lead to book writing invitation by publishers. Your youtubechannel or podcasts enable you to talk on meetups and eventually conferences. All these will allow you to be discoverable by the next best opportunities inlife. Your Manager and Colleagues, Not Company, Determine 95% of Your Experience: Before taking the offer, talk to your future direct manager and colleagues andask hard questions. Everyone has their style of working, and different teamshave disparate prospects, focus, and dynamics. Try really understand what youare getting yourselves into and if this is the right opportunity for you. Hereare some questions that I find helpful to uncover this information. Formanagers,  Is this position open because the last person left or your team expanded? What are the projects that I will be working on? Who is the customer? Why isit more important than other backlog projects? How many reports do you have? How many of them are senior or staff? How longhave they joined the company? What is your management style? What is the career outlook for this position? What is the next role that I cangrow into? Has anyone done it? How would you help develop the career of your reports?For colleagues,  If you have the power, what is the one thing that you would like to changeabout this company? What projects are you working on? What do you like or dislike about them? How do you solve XYZ? I have used ABC before but I am interested to know whyyour team uses DEF.  How do you like your team/manager? Tell me the workflow of your team from request gathering to production.  Do you know your skip manager and vice versa?You Get What You Put In: Be proactive at work. Help investigate outages even when you are not on call. Read code and documentations aggressively to understand how your and othersystems work. Do not be limited by your sprint goal or responsibility. Fix thethings that were a pain for everyone. Document tribal knowledge that you haveacquired. Attend postmortems and ask questions. Review and comment on designdocuments and pull requests. In essence, you do more to get more. People, Not Jobs, Last Forever: You will likely switch companies every few years, but the people in the industryare here for long. You take all the relationships you have built\u2014good and badones\u2014into your new adventures. Try to build good, lasting relationships. Pickthe people you work with. Capable and affable coworkers can only accelerate yourcareer. These are the group of people to put your name into referrals or to joinyour next undertaking.  Positive relationships enable serendipity, and serendipity is the source of themost interesting opportunities.  \u2013 Will Larson, Head of Foundation engineering at Stripe Switch teams and companies every now and then, help in hiring and interviewing,offline engagements at conferences or meetups will grow your personal network. Some Book Recommendations:  The Hard Thing About HardThings The MythicalMan-Month The LastLecture Site Reliability Engineering atGoogle How to Win Friends &amp; InfluencePeople Difficult Conversations: How to Discuss What MattersMost"},{id:50,url:"/2s-comp/",title:"A Brilliant Hack: Why does Layer 2/3 Checksum use 1\u2019s Complement, Not 2\u2019s",body:"2019/04/07 - A super quick recap, one\u2019s complement represents negative x by reverting every bit of x, while two\u2019s complement negative x as one\u2019s complement of x plus 1. Symbolically,    12one\u2019s complement  -x = ~xtwo\u2019s complement  -x = ~x + 1 Two\u2019s complements seem to have taken over the entire computing world. Some major reasons promoting two\u2019s complements include that the adder in ALU on your processor is the same for both positive and negative operands, that the same adder could be used as subtractor easily, and that there is exactly one way to represent the value of zero. Nonetheless, the error detection checksum makes the conscious decision to use 1\u2019s complement even when its host ISA uses 2\u2019s. A refresh on the checksum algorithm:    1234567891011Send: calculate the 1\u2019s complement sum  over the payload on a 16-bit word boundary negate the sum to use as the checksum append to the payload. Receive: calculate the 1\u2019s complement sum  on a 16-bit word boundary  over the payload INCLUDING the sender checksum assert the result has bits of all ones.  An example implementation might look like the following    12345678910111213u_short cksum(u_short *buf, int count) {  register u_long sum = 0;  while (count--) {    sum += *buf++;    if (sum &amp; 0xFFFF0000) {      // One's complement add requires      // adding carry bit back to sum.       sum &amp;= 0xFFFF;      sum++;    }  }  return ~(sum &amp; 0xFFFF);} Notice that the receiver does not sum over payload to see if it equals the sender checksum, but rather the sender sends negated sum and the receiver sums (payload + checksum). Checking if all bits are ones are super easy and quick by checking if its negated value is zero, a property that is only true if one\u2019s complement is used. Perhaps such implementation is slightly faster than that using two\u2019s complement, but I found the explanation so far unsatisfactory. I did some more digging. I realized the performance boost from using 1\u2019s complement sum is endianness independent. Endianness matters because for a 16-bit checksum, its bytes are swapped when read from link to CPU. Different hosts use different endianness. Using 2\u2019s complement in checksum mandates calling ntohs and htons on EVERY router along the way. 1\u2019s complement does not need to reorder bytes. It asserts the final sum has all bits of ones. Blazing fast, and a brilliant hack. "},{id:51,url:"/blue-green/",title:"Service API Changes: Prefer Blue-green Update to Rolling Update",body:"2019/03/24 -  Summary:  To achieve zero-downtime service update, Kubernetes rolling update implies the API must be both forward and backward compatible. Forward compatibility is hard if at all makes sense. Blue-green update requires only backward compatibility to ensure zero downtime. Blue-green update is not supported by the Kubernetes core API but achievable with simple scripts or CRD + controller. The Deployment object in Kubernetes supports service rolling update in the hope of providing zero downtime service update. The rolling update is done by scaling out a new replica set of pods using containers with the new version and shrinking the old replica set. However, rolling updates are insufficient to avoid downtime when the service API changes. The API must be both forward and backward compatible due to the coexistence of both versions of servers and clients.      As the new replica set is scaling out, the Service label selector will include all the available pods spanning both replica sets, which means both versions of the server are serving requests. Imagine these are web servers and v1 (v2) servers hand out v1 (v2) JS client that runs in browsers. During the update, many browsers still run v1 client, but the service starts giving out v2 clients. The Service object load balances the ingress to both v1 and v2 servers. Hence, requests from v2 client could be routed to v1 server (forward compatible) and v1 client could be routed to v2 server (backward compatible). The semantics of forward compatibility is always hairy. One could \u201cgracefully\u201d handle an unsupported request by ignoring it, returning the not-found status code, or returning a response asking for a retry in the hope of landing on a server with the new version (and good luck with sticky sessions). None of these is satisfactory. Blue-green update does not require forward compatibility because it ensures a single version of servers. A new Deployment is made and the Service label selector is updated.    There could still be older versions of clients after the update is complete (backward compatible), but the client of the latest version of will not be handled by the servers of old versions. Blue-green update is easy even by hand. Alternatively, one could leverage the BlueGreenDeployment CRD (CustomResourceDefinition) and deploy a controller handling updates to the BlueGreenDeployment object. Check out https://github. com/google/blue-green-deployment-controller. "},{id:52,url:"/configmap/",title:"CD Tricks for Kubernetes Deployment + ConfigMap",body:"2019/03/10 - It is common to extract the application configuration to a separate file as a runtime dependency of the container image that includes the application binary. As a result, the same image can be used (thus \u201cpromoted\u201d) across different deployment environments, from dev to staging and prod. Kubernetes offers native support to do exact so, but not without some caveats that I hope to carve out for you. The Kubernetes Deployment is an API object that manages a replica set of Pods. A Pod is a collection of one or more containers and is the smallest atomic unit to be provisioned. A replica set includes a set of identical Pods and ensures the number of replicas conforms to the desired state. The Deployment enables rolling upgrades of your applications with zero downtime by gradually scale out a new replica set of the app with the new version and scale down the old replica set. Hence, the Deployment object has been the de facto way to manage the application life cycles. ConfigMap is another Kubernetes object that essentially represents a set of key-value pairs that represents a configuration. It can also represent a file with the key as the file name and the value as its content. To be accessible to the application, a ConfigMap can be mounted to a Pod as a volume in the file system of the container. You may create/update a ConfigMap with    1234kubectl create configmap myconfig  \xa0\xa0\xa0--from-file=/path/to/config. yaml  \xa0\xa0\xa0--dry-run -o yaml  \xa0\xa0\xa0| kubectl apply -f - The first trick to share is the dry run piping into apply. If there is an existing configmap with the same name, kubectl create will fail, but there isn\u2019t a way for us to update configmap from file like we could with kubectl create. The dry run pipe make updates possible. And then the deployment manifest may look like something like this.    12345678910111213141516171819202122232425apiVersion: app/v1kind: Deploymentmetadata: name: myappspec: replicas: 3 selector:  matchLabels:   app: myapp template:  metadata:   labels:    app: myapp  spec:   containers:    - name: myapp     image: myapp:1. 0. 0-alpine     args: [ --config ,  etc/myapp/config. yaml ]     volumeMounts:      - name: myconfig       mountPath: /etc/myapp    volumes:     - name: myconfig      configMap:       name: myconfig But what if we only want to update the configmap while using the same container image? Although such an update will be immediately reflected in the container file system (i. e. , reading the config file again after the update will retrieve the latest write), most applications only load the config file during initialization. The challenge becomes how to instruct the application deployment to pick up the latest config file with zero downtime. Recall the Deployment object manages the replica set of application containers. The key here is to trigger another Deployment rollout, so the new pods created will pick up the latest config file. Updating the configmap solely will NOT trigger a Deployment rollout. The trick is to include a CONFIG_HASH in the pod template. When its value changes, a Deployment rollout is triggered.    12345678910111213141516171819202122232425262728apiVersion: app/v1kind: Deploymentmetadata: name: myappspec: replicas: 3 selector:  matchLabels:   app: myapp template:  metadata:   labels:    app: myapp  spec:   containers:    - name: myapp     image: myapp:1. 0. 0-alpine     args: [ --config ,  etc/myapp/config. yaml ]     env:      - name: CONFIG_HASH       value: ${CONFIG_HASH}     volumeMounts:      - name: myconfig       mountPath: /etc/myapp    volumes:     - name: myconfig      configMap:       name: myconfig The final deployment script becomes    12345678910kubectl create configmap myconfig  \xa0\xa0\xa0--from-file=/path/to/config. yaml  \xa0\xa0\xa0--dry-run -o yaml  \xa0\xa0\xa0| kubectl apply -f -export CONFIG_HASH=$(  \xa0\xa0\xa0cat /path/to/config. yaml   | shasum | cut -d' ' -f 1)envsubst &lt; deploy. yaml | kubectl apply -f - "},{id:53,url:"/jwt/",title:"JWT + Third-party Oauth in Single Page App",
body:"2019/02/13 - Imagine you run a single page app at example. com that communicates with backends over restful API and is authenticated with JWT tokens managed by you, but identities are managed by third-party OAuth vendors (Google, Facebook, GitHub, etc). Integration of JWT and Oauth has proven nontrivial. Especially, how exactly does one issue the JWT token back to the client after the client successfully authenticated with the third-party OAuth vendor? To appreciate this challenge fully, please endure a brief introduction of the OAuth workflow.   The API server does not manage user credentials. The client sends the credentials to the OAuth vendor and receives a redirect URL and a one-time passcode. The redirect URL is a subdomain of yours and is pre-registered by your app with the OAuth vendor. Its purpose is to relay the passcode back to API server so the server could use this code in exchange for the OAuth token. Then the server might use the token to request for some basic user info and then issue a JWT token for this user. How do we send this token back to the client? The client is still waiting for a server response to its \u201cshare code\u201d request. It is worthwhile to note that such request is done by the browser hitting the redirect URL as instructed by the OAuth vendor, so the browser is waiting for a response that it can render. Hence, replying the JWT token in JSON over HTTP just does not cut it. If the server replies just an HTML with javascript assets, all subsequent requests by the client will still be unauthorized. I have seen so many hacks on this problem.    Store session id in cookie and store the token in session on the server side. Then you must make sure session persistence if your API servers are horizontally scaled. If a user logged in on replica A and subsequent requests hit replica B, the user should not be asked to log in again. The use of cookie also defeats the entire point of JWT based API server authentication, since it is not mobile app friendly.     Pass the token through URL params by redirect elsewhere before redirecting to the final logged-in page. In the following example, the callback from OAuth vendor will first redirect the client to /saveToken with JWT token as param. The client extracts the token from the path and at the same time, the server responds dashboard. html. Not only is doing so not secure (referrer HTTP header might keep the token), it also requires a lot more work on the frontend client code to control redirect routings to extract JWT token before sending GET requests to /saveToken.     12345678const auth = passport. authenticate('google', { failureRedirect: '/login', session: false,})app. get('/auth/google/callback', auth, (req, res) =&gt; { const jwt = createJWTFromUserDATA(req. user) res. redirect('/saveToken?jwt='+jwt)}) The best solution that I encountered is the following, which stores the JWT token in local storage and then redirects to the main page with no change to service code and just a few lines of HTML.    123456789101112131415const auth = passport. authenticate('google', { failureRedirect: '/login', session: false,})app. get('/auth/google/callback', auth, (req, res) =&gt; { const jwt = createJWTFromUserDATA(req. user) const htmlWithEmbeddedJWT = `&lt;html&gt; &lt;script&gt;  window. localStorage. setItem('JWT', '${jwt}');  window. location. href = '/'; &lt;/script&gt;&lt;/html&gt;` res. send(htmlWithEmbeddedJWT)}) "},{id:54,url:"/docker_multi_stage/",title:"Docker Multi-stage Build: Fast, Minimal and Secure Images",body:"2019/01/21 - Introduced in version v17. 05, multi-stage builds feature in Dockerfiles enables you to create smaller container images with better caching and smaller security footprint. Fundamentally, the new syntax allows one to name and reference each stage and copy artifacts between them. Fast Images: In this example, the second and third stage will build in parallel.    12345678FROM ubuntu AS baseRUN apt-get update &amp;&amp; apt-get install gitFROM base AS src1RUN git clone . . . FROM base AS src2RUN git clone . . .  Minimal Images: Before multistage builds, one needs quite some heavy lifting to reduce the final image size. Tricks to do so aim at reducing the total number of layers and the size of each layer. It was common to see optimization such as chaining all commands using &amp;&amp; as shown in the following example.    123456FROM debian:wheezyRUN apt-get update &amp;&amp; apt-get install curl ca-certificatesRUN curl -L https://dep. tar. gz -o /tmp/dep. tgz   &amp;&amp; tar xzf /tmp/dep. zip   &amp;&amp; mv /tmp/dep/bin/dep /workingdir/bin   &amp;&amp; rm -rf /tmp/dep* The reason is that each COPY, RUN, ADD is a new layer. Layers are analogous to git commits. They both represent the delta between two snapshots. If you add and remove the same file, the final result is identical but the aggregated deltas are twice the size. Hence, &amp;&amp; not only reduces multiple layers into one but also merges deltas that cancel each other. Without &amp;&amp;, the downloaded zip file or in other cases source code on your local will remain somewhere in the image layers, even though the final image appears not to have them. This trick works but looks cumbersome. With multistage build, small images are so easy.    12345678FROM debian:wheezy AS depRUN apt-get updateRUN apt-get install curl ca-certificatesRUN curl -L https://dep. tar. gz -o /tmp/dep. tgzRUN tar xzf /tmp/dep. zipFROM debian:wheezyCOPY --from=dep /tmp/dep/bin /workingdir/bin It becomes much easier to read and the final image is just one additional layer on top of the base image debian:wheezy. Secure Images: Multistage builds also ensure you do not accidentally push secret credentials along with the image. Image there is a private repo that your code needs as a third party dependency. To pull its source you need to mount your ssh key as part of the build. The last thing you want is to leave your key in the layers and shipped as part of the image. Multistage builds offer a clean solution. You could download and build the private source by mounting your ssh key to the first stage, copy over only the binary output to the second stage, and push the second stage. You may argue there is still a layer on your local that keeps that key. It is true, but the key is from your local anyway. "},{id:55,url:"/sys-design/",title:"System Design Interview: Scaling Single Server",body:"2018/12/14 - Imagine your app is doing tremendously well with growing traffics. If there is a single server for your app, and the server is approaching its capacity, how would you scale to handle the load?  A slight digression to why it is bad if the server loads go beyond the saturation point. Ideally, we hope that the server throughput increases as its loads, which is only true up to a certain point due to physical limitation \u2013 RAM, CPUs, IO, etc. Beyond that, one could manage to sustain the same level of throughput if they know what they are doing (more blogs on this to come), but a more common scenario would be an exponential decrease in throughput due to receiver livelock where network packets are handled as interrupts at a higher priority than the server process. At high arrival rate, the CPU is constantly preempted to handle the incoming packets, only to drop them because of full buffer / queue, hence the server process hardly ever gets run to deplete the queue. The moral of the story is to get close to but not over the saturation point for reasonable latency and throughput. Back to the starting issue. What if the traffic going beyond the saturation point? The simplest answer would be to scale up / vertically, as in running your server process on a much more powerful machine, perhaps more RAM for caching and more cores for throughputs. But there is a limit on the state-of-the-art configuration \u2013 there are only so many cores on a single chip or so much RAM that the motherboard supports. Hence, the server must scale out / horizontally, be able to do more when it needs more. Stateless Frontend / Edge / API Servers:  To do so, the server has to be stateless, or all states must be externalized to a durable, secondary storage. A stateless server is beneficial because crash recovery is easy \u2013 just restart and connect to the database \u2013 and to scale out means to provision another identical instance connected to the same storage. Then you essentially doubled the throughput, going from one server instance to two, assuming the storage has yet to reach its capacity. The new server instance could (should) be deployed at a different geographic location so their faults are isolated. If one of your data centers is out of power, the rest of your servers could pick up the load. Usually, all of the servers sit behind a load balancer / reverse proxy, so that a single IP address is exposed but server membership could also be easily managed. Doing so also helps achieve even distribution of loads on the active set, with your favorite load balancing scheme \u2013 L4, L7, round robin, random, shortest queue, etc.   As we add more servers, the storage becomes the bottleneck. There are essentially two ways to scale the storage \u2013 split the storage into multiple shards / chunks / partitions so they can serve at the same time (and thus throughput multiplies) and a hot cache to intercept most of the loads to storage. Memory Cache Cluster:  The number one rule of any architecture design is to make common cases fast. An epitome of such design philosophy, memory caches (memcached, redis) are another downstream service that caches frequently accessed server responses only in memory. No persistency means no disk I/O and hence much lower latency and higher throughput. Prominent use cases include celebrity tweets or trending YouTude videos, where 10% of your content attracts 90% of the traffic. It is okay to miss. It is okay to fail. Those are two import points people often missed about memory caches. Cache is by definition not the storage layer. If the cache missed, the client could always fall back to DB. Indeed, one has to mind the cascading failure where if the cache fails entirely, all traffic hits and essentially kills the persistent storage, but building a replicating consensus quorum on top of memory caches is the anti-pattern of caches. Doing so introduces additional round trips in delay by the farthest replica. The better approach should be a cluster of memory caches, sharded but not replicated. If any of those failed, traffic on those partition keys hits the DB but should be miniscule on the grand scale. In the meantime, erect another instance of memory cache elsewhere and when it is ready, routes traffic on the previous cache there and none of these operations need to be synchronous \u2013 if the cache fails, hit DB; if the cache is cold / just up, hit DB and write through; if the cache is up but some client has yet to discover, hit DB. Key to success is fine-grained partition space. Sharding:  Each piece of data is assigned to a particular shard using a partition key, which is usually the primary key. Such assignment has to deterministic, so that the data can be retrieved / queried, a task not as trivial as it seems. Imagine the strawman example with N shards to start with and an assignment function partitionKey mod N. Suppose such set up is falling behind the loads. Adding any additional shards changes N and thus breaks the assignment scheme, since the same partition key is likely to be mapped to a different shard using the mod function given that the total number of shards have changed. The assignment task is exactly like hashing where a key is mapped to a bucket. Worse, to fix this mismatch, the entire storage has to reshuffled so a key to put to the right shard, and you have to do this for every expansion. Our goal is to devise a consistent hashing scheme with minimum redistribution. There isan awesome paper on this\xa0and I will not bore you with the details, but a 1,000-foot view is the following.   You have a ring that corresponds to the range [0, 1). With your favorite hashing function and modulo operation, each partition key and each shard corresponds to a point on this ring. The assignment is done by walking the ring counterclockwise and the first shard you meet is the shard you go. If a new shard is provisioned, say shard 2, then only the highlighted arc needs to be redistributed from shard 1 to 2. Even better, such redistribution could be done asynchronously in the background, so that shard 1 continues to serve while shard 2 ramps up with the highlighted arc. When shard 2 is ready, it atomically joins the set, and all traffics flows accordingly, given the hashing scheme presented. Replication: Sharding introduces a new challenge. If each shard has a failure rate of p, which means a probability of (1-p) of being up and running, and the storage is partitioned into n shards, then the chance of the storage being failure free is (1-p)^n, assuming independent failures. A large n means some failure is almost always guaranteed, and that any clients hitting on those keys on the failed shard will also fail. To increase the availability of the sharded storage, we could replicate each shard independently as Replicating State Machines (RSM). The idea is that if all operations are deterministic, we could model each replica as a state machine such that by replicating the operations in the same order, states are also replicated. Popular algorithms include Raft, Paxos, Viewstamp Replication, where they all have a concept of a master/leader to process all reads and writes. Quorum-based replication often uses the client as the coordinator rather than using server-side nodes. Quorum-based replications are strongly consistent if every quorum (read or write) intersects with every write quorum, which guarantees that at least one node has seen the latest write. Load Balancers:  Recall that there is one load balancer (LB) as a reverse proxy on top of the set of API servers. What if the load balancer fails, or if the load balancer becomes the system bottleneck? In essence, one needs an LB cluster, and each LB could independently route to all API server. But how do you load balance on load balancers? The answer is to leverage multiple DNS A records such as the following    123192. 0. 2. 1 \xa0\xa0\xa0A \xa0\xa0\xa0example. com192. 0. 2. 2 \xa0\xa0\xa0A \xa0\xa0\xa0example. com192. 0. 2. 3 \xa0\xa0\xa0A \xa0\xa0\xa0example. com What if the DNS server is down? The good news is that the DNS responses are cached (TTL) almost everywhere. The bad news is that all bets are off when caches expired and the DNS server is still down. Deeper Pipeline: Services:  If you remember anything from the fetch-decode-execute cycles from your pipelined processor, you know the pipeline that splits the processor into stages reduces the latency of each stage, so the processor can be clocked at a much higher frequency and hence higher throughput. The same principle applies in the server architecture. Instead of having the API server do all the work, build a pipeline of aggregation and services. A service might depend on other services, meaning a service backend might become the service frontend of other services. Then you need to deal with service discovery, authentication, internal load balancing, etc. Perhaps Istio is worth considering. Take another look at the graph. There is no wonder why it is called a service mesh, with everything tied to everything. But how do you scale those services behind a service? If the recursive nature is still not clear, read this passage front the top. "},{id:56,url:"/docker-intro/",title:"Docker: The Container Metaphor with Profound Revolution",body:"2018/02/24 - Many regard containers as a virtualization technology. They are missing out. Docker has much more to offer. It is a graceful solution to some of the most painful experience in development and deployment we have ever had. Just imagine the all-too-common case where before the pending deployment the Dev has to ask Ops to upgrade in production one of the binary dependencies to release x. y. z or to supply new environment variables and command-line flags to enable some new features. This stinks on so many levels. Besides the velocity loss and communication overhead, it is just so easy for the application to accidentally rely on artifacts from the previous release or outdated environment variables we forget to unset. If the upgrade aborts, rolling back all the (not atomic, not idempotent) steps will be just as painful and error-prone. Isn\u2019t it wonderful if developers could install all the dependencies themselves, implement the application, ship exactly what they have just built, and deploy it in an atomic, hermetic unit? It is such a simple idea with immense repercussion. Docker is the state-of-the-art implementation of this idea. Hermetic Encapsulation of Application, Configuration and Dependency: Product shipping includes not just the application but also its configuration (environment variables, flags, etc) and external dependencies (Debian packages, vender of Golang, node_modules of node. js, etc. ). These are packaged into a single Docker image. A Dockerfile outlines the steps of how a Docker image is built. By defining all the configuration and installing all dependencies through Dockerfile, the output Docker image is self-contained with all we need to run what we ship. It offers hermetic deployment because the container abstraction provides simple isolation boundary that ensures that the environment configuration does not escape to host or to other containers (sandbox) and it cannot be altered by either the host OS or other containers without explicit authorization (lockbox). When the container is brought down, the entire environment is brought down with the application. Nothing in the environment lives longer than the application itself, a simple yet powerful abstraction. The Container Metaphor: Universal Tooling regardless of Language and OS Distro: Docker produces a single artifact from each build. No matter what language or framework the application is implemented in, no matter what distribution of the Linux the host runs, the output is always multi-layered Docker image, which is built and handled by the Docker tooling. This is the shipping container metaphor. A single, transferable unit that universal tooling can handle, regardless of what it contains. Like a container port, any Docker tooling (such as an image registry that stores and transport images like a host git repository would for codebase) has to ever deal with one kind of package \u2013 the Docker images. This is powerful in two ways. It means all the Docker tooling you or others make will be reusable to everyone else. It also means the build artifacts are extremely portable, which can be deployed on any system with a Docker daemon running (profound impact in the world of hybrid cloud). Atomic Upgrade: As presented in the example at the beginning, upgrades in the system are often performed in a non-atomic, multi-step operation \u2013 download, patch, coalesce. Is it possible to simply pull down a new (light) release image, deploy it, and if things go south, quickly roll back to the previous image we were using? Docker does exactly this, because a Docker image encapsulates all the patches and configuration. Docker can instantiate an instance using the new image just pulled and if everything passes, do an atomic rename. Fast, Ephemeral and Stateless Instances: A container is just a process that talks to the Linux kernel directly. It takes seconds to start and run a Docker container, while it takes minutes at best for a virtual machine. Virtual machines are long-lived in nature, as they aim to simulate actual hardware and runs guest OS that might be different from that of the host. Containers, on the other hand, share the Linux kernel and the images from which they instantiate are often just a few megabytes, while a VM image usually takes a few gigabytes. Such lightweight property means containers are perfect for stateless, ephemeral, elastically scaling systems. "},{id:57,url:"/istio/",title:"Istio: Noninvasive Governance of Microservices on Hybrid Cloud",body:"2018/02/02 - As presented in my previous post, microservices are the state-of-the-art architecture for building scalable, highly-available, manageable backend. \xa0No more 30-minute build time, single point of failure, and constant regression from crazy linking or backdoor dependency. But as one breaks off the monolithic application into microservices, new challenges surface. Procedure calls are now inter-process communication through the asynchronous network, which needs service discovery, load balancing, authentication, authorization, liveness detection, traffic monitoring, etc. Handling all these not only is a ton of work but also requires code change to your services.   Istio addresses many of such challenges during the migration towards service mesh. The term service mesh is often used to describe the network of microservices that make up such applications and the interactions between them. The most profound innovation of Istio is perhaps the noninvasive approach to connect, manage, and secure microservices. It does so using Envoy, which is a high-performance proxy from Lyft. Istio pairs each of your containers with a proxy that intercepts and redirects all of your ingress and egress traffic. By configuring on the proxy specific routing rules, access control, and certificates through the control plane, microservices management can be easily achieved.    Take some powerful use cases for examples.   Canary release or A/B testing: configure your routing rules to redirect your downstream service requests to the instances of different versions. Secured RPC: Receive certificates from control plane which acts as CA / root of trust. Bootstrap secure channel using recipient\u2019s public key in certs. Exchange symmetric key\xa0through this channel. Carry on communication using the symmetric key. Policy enforcement (access control, quota limit, etc. ): Proxy queries the control plane for policies regarding the type of request it received and the client identity associated with such request. Cache the results afterward. Enforce the policy on proxy. Telemetry: Monitor all traffic in the service mesh with data reported by proxy. But how does Istio ensure that a proxy is paired with a container on creation? The answer is Kubernetes (k8s) pods. A pod is an atomic orchestration unit that consists of one or many containers with shared storage, network namespace, and a specification on how to run the containers. It is an atomic unit because all containers in a pod are always run together, creating a close coupling that is exactly what Istio wants for the proxy and service. But Istio is noninvasive, and the pod spec from the application does not include the pairing of proxy. Then we are back to the question of how does Istio ensure that a proxy is paired with a container on creation? The answer lies still in k8s. Istio uses the k8s initializer, which is an extension mechanism that allows the injection of a container before the pod (or any k8s objects) is created without modifying the pod spec. Apparently, Istio injects a proxy to each container in the pod. Back in the days when pods and k8s initializers were first introduced, people doubted that such feature is useful at all and might just be over-design. But after the debut of Istio that takes advantage of this, the visionary and insightfulness of its creators cannot be more evident. "},{id:58,url:"/mq/",title:"Killer Apps of Message Queues",body:"2018/01/16 - Message queues are an asynchronous inter-process communication protocol that gains much of its glory with the recent hypes in microservices. Senders and receivers do not interact with the middleware at the same time, so message queues are a great way of decoupling communication from business logic since upstream services may talk to the middleware only and not with any other services.   MQs are not a panacea. It introduces additional components to the system and thus new challenges in availability and reliability. The extra roundtrip to downstream receivers means extra delay. The rule of thumb would be whenever synchronous invocation makes more sense, do RPC and not MQ. Let\u2019s focus on the killer applications of MQ in the following text. Buffering Traffic Spikes to Downstream Services: One downside of RPC framework is that the downstream services/callees have no flow control. If the downstream service demand is much higher than upstream request rates, especially so when downstream handles complex business logic with atomic, durable operations, then essentially the downstream is DOS-ed and that service is gone. Common solutions include upstream queuing to limit request rate or downstream queuing to limit execution rate, yet both introduces unnecessary complexity to the service code.   With MQ, flow control could be easily achieved with downstream pulling from MQ-middleware at its own rate that matches its service demand, which may be further optimized with batch-writes or asynchronous durable writes. Pub/Sub to remove reverse dependency: Imagine a user makes a new post on a forum, which is going to reward the user with some points, update the user personal stats, push to other user feeds, etc. It seems an invocation relationship between upstream and downstream services, but the upstream hardly cares\xa0about the results of downstream execution and just want to make sure things eventually get executed. Using RPCs puts reverse dependency into upstream because whenever a new set of logic or a new service was added to the list of callees, developers have to change the upstream service code, build, and deploy. Downstream service outage also affects upstream availability.   Using MQ, new downstream service could subscribe to the upstream and thus no code change to the upstream. The upstream also no longer needs to wait for all downstreams to reply, reduces service demand, and returns success once MQ says the message is committed. Higher throughputs in long, synchronous tasks: There are also times when the upstream cares about the results but execution takes a while (for example third-party service call through public network, such as requests to PayPal payment). The upstream may call the API directly and receive an acknowledgment. After the transaction is processed, Paypal invokes a callback to a uniform gateway of ours, which pushes to MQ. The upstream subscribes to such messages and gets results eventually.   "},{id:59,url:"/sec/",title:"A Primer on Secure Communication Channels",body:"2017/09/10 - In the world of internet, sending messages in clear text is like swimming naked. We would love some secure communication channels free from eavesdropping or tampering. Security as such is not some trivial question. An evolution of designs is presented below to illustrate the challenge we face and how we end up trusting trust (PKI-CA). Design 0: Clear text: The reason clear text transmission is unsafe is that the connection between the two parties is logical. Packets are in fact routed multiple times before reaching destination, which means any intermediate node has access to the payload, hence no privacy. Exposures to the intermediate nodes are unavoidable. It is the way the internet was built and the reason the internet can be scalable to billions of users. It is funny that for hundreds of years mathematicians had engaged with cryptography but were unsure how it might be applicable to anything if at all. Luckily when the internet started booming and security concern raised, we had every tool in our hands. Design 1: Symmetric key: It is assumed that an encrypted message cannot be decrypted in tractable time unless one holds the decryption key. When the encryption and decryption keys are the same, it is said to be symmetric. One major challenge when using cryptos is the distribution of secrets, in this case the symmetric key. If the internet is the only communication medium, then any sharing of keys is in clear text (no encryption before knowing the key), and all bets are off once any third party knows the secret. If you want to encrypt the key before sending over, then you need another key for this encryption and you still face the challenge of secret sharing. We might approach the problem by hard coding it to the two communication parties, but the client code is not secure. It is subjective to manipulation even in binary format. Design 2: Public key and private key: We may choose different keys for encryption and decryption respectively and share the encryption (public) key only. Any message encrypted with this public key can only be decrypted with the private key, which is assumed to be known only to the principal. Each party mints a key pair and it never has to change. But sharing the public key leads to the initial problem of secret distribution. When Bob receives a key marked as Alice\u2019s, how can Bob be sure? Mallory in the middle could easily swap the key with hers so that when Bob thinks he is talking to Alice, it is actually Mallory. Design 3: Root of trust and public key as identity: To ensure that the key Bob receives is indeed Alice\u2019s public key, Bob needs to hear it from someone he trusts before he can trust Alice, if we assume trust is transitive. However, to find that someone Bob trusts, Bob has to trust another one who trusts that someone. To end the inductive chain, we must have some root of trust. In the internet, it is the certificate authority (CA). A certificate of Alice is Alice\u2019s public key digitally signed by the CA. The digital signature of any content is the hash of that content then encrypted with the CA\u2019s public key. All CA\u2019s public keys are baked in the client, like your browsers. If Bob trusts the CA, who trusts Alice, then Bob could trust Alice given a signed certificate. With knowledge of Alice\u2019s public key, Bob may bootstrap a secured communication with Alice by encrypting his public key using Alice\u2019s public key. Only Alice will be able to decrypt this message, who may reply messages encrypted using Bob\u2019s public key. Since asymmetric keys are computationally expensive to use, they are often used to bootstrap a secure channel through which a symmetric key is shared. Subsequent communication is then encrypted and decrypted using that symmetric key. Problem remains: The trust aforementioned refers to the confidence in one\u2019s identity but says nothing about the information provided or assertion made by that principal. Also in such centralized design, once the root of trust compromises, then all bets are off. DigiNotor is a contemporary example. "},{id:60,url:"/multi-repo/",title:"Dependency Update and Artifacts Promotion in Multi-repo Project",
body:"2017/08/13 -  We all know Google employs a version tracking system that uses a single repository/depot. Every close-source google product that you love is tracked by this single repo, which is so large that it cannot fit onto a single disk drive and must be hosted on the cloud. During my first encounter with the monstrous system, I was just as baffled as you might be, but I have come to appreciate the benefits (and costs) of a single repo after I start working on the Istio project. An open-sourced management platform for microservices, Istio presents a uniform abstraction over heterogeneous cloud vendors to support canary release, policy enforcement, telemetry, and much more (shameless plug). It consists of multiple repositories on Github with the vision that each module could be used independently outside of Istio, except that for Istio itself the independence we would like does not exist due to the dependency among these repos. By dependency, I mean each repo needs other repos to build itself. Istio uses Bazel\xa0build tool. All dependencies are defined in the WORKSPACE file at the root directory of each repo, and each dependency includes a commit SHA pointer that specifies the exact version used in this build.   One nice feature of a single repo is the ability to make atomic changes given the property of a single point of serialization. Each commit/snapshot/diff/view may touch multiple files and is applied to the code base atomically (analogous to transactions). For files spanning multiple repos, atomicity is no longer guaranteed. The major challenge in a multi-repo scheme is staleness. When the stable branch of a dependency advances to a new version, the parent repo still uses the dependency of the older version, unaware of such an update. To change the SHA pointer that the parent repo has, it is going to be a separate PR on the parent aside from the one that updates the dependency, which is the reason atomicity is gone. Yet having one single repo makes continuous integration slow, where each PR must pass the pre-submit test before merging. Even a one-line change runs tests on the entire project to prevent regressions. As the repo grows in size, so does the test suite. When each CI takes two hours to complete, productivity suffers. Google\u2019s solution is Blaze (whose open source version is Bazel, shout out the to the anagram), which defines a hierarchical build dependency so that the affected files by any code change and be exactly identified, so only the affected tests need to be run. Multiple repos, on the other hand, make CI much easier since tests could be partitioned in different repos and PRs on each repo only triggers tests on that repo. Back to our problem of stale dependency. For changes involving separated shards/participants, the first idea that comes to mind is distributed transactions (say two-phase locking and two-phase commit). But it means an entire revamp that requires additional tooling on code reviewing multi-repo, aggregating pre-submit CI testing, etc, which is likely to take up the entire quarter and everyone on the dev team still suffers in the meantime. We prefer something less invasive but sooner to deployment. The design is to let go of strong consistency for eventual one, by running cron jobs periodically checking dependency versions and if changed, create a PR on the parent repo. It may take a few runs for a change in the leaves to propagate to the root, but it is okay for the repo to be stale by a couple of hours, because we know eventually the entire project is consistent. If you are interested in the binary used to do this, check this out. "},{id:61,url:"/session/",title:"Session Consistency in Replicated Frontend Servers",body:"2017/07/02 - HTTP provides an abstraction of short connections. Unlike the continuous byte streams in TCP, exchanges between client and server over HTTP starts with a client request and ends with server response, which is meant to be stateless. Sometimes, the server needs more context of the conversation to properly respond, for example, confirmation of user login. Information as such persists throughout the session with the server. When there is only one instance of the server, that is where the session resides and session consistency follows because of the trivial single-copy semantics.   This design, however, is by no means scalable or highly available. Oftentimes, such frontend servers are identically geo-replicated and routed with a reverse proxy. The challenge is how to maintain a session even if subsequent client requests hit some different server. Several solutions are presented here and the pros and cons discussed. Server synchronization: gossip/epidemic/anti-entropy: If the session is synchronized among all servers, then it does not matter which one the client hits. We do so by having servers exchange all session information.   It is nice that such a change affects no application code. However, synchronous server exchanges impose additional delays to ensure all other replica servers have received such session info, and worse, then entire endpoint becomes irresponsive during network partition among servers. Asynchronous server exchanges, on the other hand, provide only weak/eventual consistency, so in our example, users might be asked to log in again even after they have done so. Lastly, such a scheme does not scale as the number of exchange messages grows exponentially with the number of servers. Consistent hashing: If we ensure that requests from the same client always reach to the same server, then we do not have to deal with session replication and consistency anymore. Instead of doing randomization or round robin, perhaps the reverse proxy could be based on IP address (Level 4 routing) or business logic property such as user_id, order_id, item_id, etc (Level 7 routing). This is nice since we only need to change the Nginx configuration and application/server code remains intact. Loads at the servers are also balanced, assuming good hashing scheme. Also scalable with more server instances. However, this does not solve the high availability part, since whenever the server fails or partition happens, clients are unable to talk to the original server and must be remapped to a different server and log in again. Upon replica set resizing, redistribution of session information must happen the same client might be hashed to the newly added servers. Client-side storage: To manage the session information on the client side, one might take advantage of the browser cookies, which relieves bandwidth and storage burden from servers. However, it might be subjected to illegal mutation, leaking, and other security threats. The session is also limited by cookie size. Server-side durable storage: This is, in my opinion, the best solution. We keep the session information in our backend storage, which makes the frontend server truly identical and stateless. You may ask what if the storage fails. That is an excellent question, and the state-of-the-art answer would be sharding, replication and high-power consensus (paxos, raft, viewstamp). It makes more sense to consolidate these properties as a service and build other services on top of that (Chubby, Zookeeper, BigTable).   "},{id:62,url:"/pagination/",title:"Pagination Ordered by Secondary Keys on Sharded Stores",body:"2017/04/23 - A common design for content display, pagination partitions information into multiple pages and serves one at a time. We have seen it in search results, message history, and cascading news feed, etc. It shrinks the payload of server response and therefore reduces the response latency. It is often the case that the table in question (1) has a primary key, such as orderId, itemId, msgId, and (2) pagination is sorted on some secondary key such as time, price, popularity. When the table is small, indexing on the secondary key plus the SQL offset/limit in will do. For higher throughput, the table might be sharded on a partition key, and each table running on different nodes governs a disjoined segment of key space. The partition key is often chosen as the primary key, which makes pagination using secondary keys much more challenging, because none of the nodes have a global view of the entire dataset. We describe three approaches to this problem. External Merging: Say the client wants the 3rd page sorted by time from the storage of two shards. Entries are likely interleaved between the two shards, but an extreme case would be that the latest 3 pages all reside on either of the shards. For correctness, we must read the first 3 pages (not just the 3rd) from each shard, then merge the responses in memory on service tier and return the 3rd page from the total order. This may be the most obvious solution, but it does not scale. Imagine the client wants the 100th page and each page has 200 entries. Or you have got multiple shards that are geo-distributed. Now you have to read 20000 entries from each shard and then suffer the high network latency as well as high CPU usage. Compromise: No Skipping but only Next Page:  The limit on the page index query only allows the client to visit the next page (e. g. your facebook news feed). This is essentially an optimization over external merging in that each shard always returns only one page for each request. The coordinator/aggregator merges the result and records the highest value of the secondary key from each shard. On the fetching next page, such value is used as the selection condition to get the next load of one page from each shard. Lookup Twice: Assume each page has 5 rows and we were to get the 200th page. The SQL query is    1select * from T order by time offset 1000 limit 5; If there are 3 shards in total, rewrite the query as    1select * from T order by time offset 333 limit 5; and the following is the result from each shard.   Then, do a range query using the\xa0between directive, which starts from the min and ends at the local_max. The results are shown below.   We easily conclude that\xa0min has offset 333 on shard 0, 331 on shard 1, and 330 on shard 2 (offsets rounded down since min does not really exist on the other shards). It follows that min on the total order has an offset of 333+331+330 = 994. This is a crucial piece of information, because after an in-memory merging (results from each shard already sorted), we know that the entry 6 rows below min is of offset 1000 on the global order, and limit 5 is just the next 5 immediate neighbors. "},{id:63,url:"/kip/",title:"Kip\u2019s Warehouse: Building Scalable, Reliable, Consistent Web Application from the Ground Up",body:"2017/02/25 - I have been working with another three wonderful people on the senior design project, which is a web application of an inventory management system, and the production is up at kipswarehouse. com. The Department of Electrical and Computer Engineering at Duke will be the first customer for our system to manage the items and requests, etc for all ECE labs. Our tech stack is summarized below.    12345678910111213Front-end \xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 React + BlueprintJSBack-end \xa0\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0node. js + ExpressWeb Server \xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 nginxDatabase \xa0\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0MySQLORM \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0SequelizeBackup \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0AWS S3 + GlacierAPI \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0json RESTCICD \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0TravisCIVersion Control \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 GithubCode Review \xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 PhabricatorVPS \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0DigitalOceanDNS + CDN \xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 CloudFlareRemote Process Monitor \xa0\xa0\xa0\xa0keymetrics This is a super awesome opportunity. For all my previous internships, I came in with a colossal amount of legacy code that needs to be maintained and carefully updated. This project the very first time where we decide on EVERYTHING in our tech stack, deploy and evolve. This is also the first node. js application I built. For a semester-long project like this, we prefer fast development and well support. The node engine interprets the script at runtime, so every time we make a change to the server code, no recompilation is required. Our frontend is ReactJS, so it makes sense for our RESTful API to be done over json, and serialization/marshaling/flattening/pickling for RPCs is given for free. We are also using Sequelize as our ORM to avoid handing writing SQL query ourselves to risk performance. Sequelize by itself has been robust and well documented, but it certainly brings new complexity, if not unnecessary. One notorious case we run into is the transactions on the\xa0database. In SQL query, a transaction is a really simple yet powerful abstraction. Start Transaction; Commit; Boom. Two extra lines and you got atomic, consistent, independent, durable records. Sequelize, however, implements it with promises and callbacks. Doing transactions in Sequelize turns into nested callbacks with the transaction object passed all the way into the deepest one. Worse off, if one runs into bugs such as accessing fields on an undefined object, the transaction simply aborts without telling what went wrong in between (which reminds me the dark old days of mysterious segfaults). Normally node. js handles this case well, but now Sequelize masks it and debugging could have been easier. I attach some slides I used for the presentation of our alpha release. We are definitely hyped about this and will keep y\u2019all updated! (Oh good lord of Richard Hendricks, please don\u2019t sue us. )          "},{id:64,url:"/git/",title:"Git as Version Vector",body:"2016/12/22 - Git is one of the most widely used version control systems. Traditionally, arepository on git is considered as a complete history of the entire project inthe form of chains of blocks. Each block is a commit, and one could create aversion branching from any point throughout the chain. Alternatively, one may view git as a highly available system that implementsversion vectors. It is highly available in that whenever you push to your localrepo git never rejects you, but of course that local update is tentative untilit is appended or merged with master. The interesting aspect of git arises whenyou branch off master and work on a feature branch, while others at the sametime could do exactly the same. This is a concurrent update. We are speaking of concurrency in a rather strict sense. Consider thefollowing example.   When C3 branches off C1, C3 must have seen C1 as well as C0, so we say C0causally precedes C1, and C1 causally precedes C3. Causality here is defined asa happened-before relationship, where C0 might not necessarily cause\xa0C1 but itcould. However, when someone else appends C2 onto C1, he or she has not seen C3or C4, and the person working on C3 and C4 has not seen C2, such that C4 doesnot causally precede\xa0C2 and C2 does not causally precede C4. Therefore, C2 andC4 are concurrent. Remember that git ensures a total order of updates/commits as seen by all theparticipants, which means concurrent modification as such must eventually beserialized as either C2 precedes C3 and then C4, or C3, C4 and then C2. Whichorder to choose is rather arbitrary but what matters is that once a causal orderis chosen, the rest of all participants must agree and observe such ordering. Concurrent updates might be conflicting, and that is why the git mergingprocedure requires user involvement to resolve merge conflicts. But how does git know whether two updates are concurrent? Version vectors. Itall started with the hall of fame paper Time, clocks, and the ordering of eventsby Leslie Lamport, who defined a total causal order using the logical clock. Thelogical clock (LC) is just a monotonically increasing counter and each branchhas its own logical clock. Each commit on branch i is then stamped with &lt;LC_i,i&gt; and a causal order could be defined as from lower LC to higher LC and breakthe tie using i. Notice that there could multiple causal orders given a set of events, and theorder derived from LC while preserving causality imposes an unnecessary orderingconstraint on concurrent events, and worse, using just logically clock isinsufficient to tell whether two events are concurrent. Users might desire adifferent order after all. So we need a vector clock, which is a vector whose element at index i is thelogical clocks of branch i. Now we know update u causally precedes update v ifVC(u) dominates VC(v), and such domination is true if each logical clock inVC(u) is greater than or equal to that in VC(v) at the same index. Therefore, ifneither vector clock dominates the other, then we conclude that the two updatesare concurrent, which is where a merging procedure is needed. "},{id:65,url:"/quorum/",title:"Sloppy Quorum And Eventual Consistency",body:"2016/11/18 - Here is where we stand. Fisher-Lynch-Patterson has shown that consensus is notguaranteed in bounded time in a purely asynchronous network. The CAP theoremshows that from consistency, availability, and partition-resilience, we couldonly choose two. We have seen systems using ACID transactions and high poweredconsensus protocols such as Paxos, Viewstamp Replication, and Raft. We have beenchoosing to stand on the CP side, forgoing availability to ensure consistencyunder network partition. After all, isn\u2019t it nice for a read to always see the latest write? But availability is quite a sacrifice. When you Google something, or check yourTwitter NewsFeed, would you rather have some results/tweets show up, albeit alittle stale, than be denied service just because you happen to be on theminority side of a network partition? Or when I add an item to shopping cart, Ireally do not expect it to reject such operation in any case. It is worthwhileto reconsider the tradeoff between availability and consistency under thesescenarios. Classic consensus algorithms serialize all operations at the primary / leader /master / coordinator that maintains the single-copy semantics to preserveconsistency. Consistency could be generalized to a multi-coordinator case withNWR quorum where a write quorum intersects with a read quorum, or W + R &gt; N. Inthe following example, N is 5 and W = R = 3.   We could even optimize read for lower latency by letting R = 2 and W = 4. Thenodes might scatter across a wide area, and the less confirmations we have towait, the sooner we could commit.   We could push further down for lower latency in that R + W &lt; N + 1 where we haveonly eventual consistency. It is weaker since a write might not be immediatelyavailable for subsequent read, but eventually it will, as two quorums might bedisjoint. In the next example, Alice first writes a new value to v and then tellBob to read it, but Bob still observes a stale value.   Eventually, Bob will be able to read the latest write. But how eventual? Howlong does Bob have to wait? The answer is that we can give promises but we couldprovide expectations using probability bounded staleness as a way to quantifylatency-consistency trade-offs. We define t-visibility as getting consistentreads with probability p after t seconds. Here is some interesting figure fromLinkedIn.   "},{id:66,url:"/fourier/",title:"Fourier, Phasors, LTI and All That",body:"2016/10/08 - We all share the sorrow and misery from that signal processing class. \xa0You werethrown at some\xa0crazy formula, kind of know how to use them but probably neverunderstand why we are doing this after all. I hope this post helps with yourlingering confusion to have you realize the power if not the\xa0beauty of the toolsin your hand. Consider some\xa0vector in a 3D space. It could be represented by rectangularcoordinates &lt;x, y, z&gt; or spherical coordinates &lt;r, theta, phi&gt;. It is the samevector, after all, represented using different basis vectors. A time-varying signal is no different. It is represented as a linear combinationof an infinite set of functions each defined only in only a single point in timeand whose time components are separated by an infinitesimal\xa0piece. Theinteresting thing is, we could choose a different set of basis vectors that alsospan such infinite\xa0vector space to represent the very same signal. And this iswhen the Fourier basis comes into play. Representing the signal using the Fourier basis essentially transforms thesignal from the time domain to frequency domain. Why is this helpful? Well, anytwo\xa0Fourier basis vectors are orthogonal to each other, which means their innerproduct is zero, which is a fancy\xa0way of saying they are \u201cperpendicular\u201d to eachother, even though it is hard to visualize what it looks like in theinfinite-dimension space. This property makes the calculation of thecoefficients of the linear combination of the\xa0Fourier basis really easy toderivate. The full power of Fourier transformation is realized in the LTI system. LTIstands for linear, time-invariant. Any LTI system possesses both scaling andsuperposition properties. It means if an input signal is represented as a linearcombination of\xa0Fourier basis vectors, then we could analyze it on a term by termbasis, and the final output is the addition of all. But the punch line is this: each Fourier basis vector is the Eigenfunction ofthe LTI system, so if it is the input to some LTI, then its output is the samebasis vector scaled by an\xa0Eigenvalue. The\xa0Eigenvalue here is what we referred toas the frequency response of the LTI system, which is also a phasor. We are using vectors and functions interchangeably in this particular construct. Once we know the frequency response, finding the output signal of this oneFourier basis vector is easy, and the total output is just the sum of outputsfrom the spectrum of frequencies. "},{id:67,url:"/rsm/",title:"Reliable & Consistent Service: Linearizable RPC and Replicated State Machine",body:"2016/09/13 - Remote Procedure Call (RPC) is a canonical structuring paradigm forclient-server/request-response services.   This simplified diagram overlooks the challenges we face such as unreliablenetworks and remote server crash and recovery. In the famous Birrell/Nelsonpaper, at-most-once semantics was proposed that request is resent upon timeoutso it eventually does reach the\xa0server, and that server is stateless so itrestarts when failed and continues service probably retransmitted request (afailed server is a slow server). But then a request might be retransmitted right before the response\xa0arrives\xa0sothat the operation might be executed twice.   In the 80s, people believe that requests are essentially read and write, both ofwhich are idempotent, meaning executing them twice will not change the response,so at-least-once semantics are fine. \xa0For a long time, we thought at-least-oncesemantics + idempotent operations = linearizable RPC, but it turns out it is notnecessarily true. Consider the following example. The very last read shouldreturn 3 in a linearizable system. The takeaway is we need exactly-oncesemantics.   Reply cache has come to rescue. Each RPC is tagged with an id and its returnvalue is put in the reply cache such that when the retransmitted RPC arrives,the server does not re-execute and return the value in cache immediately. Thecache entry is evicted when the client ACKs. Using a reply cache achievesat-most-once semantics, which combined with retransmission achievesexactly-once\xa0semantics. But what about failovers? When an instance of the server fails, all loads areshifted to other servers. To get\xa0consistent results requires the reply cache tobe durable, which means each operation replicated across the entire server grid. This is why linearizability is so important. A concurrent implementation iscorrect if its behavior is equivalent to some serial order. Each server could beviewed as a state machine and with a sequence (linearized) of applied actions,its final state is deterministic and could be replicated. Paxos, ViewstampedReplication, Raft are all consensus algorithms for replication. The one last twist is that upon failovers, the client needs to keep track of thenew elected master/leader and send requests to her. Of course, the clientapplication\xa0is not the one to\xa0do such bookkeeping.   Take Viewstamped Replication for example. VR Proxy on the client side keepstrack of the current master. On the server side, it is the VR code that managesreplication, election, etc. It is therefore independent of the underlying servercode and could be reused readily.   "},{id:68,url:"/microservices/",title:"Service-Oriented Architecture: Why did Microservices Catch On",body:"2016/09/04 -    All teams will henceforth expose their data and functionality through service interfaces.   There will be no other form of inter-process communication (IPC) allowed: nodirect linking, no direct reads of another team\u2019s data store, noshared-memory model, no back-doors whatsoever.   The only communication allowed is via service interface calls over the network.   Anyone who doesn\u2019t do this will be fired. \xa0Thank you; have a nice day!  \u2013\xa0Jeff Bezo, Amazon CEO You may have been wondering why in the world would anybody prefer the extralayer of abstraction (the service tier) instead of just doing things the oldway.   Microservices are really just a more concrete and modern interpretation ofservice-oriented architectures (SOA). It goes one step further by taking on theUNIX design philosophy where each service/command does only one thing and doesit well. \xa0To help understand the advantages of SOA,\xa0I have listed out somepotential problems that one could encounter without the service tier and I willexplain how SOA has\xa0cracked them. Duplicated code: Consider the following hierarchy where\xa0all three types of requests do queries on user data.   Without the service tier, if requests are handled by different servers, theneach of the servers need to implement the SQL query individually, whichessentially creates duplicated code. Spread of Complexity: As concurrency bulks up, database queries become bottlenecks. Cache is thenintroduced to the to hierarchy to mitigate\xa0query\xa0volumes that actually hit DB. Without a unified service tier for user_db queries, upper layers must upgradeto deal with the new cache layer directly. This new directly-exposedrequest-unrelated complexity forces all servers to upgrade. Imagine doing thisover and over as your business goes. Did I mention you also need to keep trackof all different places that are impacted every time you make a new\xa0change? Reused but\xa0Coupled Library \xa0: SOA is not the only solution to the two problems aforementioned. Extractingcommon\xa0actions to a library, say user. so, \xa0is mostly the first solution found. Single point of update, hidden implementation, unified interface, great. Butdoing so introduces new problems \u2013 version maintenance\xa0and coupling of code. Ifserver pipeline A handling request type A upgrades user. so from version 1 toversion 2, other\xa0server pipelines are forced to upgrade to be compatible withthe new version of user. so, the comeback of problem 2. Or, we could retain adifferent version of\xa0user. so based on the pipeline, but then problem 1 arises. Performance Impact by\xa0Other Server Pipelines: Each server executes SQL queries on its own. If server A does a full scan onuser_db and takes DB CPU \xa0to 100% to usage, it essentially blocks all otherservers from making progress. \xa0  How SOA solves them:  Single point of change. Fix one bug in service tier you fix them all.  Unified abstraction for upper layers hides implementation details. Changessuch as new cache, extract db, split\xa0table preserves the interface exposed toclients, who no longer needs to keep on updating along with the service.  With the new service tier  Now the user_service could mitigate/schedule queries to DB and no single server could have it all, which preserves the minimum performance spec.  Easy to use. Before you need crazy linking and binding for communication, ifnot serialization, reliable transmit, background execution, etc. Now it is allRPC which appears no different from invoking local procedures. "},{id:69,url:"/git2/",title:"Git: Branch off An Unmerged Branch While Committing Often - Disasters and Salvage",body:"2016/08/14 - Committing often and pushing often has been advocated as good practice whenusing Git, which saves your latest work on remote even if your hard drive diesright after and which provides a more fine-grained\xa0selection when rolling back. It is also good practice to make\xa0each pull request consist of\xa0only\xa0one commit,so that it is easier to cherry-pick stable features from master to releasebranch. So what I do is to squash all commits into one right before I send themout for review. Sometimes, I would branch off the current branch I was\xa0working on but wasblocked when the project was building/compiling or my reviewers have yet to getback to me. It works out by having multiple local repos in differentdirectories. The problem comes in when I later make more commits onto the original branch Ijust branched off, as illustrated below.   Say we rebase from the master branch at commit C0, and we branch off fromFeature A at commit C1 to work on Feature B. Later, our reviewers finallygot back to us and we continue work on Feature A to create commits\xa0C2 andC5. Now Feature A is ready for the pull request if squashed. The problemshows up when\xa0some files are touched by both\xa0the branch Feature A and FeatureB. If we squash Feature A,\xa0submit\xa0the pull request from Feature A againstmaster, and finally rebase Feature B from master, crazy merge conflictsarise.   The conflict results from the fact that both Feature A and Feature B touchthe same files and that we rewrite history by squashing (interactive rebase) allcommits of Feature A, including those between C0 and C1. Now S0 is anew\xa0commit different from any of Feature B, and the merge conflicts really isabout the conflict between S0 and C0-C1. Resolving this conflict by hand is a nightmare, because Git applies commits oneby one chronologically, which means we have to resolve the conflict again andagain until the last commit. I encountered this problem in my last internship. At first, I tried to resolveall conflicts by hand and it was not pretty and took me an hour to do so. Thisproblem kept showing up and I believe there must be some other way. I did eventually find a solution.   I first squash all commits on Feature B right until the one it branches offFeature A. Then, squash Feature A until C0. Notice that S2 is now danglingas its parent is not in the source tree anymore. \xa0Then, instead of a merge,cherry-pick S2 to apply onto S1, and do a hard reset to have Feature Bpoint to the new commit. Now approve the pull request to merge\xa0S1 to master. After that, when youcheckout Feature B and do a rebase from master, since the exact S1 isalready in master, no conflicts arise because of Feature A. (there mightstill be conflicts of course if your teammate pushed the delta on the same filesright before you rebase). The bane of the problem might just be that the Git way of taking snapshots of abranch, in that it allows the coexistence of\xa0multiple snapshots (i. e. commits)on a branch, which only comes in handy on fine-grained rollback but also bringsmore trouble in a more common setting in my opinion. Google uses customizedPiper for version control, in which deltas are submitted in the quantum of achange list of files, which minimizes the problem encountered here. "}],idx=lunr(function(){this.ref("id"),this.field("title"),this.field("body"),documents.forEach(function(e){this.add(e)},this)});$(function(){$("#lunrsearchresults").on("click","#btnx",function(){$("#lunrsearchresults").hide(5),$("body").removeClass("modal-open")})});