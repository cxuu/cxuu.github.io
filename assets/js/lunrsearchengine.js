function lunr_search(e){if(document.getElementById("lunrsearchresults").innerHTML="<ul></ul>",e){document.getElementById("lunrsearchresults").innerHTML="<p>Search results for '"+e+"'</p>"+document.getElementById("lunrsearchresults").innerHTML;var t=idx.search(e);if(0<t.length)for(var o=0;o<t.length;o++){var a=t[o].ref,n=documents[a].url,i=documents[a].title,s=documents[a].body.substring(0,160)+"...";document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML=document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML+"<li class='lunrsearchresult'><a href='"+n+"'><span class='title'>"+i+"</span><br /><span class='body'>"+s+"</span><br /><span class='url'>"+n+"</span></a></li>"}else document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML="<li class='lunrsearchresult'>No results found...</li>"}return!1}function lunr_search(e){if($("#lunrsearchresults").show(400),$("body").addClass("modal-open"),document.getElementById("lunrsearchresults").innerHTML='<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>',e){document.getElementById("modtit").innerHTML="<h5 class='modal-title'>Search results for '"+e+"'</h5>"+document.getElementById("modtit").innerHTML;var t=idx.search(e);if(0<t.length)for(var o=0;o<t.length;o++){var a=t[o].ref,n=documents[a].url,i=documents[a].title,s=documents[a].body.substring(0,160)+"...";document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML=document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML+"<li class='lunrsearchresult'><a href='"+n+"'><span class='title'>"+i+"</span><br /><small><span class='body'>"+s+"</span><br /><span class='url'>"+n+"</span></small></a></li>"}else document.querySelectorAll("#lunrsearchresults ul")[0].innerHTML="<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>"}return!1}var documents=[{id:0,url:"/404.html",title:"404",body:" 404 Page does not exist! Please use the search bar at the top or visit our homepage! "},{id:1,url:"/about",title:"Who am I",body:"               Charles Xu          mail@charlesxu. io          cxuu       Charles .  I am a Senior Plumber at Snowflake building and unclogging the things that run things.  I am also studying Engineering Management on the Stanford Farm.  Previously, I worked at Cruise developing multi-tenant container platforms with self-driving Kubernetes and at Google Cloud building (almost) production-ready service mesh Istio.  Before moving to the Bay Area to pay so much in rent for so little, I was an undergrad at Duke studying CS, EE, and Finance. I would be a totally different person had I not met my rock star research advisor Prof. Jeff Chase. He taught me to make the investment early that pays dividends in the future. He exhorted me to be serious and sincere about life and work\u2014not to fight a fake war or someone else's war but to strive for what matters to the living or the dead.  He is also probably the only person who has read my thesis cover to cover.  I have been writing on this blog since 2016. My technical interests are Distributed Systems and Networks in general, but this blog engages a wider audience. It stands as a living record of my growth and struggles, triumphs and lessons in my work and life. I hope you find them useful, too.  I am always excited to hear from folks. Drop a note! "},{id:2,url:"/bookshelf",title:"Bookshelf",body:"These are the books I have read and recommend, in no particular order. Titles in bold are the ones I enjoyed the most. I think of life as a search problem. Yet in a lifetime, I could only traverseso many paths and possibilities. I find reading a powerful and rewarding passionthat allows me to learn what others have explored. With millions of books to choose from, we are faced with another search problem. Like living, reading is highly personal. Alas, I hope this list is useful if youshare my interests below.  Engineering Management Personal Development Career Growth Startups and Ventures Finance and Markets ProgrammingEngineering Management:  The Making of a Manager: What to Do When Everyone Looks to YouA good manager improves collective outcome with multiplier effects. Focus on purpose, people, and process. Prepare for meetings. Feedback is a gift. Manage expectations. Trust, delegate, grow.  The Manager's Path: A Guide for Tech Leaders Navigating Growth and Change Good Boss, Bad Boss: How to Be the Best and Learn from the WorstPeople quit bad managers, not companies. Direct reports watch everything you do ( toxic tandem ). Watch your tone of voice, the way you look at people,the use of nicknames, a memory for faces, names and dates. These details refine your relationships. Effective leaders are both competent and benevolent. Humans prefer hierarchical relationships. The challenge is not to reinvent managementbut dampen known drawbacks. Personal Development:  * The Almanack of Naval Ravikant: A Guide to Wealth and HappinessRenting out your time or just working hard will not lead to financial freedom. 99% of effort is wasted. Wealth comes from judgment. Play long-term games with long-term people. Find a worthy mate; be worthy of a worthy mate. Find work that feels like play. All returns in life\u2014wealth, relationships, knowledge\u2014come from compound interest. Reading is faster than listening; doing is faster than watching. If you cannot decide, the answer is no.  Atomic Habits: An Easy &amp; Proven Way to Build Good Habits &amp; Break Bad OnesHabits are compounding and shape who we are and what we will achieve. The best way to build a habit is to make it part of your identity(I am trying to quit smoking vs. I am not a smoker), attach the new habitto existing ones, make it small (atomic), and surround yourself with peoplewho have the habits you want. Willpower is limited; instead, design the environment to support your habits.  Influence: The Psychology of PersuasionIn decision making, human often resort to shortcut/reflex, such asreciprocation, consistency between commitment and action, conforming withthose similar to us, adherence to authority, preference to folks we like, andequating scarcity with value. Awareness of such routines keeps oneself alertedto exploiters and magnifies your influence in work and life.  What I Wish I Knew When I Was 20: A Crash Course on Making Your Place in the WorldGrant yourself permission instead of waiting for others to do so. Cold email the people you admire. If you are not failing sometimes, you arenot taking enough risks. Do not burn bridges. You are not going to like everyone and not everyone is going to like you, butthere is no need for enemies. To make good decision in dilemmas, think about howyou want to tell the story in a future job interview. Recognize your mistake,apologize early and profusely. Career Growth:  So Good They Can't Ignore You: Why Skills Trump Passion in the Quest for Work You Love Follow you passion  is a bad advice. A rare and valuable job requiresrare and valuable skills (career capital). Acquiring more career capital withdeliberate practice gives you the autonomy to pursuit the work you love. Deliberate practice means doing things that hurt: playing guitar pieces aboveyour skill level, or practicing the tennis backhand that you suck at.  Never Split the Difference: Negotiating As If Your Life Depended On ItMirroring upwards invites elaboration. Mirroring downwards (late-night FM DJ voice)builds empathy, trust, and calmness. Humans are emotional, sometimes irrational. Label the counterpart's emotion and motivation to get them to say  that's right .  No  is not the end of negotiation. Explore alternatives.  The Holloway Guide to Equity CompensationDetailed explanations of ISO, NSO, RSU, and taxes with lots of references. 83(b) election. Secondary markets. Right of first refusal. AMT trap. Liquidation overhang.  The Coding Career Handbook. Guides, Principles, Strategies, and TacticsLearn in the public. Write a lot. Open source your knowledge. Good enough is better than best. Invest in new technologies. Know your tools. Startups and Ventures:  Zero to One: Notes on Startups, or How to Build the FutureCompetition is for losers. 0 to 1 is different from 1 to n. Every moment in business happens only once. It is easier to copy than to create. Leanness is a methodology, not a goal. Making small changes to things that alreadyexist might lead you to a local maximum, but it won\u2019t help you find the global maximum.  Blitzscaling: The Lightning-Fast Path to Building Massively Valuable CompaniesDo the things that don't scale. Prioritize growth and speed over efficiency. Good insights, but could be condensed into just one chapter.  The Hard Thing About Hard Things: Building a Business When There Are No Easy Answers Hello, Startup: A Programmer's Guide to Building Products, Technologies, and TeamsFinance and Markets:  * Fooled by Randomness: The Hidden Role of Chance in Life and in the MarketsWe often confuse luck with skills, and noise with signals. Investment returns areunpredictable. Stay in the game. Manage risk to never get wiped out. View the past and the futurewith probability/uncertainty, but understandthe difference between probability and expectation.  The Psychology of Money: Timeless lessons on wealth, greed, and happinessAim to be reasonable, not rational. The highest form of wealth is the abilityto do whatever wherever whenever. Note that financial independence is not exactlyabout maximizing returns. Few things matter more with money thanunderstanding your own time horizon and not being persuaded by the actionsand behaviors of people playing different games than you are.  Flash Boys: A Wall Street RevoltWall street in 2010s. High frequency trading is not market making. It createstwice the volume, no additional liquidity, and perhaps a slightly worseexecution price for the public. I wonder if network switches and FPGA got so much fasterbecause of it. Autobiography:  The Last LectureThe last lecture delivered by Professor Randy Pausch after his pancreatic cancer diagnosis. Feel empowered to dream big and enable others' dreams. Try hard. Be kind.  When Breath Becomes AirMemoir by a young neurosurgeon faced with terminal cancer. A bit about dying, but more about being alive. Programming:  Site Reliability Engineering: How Google Runs Production SystemsHope is not a strategy. Build systems to automate ops. Have an error budget instead of aiming 100% uptime. Monitor latency, traffic, errors, saturation. Push actionable alerts. Blameless postmortems.  Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems Everything Curl Kubernetes: Up and Running: Dive into the Future of Infrastructure Docker: Up &amp; Running: Shipping Reliable Containers in Production Terraform: Up &amp; Running: Writing Infrastructure as CodeBooks I Could Not Recommend:  The 4-Hour Workweek: Escape 9-5, Live Anywhere, and Join the New RichReading it felt like I just replied to a  get rich quick  email spam. I appreciate the emphasis on time management and delegation, but unfortunatelyMr. Ferris sees his employment no more than doing what was assigned, or merely an exchangefor money with time. I have to look elsewhere for aspiration, leadership, empowerment,relationships, and growth. "},{id:3,url:"/categories",title:"Categories",body:""},{id:4,url:"/wiki/go/development/",title:"More Effective Go",body:"Testing: Use google/go-cmp instead of reflect. DeepEqual to compareobjects, because cmp gives you detailed diff and allows flexible optionssuch as  IgnoreUnexported(typs . . . interface{}) EquateApprox(fraction, margin float64) SortMaps(lessFunc interface{})List of options: cmpoptsOptions for protobufs: protocmp Examples: 1234567891011import (   testing    github. com/google/go-cmp/cmp )func TestFoo(t *testing. T) {  if diff := cmp. Diff(want, got); diff !=    {    t. Fatalf( unexpected diff (-want +got):\n%s , diff)  }}During development, fmt. Printf( %+v , obj) shows the object in a readable way. Packages: It is an anti-pattern to have command-line flags or panics in external packages. Flags dictate how parameters are passed to the library and hence are not flexible. Unrecovered panics will crash the entire program, not just one goroutine. Use error instead. "},{id:5,url:"/",title:"Home",body:"{% if page. url == \u201c/\u201d %}  12345678910111213141516&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;Popular&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row &gt;{% for post in site. posts %}  {% if post. featured == true %}    {% include featuredbox. html %}  {% endif %}{% endfor %}&lt;/div&gt; {% endif %}  123456789101112131415&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;New Posts&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row listrecent &gt;  {% for post in paginator. posts %}  {% include postbox. html %}  {% endfor %}&lt;/div&gt;    &lt;/div&gt;  {% include pagination. html %}  "},{id:6,url:"/wiki/",title:"About This Wiki",body:"This is a knowledge base of tools, best practices, and productivity. Whereasblogs are my opinions and stories, wiki pages are living documents of indexedfacts for fast retrieval. It is unfortunately biased towards tech and startups, but I do not aspire toreplace wikipedia either. In spirit of open-source and learning in the public, Ihope you find answers here. This is work in progress. Let me know if you spot any error (contact info). Menu is on the left, or at the top for mobile. "},{id:7,url:"/inspirations",title:"Inspirations",body:"These are some inspiring people, newsletters, and websites that I regularlylearn from. Startups, Products, SaaS, Funding:  Paul GrahamCo-founder of Y Combinator and HackerNews.  Sam AltmanCEO of OpenAI. Former president of Y Combinator.  Andrew ChenGeneral Partner at Andreessen Horowitz. OG Rider Growth team at Uber. Prolific writer on consumer tech, including mobile, metrics, and user growth.  Lenny RachitskyFormer Growth Product Manager at Airbnb. Weekly newsletter on product, growth, people management.  Alex ClaytonGeneral Partner at Meritech Capital. Primarily investing in enterprise software and infrastructure. Gives insightful S-1 breakdowns.  Leo PolovetsA software engineer turned VC. General Partner at Susa Ventures. Generous adviceon seed-stage logistics, pitching, and fundraising Jos\xe9 Ancer - Silicon Hills LawyerA startup lawyer and Harvard JD who helps founders safely navigate high-stakeslegal and financing issues, while keeping VCs honest. Career Development:  Avery Pennarun Will Larson StaffEng Julie Zhuo Cedric Chin"},{id:8,url:"/wiki/istio/",title:"Istio",body:"Turn on sidecar proxy debug logging: 12kubectl exec ${POD_NAME} --   curl -X POST 'http://localhost:15000/logging?level=debug'Download istioctl at specific version: 1curl -sL https://istio. io/downloadIstioctl | ISTIO_VERSION=1. 9. 1 sh -Addressing long-running proxy in short-lived Pods: See The Good, Bad, and Ugly: Istio for Short-lived Pods. "},{id:9,url:"/wiki/kubernetes/",title:"Kubernetes",body:"API resources: List all API objects in a namespace: 12kubectl api-resources --verbs=list --namespaced -o name  | xargs -n 1 kubectl get --show-kind --ignore-not-found -n ${NAMESPACE}because kubectl get all does not really give you all the resources. Traffic: Port-forwarding: Listen on port 8888 locally, forwarding connection to 5000 on ${POD_NAME} 1kubectl -n ${NAMESPACE} port-forward pod/${POD_NAME} 8888:5000Note that pod/mypod can also be svc/ or deployment/. This is useful to troubleshoot in-cluster services without exposing them. Port-forwarding works only for TCP traffic at the moment. Pods and Shells: Execute a command in a Pod: 123kubectl exec ${POD_NAME} -- echo hello worldkubectl exec ${POD_NAME} -c ${CONTAINER_NAME} -- curl https://www. example. orgOpen a shell to a Pod: 1kubectl exec -it ${POD_NAME} -- bashSpin up a debug Pod and open a shell to it: 1kubectl run -it ${POD_NAME} --image=debian --rm --command -- shThis is useful when the application Pod is super stripped down, such as usingdistroless base image that makes troubleshooting difficult due to the lack totooling. Logs: Get last 20 lines of logs 1kubectl logs --tail=20 ${POD_NAME}Get logs in the last 3 hours 1kubectl logs --since=3h ${POD_NAME}Get tail logs and stream new logs 1kubectl logs --tail=20 ${POD_NAME} -fAggregate logs from multiple Pods using label selector 1kubectl logs -l app=server --tail=20 -fNodes: List all used NodePorts in a cluster: 12$ TEMPLATE='{% raw %}{{range . items}}{{range. spec. ports}}{{if . nodePort}}{{. nodePort}}{{. }}{{ \n }}{{end}}{{end}}{{end}}{% endraw %}'$ kubectl get svc --all-namespaces -o go-template= ${TEMPLATE} Useful kubectl plugins: Namespace-wide rolling restart: Repo: LifeWay/kubectl-roll-plugin kubectl roll -n ${NAMEPSACE} will trigger a rolling restart of all StatefulSets, DaemonSets,and Deployments in a given namespace Best practices: See How to Configure Applications for High Availability in Kubernetes "},{id:10,url:"/wiki/networking/",title:"Networking",body:"Curl: Force domain name resolution: 123DOMAIN=example. comLB_IP=10. 139. 0. 123curl -H  HOST: ${DOMAIN}  https://${DOMAIN} --resolve ${DOMAIN}:443:${LB_IP}This is useful in blue-green upgrades before cutting over DNS. For example, we can use this command to talk to the load balancer upstreambackends without DNS. Robust Curl:  Use -L to follow 301/302 redirects Use --fail to exit with non-zero code given 4xx and 5xx HTTP response.      By default, Curl does not consider 4xx and 5xx failure, since the HTTPrequest completed, but for application use cases, they almost certainly arehandled as errors    Use --retry &lt;count&gt; to retry request upon transient errors. Combined with --fail, --retry will also retry HTTP 4xx. Transient error means:     timeout   FTP 4xx response code   HTTP 5xx response code    Optional. Use --retry-delay 3 turns off exponential backoff to always wait 3 seconds before retrying.  Use --show-error to print any error message even in silent mode.  Use -v to turn on verbose logging. Example 1234567891011CURL_OPTS=(  -L  -v  --retry 5  --retry-delay 5  --fail  --show-error)curl  ${CURL_OPTS[@]}    -H  Authorization: token ${MY_TOKEN}    https://api. github. com/user/reposSilence the progress bar: 1curl -s https://example. com/big. file -o output. file"},{id:11,url:"/wiki/go/pitfalls/",title:"Go Common Pitfalls",body:"Memory Model: Go is not sequentially consistent. 12345678910111213func main() {  var s string  var done bool  go func() {    s =  hello     done = true  }()  for !done {}  // Behavior undefined, possible to print   .   fmt. Println(s)}If you need ordering constraint, establish happens-before relationship as oneof the following:  If p imports q, q\u2019s init happens before p\u2019s.  Package main\u2019s init happens before main. main The go statement happens before the execution of the created goroutine A send (or close) on a channel happens before the receive Unlock happens before subsequent LockDeferred Calls: log. Fatal or os. Exit does not respect deferred calls whereas t. Fatal does,where t is a testing. T. In-line deferred statement is evaluated at the time of calling defer. 123456789101112131415func main() {  s :=  old   defer fmt. Println( defer inline , s)  defer func() {    fmt. Println( defer func , s)  }()  s =  new   fmt. Println(s)}/*OUTPUT:newdefer func newdefer inline old*/Deferred calls are executed after return: 12345678910func str() (s string) {  defer func() {    s =  prefix-  + s  }()  return  hello }func main() {  fmt. Println(str()) // prints  prefix-hello }Defer within loops hold resources for too long: 12345678910func main() {  var someChannel chan string  for filepath := range someChannel {    f, err := os. Open(filepath)    if err != nil {      log. Fatal(err)    }    defer f. Close()  }}Do this instead. 123456789101112func main() {  var someChannel chan string  for filepath := range someChannel {    func(filepath string) {      f, err := os. Open(filepath)      if err != nil {        log. Fatal(err)      }      defer f. Close()    }(filepath)  }}Defer cleanup as soon as possible: 12345678910111213141516func CopyFile(dstName, srcName string) error {  src, err := os. Open(srcName)  if err != nil {    return err  }  dst, err := os. Create(dstName)  if err != nil {    return err  }  _, err = io. Copy(dst, src)  dst. Close()  src. Close()  return err}The above code will not close src if os. Create(dstName) failed. Instead ofadding src. Close() before every return, use defer to cleanup as soon asyou can. Similarly, defer unlock as soon as you can. 12345678910111213141516func CopyFile(dstName, srcName string) error {  src, err := os. Open(srcName)  if err != nil {    return  }  defer src. Close()  dst, err := os. Create(dstName)  if err != nil {    return  }  defer dst. Close()  _, err = io. Copy(dst, src)  return err}Array and Slice: Array is a value type. Slice is a reference type. Be careful when you pass themto other functions. Internally, slice is defined as the following 12345type slice struct {  array unsafe. Pointer  len  int  cap  int}To duplicate a slice 123456s := []string{ hello ,  world }s2 := make([]string, len(s))copy(s2, s)// or thiss2 = append([]string{}, s. . . )Be careful with make: 12345678func main() {  // func make([]T, len, cap) []T, where cap is optional.   s := make([]int, 3)  s = append(s, 1)  s = append(s, 2)  s = append(s, 3)  fmt. Println(s) // [0 0 0 1 2 3]}Slicing retains the entire underlying array: 1234567891011121314func main() {  prefixCache := make(map[string][]byte)  for filepath := range someChannel {    bytes, err := ioutil. ReadFile(filepath)    if err != nil {      log. Fatal(err)    }    // This will not release the underlying array of `bytes`.     prefixCache[filepath] = bytes[:8]    // Instead, let's create a smaller copy.     prefixCache[filepath] = append([]byte{}, bytes[:8]. . . )  }}Interface Holding Nil Is Not Nil: 1234567func main() {  var a, b interface{}  fmt. Println(a == nil) // true  var p *int = nil  b = p  fmt. Println(b == nil) // false}Internally, Go interface is represented as 1234567type iface struct {  // tab holds the type of the interface and  // the type of the `data`.   tab *itab  // data points to the value held by the interface.   data unsafe. Pointer}In Go, an interface is nil only if both its type and value are nil. In the example above, b = (*int)(nil) means b\u2019s type is not nil. This behavior surprises people the most in error, which is an interface. See Why is my nil error value not equal to nil?. 1234567func returnsError() error {  var p *MyError = nil  if bad() {    p = ErrBad  }  return p // Always return a non-nil error. }Return an explicit nil instead. 123456func returnsError() error {\tif bad() {\t\treturn ErrBad\t}\treturn nil}Goroutine: Wait group: When main goroutine exits, everything dies. 1234func main() {  go println( hello )}// May not print hello at all. Instead, use WaitGroup as barrier. 12345678910111213func main() {  var wg sync. WaitGroup  // Important that Add happens before starting goroutine.   wg. Add(1)  go func() {    defer wg. Done()    println( hello )  }()  wg. Wait()}Scheduling and preemption: Go version 1. 14 introduced asynchronouspreemption, so that loops withoutfunction calls no longer potentially deadlock the scheduler or significantlydelay garbage collection. Previously, goroutines are only context switched when  blocked on syscalls, channels, locks, or sleep the goroutine has to grow its stack calling runtime. Gosched() directlyHence, a goroutine calling for {} will occupy the processor forever. "},{id:12,url:"/wiki/go/production/",title:"Production-ready Go",body:"Logging: Abort through log. Fatal instead of os. Exit to allow logger to flush bufferedlogs. Use structured JSON logging in production. I recommend uber-go/zap orrs/zerolog, both of which are performant with zero allocation. Use Error, Don\u2019t Panic, But Always Recover: A panic is delivered continuously up the stack until all functions in thecurrent goroutine have returned, at which point the entire program, not just thegoroutine, crashes. We don\u2019t want our server to crash and drop all requests and connections becauseof some rare bug for \u201cindex out of range\u201d. And this bug may be from vendoredcode. As much as possible, we prefer failing the request instead of outages. Hence, I recommend recover() even if you already use error everywhere. 123456789func main() {\tdefer func() {\t\tif r := recover(); r != nil {\t\t\tfmt. Println( Recover main.  , r)\t\t}\t}()\tstartServerThreads()}"},{id:13,url:"/wiki/infra-as-code/pulumi/",title:"Pulumi",body:"Pulumi: Render go templates as pulumi output futures: It is error-prone and counter-productive to call Sprintf() with many %ss. Imagine counting the i-th %s and ensuring it matches the provided args. Templating is a better solution is this case. However, it is a challenge if your template rendering requires pulumi outputsonly known after apply (i. e. futures), because when the template is rendered,the input values are empty. The way to solve this is using pulumi\u2019s ApplyT() transformation, everyone\u2019sfavorite duck tape. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566{% raw %}import (\t github. com/pulumi/pulumi/sdk/v3/go/pulumi )// OIDCUrn is the output of some other pulumi resource. Its value is only known// after pulumi up, hence it is a future. var OIDCUrn pulumi. StringInputfunc main() {  pulumi. Run(runFn)}func runFn(ctx *pulumi. Context) error {  myPolicy := pulumi. All(OIDCUrn). ApplyT(    func(args []interface{}) string {      arn := args[0]. (string)      t := template. Must(template. New(  ). Parse(`{     Version :  2012-10-17 ,     Statement : [      {         Effect :  Allow ,         Principal : {           Federated : [ {{. ArnPrefix}}:iam::{{. AccountID}}:oidc-provider/{{. OIDCUrn}} ]        },         Action : [           elasticloadbalancing:AddTags ,           elasticloadbalancing:RemoveTags         ],         Resource : [           {{. ArnPrefix}}:elasticloadbalancing:{{. Region}}:{{. AccountID}}:targetgroup/*/* ,           {{. ArnPrefix}}:elasticloadbalancing:{{. Region}}:{{. AccountID}}:loadbalancer/net/*/* ,           {{. ArnPrefix}}:elasticloadbalancing:{{. Region}}:{{. AccountID}}:loadbalancer/app/*/*         ]      }    ]  }`))    arnPrefix =  arn:aws     if onGovCloud() {      arnPrefix =  arn:aws-us-gov     }    var buf bytes. Buffer    if err := t. Execute(&amp;buf, struct {      OIDCUrn string      ArnPrefix string      AccountID string      Region  string    }{      arn,      arnPrefix,       some-account-ID ,       us-west-2 ,    }); err != nil {      return       }    return buf. String()  })  policy, err := iam. NewPolicy(o. Ctx,  my-policy , &amp;iam. PolicyArgs{    Name:    pulumi. String( my-policy ),    Policy:   myPolicy,  })}{% endraw %}"},{id:14,url:"/wiki/shell/",title:"Shell Script",body:"Robust Shell Scripts: First of all, if your script is over 100 lines, it should probably be rewrittenin Python or Go. Bash Options: Always do set -o errexit -o pipefail -o nounset at the start of the script.  -o errexit: Abort when a command exits with non-zero status (except in until, while, if) -o pipefail: Causes a pipeline to return the exit status of the last commandin the pipe that returned a non-zero return value.  -o nounset: Attempt to use undefined variable causes an error and forces an exitTrapping Deferred Calls: 123trap '{ rm -f ' ${TEMP_FILE}   ${LOCK_FILE} '}' INT TERM EXIT INT: capturing SIGINT (Interrupt). ctrl-c sends such a signal.  TERM: capturing SIGTERM (Terminate).  EXIT is a pseudo-signal triggered when the script exits, either throughreaching the end of the script, an exit command, or a failing command whenset -o errexit. Just Quote Everything: Variable expansion:  Good:  $my_var  Bad: $my_varCommand substitution:  Good:  $(cmd)  Bad: $(cmd)Linter and Formatter:  Use shellcheck, a shell script staticanalysis tool, to improve your script.  Use shfmt to format your scriptFurther Reading: Bash Pitfalls: a compilation ofcommon mistakes made by bash users. Snippets: 12345678910111213141516171819202122232425262728293031323334353637# log interpolates and writes message to stderr with timestamp. log() { echo  [$(date +'%Y-%m-%dT%H:%M:%S%z')]: $*  &gt;&amp;2}# Retry a command up to a specific numer of times until it exits successfully,# with exponential back off. ## $ retry 5 echo Hello# Hello## $ retry 5 false# Retry 1/5  false  exited 1, retrying in 1 seconds. . . # Retry 2/5  false  exited 1, retrying in 2 seconds. . . # Retry 3/5  false  exited 1, retrying in 4 seconds. . . # Retry 4/5  false  exited 1, retrying in 8 seconds. . . # Retry 5/5  false  exited 1, no more retries left. #retry() {\tlocal retries=$1\tshift\tlocal count=0\tuntil  $@ ; do\t\texit=$?\t\twait=$((2 ** count))\t\tcount=$((count + 1))\t\tif [ $count -lt  $retries  ]; then\t\t\tlog  Retry $count/$retries  $*  exited $exit, retrying in $wait seconds. . .  \t\t\tsleep $wait\t\telse\t\t\tlog  Retry $count/$retries  $*  exited $exit, no more retries left.  \t\t\treturn $exit\t\tfi\tdone\treturn 0}"},{id:15,url:"/wiki/go/snippets/",title:"Useful Go Snippets",
body:"Interface: To assert that a struct implements an interface: 1var _ CoolInterface = (*CuteStruct)(nil)Struct: Validate Fields and Set Default Values: It gets verbose to validate configuration input or set default values, like thefollowing. It gets worse when add more fields to Config. 12345678910111213141516171819type Config struct {  KubeClientSet kubernetes. Interface  StopCh    &lt;-chan struct{}  Threadiness  int}func NewController(c *Config) (*Controller, error) {  if c. KubeClientSet == nil {    return nil, errors. New( KubeClientSet cannot be nil )  }  if c. StopCh == nil {    return nil, errors. New( StopCh cannot be nil )  }  if c. Threadiness == 0 {    c. Threadiness = 2  }  return &amp;Controller{/* . . . */}}I recommend using go-playground/validator for validation and creasty/defaults for settingdefaults. Both packages leverage the field tags in struct. 123456789101112131415161718192021import (   github. com/creasty/defaults    github. com/go-playground/validator/v10 )type Config struct {  KubeClientSet kubernetes. Interface `validate: required `  StopCh    &lt;-chan struct{}   `validate: required `  Threadiness  int         `default: 2  validate: gte=0,lte=10 `}func NewController(c *Config) (*Controller, error) {  if err := validator. New(). Struct(conf); err != nil {    return nil, fmt. Errorf( invalid config: %v , err)  }  if err := defaults. Set(conf); err != nil {    return nil, fmt. Errorf( could not set default values: %v , err)  }  return &amp;Controller{/* . . . */}}HashSet with Empty Struct: Empty struct takes exactly zero memory. 123456789strSet := make(map[string]struct{})// Add to set. strSet[ hello ] = struct{}{}// Check if the set contains an element. if _, ok := strSet[ hello ]; ok {  log. Println( set contains `hello' )}Functional Options: 12345678910111213141516171819202122232425262728293031323334353637type Foo struct {  Num int  Str string}type Option func(f *Foo)func WithNum(num int) Option { return func(f *Foo) {  f. Num = num }}func WithStr(str string) Option { return func(f *Foo) {  f. Str = str }}func New(opts . . . Option) *Foo { foo := &amp;Foo{  num: 10,  str:  hello , } for _, applyOpt := range opts {  applyOpt(foo) } return &amp;foo}func main() { foo := New() foo = New(WithNum(30)) foo = New(WithNum(20), WithStr( world ))}For more details, see Parameters with Defaults in Go: Functional Options Certificate Signing Requests with Email Address: 1234567891011121314151617181920212223242526272829303132333435363738var oidEmailAddress = asn1. ObjectIdentifier{1, 2, 840, 113549, 1, 9, 1}func NewCSR(commonName string, key crypto. PrivateKey, w io. Writer) error {\tsubj := pkix. Name{\t\tCommonName:     commonName,\t\tCountry:      []string{ US },\t\tProvince:      []string{ California },\t\tLocality:      []string{ Palo Alto },\t\tOrganization:    []string{ Pied Piper, Inc.  },\t\tOrganizationalUnit: []string{ Best team },\t\tExtraNames: []pkix. AttributeTypeAndValue{\t\t\t// Crazy dance just to set the email address in CSR. \t\t\t// https://stackoverflow. com/questions/26043321/create-a-certificate-signing-request-csr-with-an-email-address-in-go. \t\t\t{\t\t\t\tType: oidEmailAddress,\t\t\t\tValue: asn1. RawValue{\t\t\t\t\tTag:  asn1. TagIA5String,\t\t\t\t\tBytes: []byte( foo@example. com ),\t\t\t\t},\t\t\t},\t\t},\t}\t// Don't add email addresses to the CertificateRequest EmailAddresses field,\t// which sets a SubjectAltName (SAN). Those are designed more for things\t// like signed emails. In the context of TLS certificates, SANs should be\t// used for alternatively valid hostnames and IP addresses. \ttemplate := x509. CertificateRequest{\t\tSubject:      subj,\t\tSignatureAlgorithm: x509. SHA256WithRSA,\t}\tcsrBytes, err := x509. CreateCertificateRequest(rand. Reader, &amp;template, key)\tif err != nil {\t\treturn err\t}\treturn pem. Encode(w, &amp;pem. Block{Type:  CERTIFICATE REQUEST , Bytes: csrBytes})}Operators: ^ only means bit-wise XOR in Go. For logical XOR, one can use !=. 1234var a, b boolif a != b {  // only true if exactly one of {a, b} is true. }"},{id:16,url:"/wiki/infra-as-code/terraform/",title:"Terraform",body:"Terraform: No diff if you only updated outputs: To force a state change based on an output update, you need to update your codeto include a resource that gets update based on output changes such as thefollowing: 12345resource  random_uuid   tfc_output_refresh  { keepers = {  refresh =  ${filesha256(${path. module}/output. tf)}  }}Switch between Terraform versions: Use tfswitch to update your localterraform CLI version. This is useful when you need to do terraform state surgery,because terraform states are versioned. Terraform will complain when operatingon states with future versions, which happens when Terraform remote executoris using a dated version of terraform unlike yours. Select specific resources: If your team (unfortunately) applies terraform manually and planning takes along time, consider selecting only the subset of resources that youchanged. You can provide as many -targets as you wish. 123456terraform plan -target=${RESOURCE_TYPE}. ${RESOURCE_NAME}# Exampleterraform plan  -target=aws_route53_record. example-com-A  -target=aws_route53_record. www-example-com-CNAME"},{id:17,url:"/thesis",title:"Thesis",body:""},{id:18,url:"/robots.txt",title:"",body:"      Sitemap: {{ \u201csitemap. xml\u201d   absolute_url }}   "},{id:19,url:"/page2/",title:"Home",body:"{% if page. url == \u201c/\u201d %}  12345678910111213141516&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;Popular&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row &gt;{% for post in site. posts %}  {% if post. featured == true %}    {% include featuredbox. html %}  {% endif %}{% endfor %}&lt;/div&gt; {% endif %}  123456789101112131415&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;New Posts&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row listrecent &gt;  {% for post in paginator. posts %}  {% include postbox. html %}  {% endfor %}&lt;/div&gt;    &lt;/div&gt;  {% include pagination. html %}  "},{id:20,url:"/page3/",title:"Home",body:"{% if page. url == \u201c/\u201d %}  12345678910111213141516&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;Popular&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row &gt;{% for post in site. posts %}  {% if post. featured == true %}    {% include featuredbox. html %}  {% endif %}{% endfor %}&lt;/div&gt; {% endif %}  123456789101112131415&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;New Posts&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row listrecent &gt;  {% for post in paginator. posts %}  {% include postbox. html %}  {% endfor %}&lt;/div&gt;    &lt;/div&gt;  {% include pagination. html %}  "},{id:21,url:"/page4/",title:"Home",body:"{% if page. url == \u201c/\u201d %}  12345678910111213141516&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;Popular&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row &gt;{% for post in site. posts %}  {% if post. featured == true %}    {% include featuredbox. html %}  {% endif %}{% endfor %}&lt;/div&gt; {% endif %}  123456789101112131415&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;New Posts&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row listrecent &gt;  {% for post in paginator. posts %}  {% include postbox. html %}  {% endfor %}&lt;/div&gt;    &lt;/div&gt;  {% include pagination. html %}  "},{id:22,url:"/page5/",title:"Home",body:"{% if page. url == \u201c/\u201d %}  12345678910111213141516&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;Popular&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row &gt;{% for post in site. posts %}  {% if post. featured == true %}    {% include featuredbox. html %}  {% endif %}{% endfor %}&lt;/div&gt; {% endif %}  123456789101112131415&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;New Posts&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;&lt;div class= row listrecent &gt;  {% for post in paginator. posts %}  {% include postbox. html %}  {% endfor %}&lt;/div&gt;    &lt;/div&gt;  {% include pagination. html %}  "},{id:23,url:"/ip-law/",title:"Intellectual Property and Entrepreneurship",body:"2021/05/06 - Notes on Intellectual Property (IP) law for founders and busy professionals. Not legal advice. For informational purposes only. Laws can change, so thisarticle may contain dated information. Always consult an attorney. The big 4: Patents: Patents protect novel, useful, non-obvious inventions. With full disclosure ofthe innovation, a patent grants the right to exclude others from making, using,selling, offering to sell, or importing the invention. It does not require itsowner to use the patent. Copyright: Copyright protects original works of authorship fixed in any tangible medium ofexpression, such as source code, art (painting, photographs, acting), andsculpture. Copyright does not protect functionality but only the expression. The author mustindependently create the work (no copying) and display some level of creativity. Copyright is secured automatically when the work is created. Copyright owners have the exclusive right to create copyrightable derivativeworks, unless others obtain a licence from the owner. However, fair use isoften a defense against infringement claims. Reverse engineering of competingproducts is fair use, but you can protect against reverse engineering usingcontracts. There are 4 factors to evaluate a fair use defense. Talk to yourattorney. Trademark: A trademark is a word/phrase/logo that identifies the source of goods. Toregister as a trademark the name of a well-known political figure and celebrity,their written consent is needed. Colors can be a trademark. Think Tiffany blue. Trade secrets: Trade secrets protect confidential company information that is commerciallyvaluable and not publicly known, such as the Coca-Cola Coke formula. They do notrequire any registration with government agencies. Patent applications areconsidered trade secrets by the USPTO until their eventual publication. The trade secret owner must have demonstrable intent to maintainconfidentiality (use NDAs). Trade secrets can be licensed. Ideas are not IP: IP law only protects the embodiment (expression, implementation) of ideas. Becauseideas are not IP, they are not proprietary (i. e. cannot be owned), but they canbe confidential (i. e. secrets). However, ideas may be protected with contracts, such as nondisclosure agreements(NDA) or payment agreements. California law might imply a contract in certaincases, but a contract will not be implied if the disclosure occurs before it isknown that payment is a condition of use. Nondisclosure agreements: Without an NDA, discussion of any invention is considered public disclosure,which prevents you from filing patents and voids your trade secrets. VCs do notsign NDAs. Hence, when pitching to VCs, 1) focus on the business and market, and2) file a provisional patent application to lock in the priority date beforetalking to anyone who is not an inventor. Like all agreements, an NDA must state the parties bounded by the agreement withdate and signatures. It should identify the information discloser and receiver. It should describe with sufficient particularity the information to bedisclosed. Saying that the NDA covers \u201ceverything provided\u201d is likely to be notenforceable. The NDA should state the intended purpose and limitations of the use ofthe disclosed information (e. g. not a license to IP and cannot be used todevelop any product). Make sure to mark confidential documents as such. Definethe duration for disclosure exchange and for confidentiality preservation. Declare which state or country law governs the interpretation of the agreement. Avoid the \u201cresiduals\u201d provision, which says it is okay to use things rememberedin memory. Patents: Utility patents have a 20-year life from the application date and cannot beextended. Patents are granted to the first to file. Laws of nature (theorem,algorithm) and products of nature (animals) are not patentable. The patentowners, not the government, police their patent rights. A provisional application is recommended not only because it secures an earlierpriority date but also because filing patents is expensive (thousands of dollarsbefore lawyer fee) and time-consuming (takes about 25 months to issue), so the1-year grace period allows the inventor to evaluate the business strategy todecide if patent filing is worth it. Patent protection does not start until thepatent issues. A patent requires the invention to be novel. An invention is not novel if it isanticipated (identical to a prior art). Public disclosures\u2014talking tononinventors, publication, selling, offering to sell\u2014count as prior art. So again, NDAs and provisional applications are recommended.  That which will infringe, if later, will anticipate, if earlier.  \u2013 Peters v. Active Mfg. , 21 F. 319 Patents are always issued on Tuesday. Why? Because the USPTO said so. Patent claims must be written in one sentence. Why? Because the USPTO said so. A patent licensee may challenge the patent (e. g. that it was anticipated) whileretaining a licence to the patent. If you lose the patent challenge, you stillhave the licence. AI cannot be an inventor, who must a human being. Employment agreements: An employment agreement likely includes an IP assignment agreement, meaning IPcreated within the scope of employment is owned by the employer. The employerowns the code written by its engineers, but not the code written by janitors. Always include such assignment agreement, essentially for contractors. Do notuse the phrase \u201cwork made for hire\u201d, which in California makes a contractor anemployee. The assignment agreement will protect the company in case someemployees leave and start their own thing using the same IP. California safe harbor: The assignment agreement with the employer does not apply toinventions an employee developed entirely on their own time withoutusing the employer\u2019s equipment, supplies, facilities, or trade secret. However, exceptions are  (1) Relate at the time of conception or reduction to practice of the inventionto the employer\u2019s business, or actual or demonstrably anticipated research ordevelopment of the employer; or  (2) Result from any work performed by the employee for the employer.  \u2013 California labor code section 2870 (a) It is always a peril when someone is still employed elsewhere while also workingon the startup. This person probably also signed IP assignment agreement withthat other employer, and exception (2) above makes things really tricky. Ideally, have a clean cut. Quit former employer, start the company, thendevelop. This issue always comes up during due diligence when you raise funding orseek exits. Ownership and multiple creators: For every type of IP, the default owner is the creators:  patents: inventors copyrights: authors trademark: first user in commerce trade secrets: creatorsThe legal default is that creators are the joint owners of equal rightsregardless of the disparity in contribution. Equal ownership can only be shifted byassignment agreements. Among co-owners, licensing rights are often the most contentious. To avoiddisputes, one option is to divide up the right so that each owner will haveexclusive rights to grant licenses only in a particular industry or country. In community property states, such as California, spouses have ownership rightsin IP created by the other spouse during the term of the marriage. As shown below, there are unique ownership issues for different types of IP. Patents: Absent written agreement, all inventors can each independently license to thirdparties without any duty or notice to co-inventors, perhaps with inconsistentterms. Hence, co-owners could compete with each other. Copyrights: Absent written agreement, each owner can independently license to third partieson inconsistent terms, but contrary to patent law, all co-owners have a duty toeach other to provide financial accounting and share the proceeds fromlicensing. Such duty can be changed with written agreements. Trademarks: It is the business, not the creators that own the trademark. The artisticappearance of the trademark might be protected by copyright. Do not use the logounless the copyright is assigned or licensed to the company, especially when thelogo is produced by a contractor. Open-source software (OSS) licensing: The following 6 OSS licenses cover 99% of the use case. Copy-left licenses (free software):  GPL LGPL Eclipse Public License, CDDL, Mozilla Public License (these 3 are effectively identical)Permissive licenses:  BSD MIT Apache 2Permissive licenses allow the user to do whatever as long as the license header is kept. Copy-left licenses require that if you distribute your software in binary form,you must make the corresponding source code available to binary recipients andonly license your code on the same copy-left licensing terms. Distribution is the key here because it is a right under the US copyright law. Distribution means transferring a copy from a legal person to another. Absentdistribution, most OSS licenses impose no condition. For most licenses, SaaS isnot considered distribution, but pay attention to the recent trend of adoptionof the Server Side Public License. Copy-left licenses have different levels of restriction as shown below. GPL - strong copyleft: if any code in a program is GPL, it must all be GPL. This means no proprietarycode in a GPL codebase and no linking to proprietary code, because linking is aprocess that creates executables, as opposed to ways of inter-processcommunication. In a GPL codebase, code may come from other GPL compatiblelicenses, such as permissive licenses. Everything in kernel space is GPL. Proprietary code in the kernel space isrisky. Application code that communicates with the kernel through syscalls can beproprietary. LGPL - library copyleft: If any code in a library is LGPL, then the entire library is LGPL. However, youcan dynamically link to proprietary code, which means you need to share changesto the library but not the code using the library. Static linking to proprietarycode might be permitted but most companies decided not to after talking to theirattorneys. Note that scripting languages basically work by the equivalent ofdynamic linking for the purpose of this analysis. Mozilla, Eclipse, CDDL - weak copyleft: With no rules about linking, these licenses are friendly to proprietary code,which should be kept in a separate file. If you did not modify the licensedcode, you easily comply. If you did modify, a simple legal review can help youcomply. "},{id:24,url:"/uncertain/",title:"Life and Investment Through the Lens of Uncertainty",body:"2021/01/03 - Disclaimer: Opinions are my own. Not investment advice. Decisions are not judged by results: Decisions and results are separate. The quality of a decision should not bejudged by the quality of its result, which often depends on randomness. If I drove drunk yet got home safely, I have made a bad decision with a goodoutcome. If I drove sober and careful but was rear-ended, I have made a gooddecision with a bad outcome. If I rushed into $NIO upon open, dumped before close, and netted some decentgain, did I make a good decision? So much of one\u2019s success was attributed to theresearch and painstaking, often confusing luck with skills, and signal withnoise. In the last 20 years, $SPY has a nontrivial mean daily return of 0. 0338% but astandard deviation of 1. 2405%, which is 37 times the return. Holding it for justa day is really speculating as opposed to investing. We always make decisions under uncertainty, but our weapons are legions\u2014probability, expectation, and Monte Carlo. Vanish of randomness: Suppose we treat each daily return R1, R2, . . . , Rn as a random variable fromthe same distribution\u2014any distribution, especially not Normal. Suppose thedistribution has a mean mu and standard deviation sigma. Then, the sum ofR1, R2, . . . , Rn has mean n * mu and and standard deviation sqrt(n) * sigma. In layman\u2019s terms, the longer you hold a stock, the narrower the distribution ofits return. If you end up with a spike, it means you almost always make money. This analysis is simplified and assumes the underlying distribution does notchange. The real distribution is not observable, but with so many pension andretirement funds in the market, your investment positions are also a reflectionof your confidence in the federal government. Uncertainty has existed in the past: Uncertainty implies unknown and seems applicable only to future events. After all, I cannot predict the tomorrow price of a stock, but I know for surewhat its price was yesterday. The past can only be learned but not altered. Yet uncertainty has existed in the past. The price went up yesterday, but itcould have gone down. Be very careful with past observations. What has happenedis only one of the possibilities. Watch out for the \u201calternative\u201d histories,cried Nassim Nicholas Taleb. If a certain kind of events\u2014no one has seen a black swan, or the market hasnever gone down more than 20% in a day\u2014has never been observed before, wecannot conclude it is impossible. Stay in the game: The black swan is not scary as long as you can keep playing. Stay away fromRussian roulette. No matterhow lucrative the upside is, there is no come back if you get wiped out. Limittail risk. Always fasten the seat belt. Know your tools: Through what instruments, you asked. It is like choosing the tech stack for yourstartup: always the ones you know. Call options have the leverage and convexity that bends in your favor regardlessof the price going up or down, or so you have overheard in an anonymous webforum. Not entirely in apropos, but at what cost? What if the price remainsstationary? Hint: look intotheta risk. Being reasonable, not rational: It is never about achieving the most optimal return, but a good enough onethat allows me to sleep at night and achieves my life goals. Investing is ameans to an end. Be patient and start living.  Warren Buffett\u2019s skill is investing, but his secret is time.  \u2013 Morgan Housel, The Psychology of Money "},{id:25,url:"/shell/",title:"Navigating Shell for Productivity and Profit",body:"2020/11/20 - I hope you find inspirations from these pretty neat shell tricks and my shell setup. Array Expansion: Use Array Expansion to quickly build arguments to a command. For example, to downloadseveral HTTP resources:    1$ wget https://kubernetespodcast. com/episodes/KPfGep{001. . 062}. mp3 To rename files when you just want to change a substring.    1$ mv /path/to/same-prefix-{old,new}. txt Process Substitution and Anonymous Files: &lt;() redirects the stdout of the command in the subshell to a file descriptor openedat /dev/fd. This is known as process substitution.    1$ sh &lt;(curl -sL https://istio. io/downloadIstioctl) The above is equivalent to the following, where - means to use the stdin as a file.    1$ curl -sL https://istio. io/downloadIstioctl | sh - Note that process substitution is more powerful than pipes, since you can do things like    1$ diff &lt;(ssh somehost cat /etc/hosts) &lt;(ssh some-other-host cat /etc/hosts) Start new shell: Just updated your bashrc/zshrc file, but want to hold on to your environmentvariables, file descriptors, etc? Do this.    1$ exec -l $SHELL The password is sudo: Dealing with production outage and in dire need of mitigation?    1$ sudo su And then you are always the root master. \u201cI don\u2019t test in prod; I dev in prod. \u201d Alias for the Win: Type less. Move fast. Here are some I use everyday.    12345678mcd() { mkdir -p  $1  &amp;&amp; cd  $1  }cdl() { cd  $1  &amp;&amp; ls }alias ls='ls -F -G'alias ll='ls -lh'alias o='open'alias ping='ping -c 5'alias gauth='gcloud auth login --update-adc' (I consider it a work injury that I now quote everything in shell and has long stopped usingwhite space in file names even for personal stuff. Maybe you are like me. ) Cursor Navigation:  Command Deletion:  In-line file:    12345678910cat &lt;&lt; EOF | kubectl create -f -apiVersion: v1kind: Podmetadata:name: nginxspec: containers: - name: nginx  image: nginxEOF Search Command History with Fuzzy Find: You probably know you could use ctrl+r to retrieve a previously executed commandquickly, but it only gives you one suggestion. fzfsolves it not only for commandsbut finds files as well.   Auto-correct Command Typos: I don\u2019t endorse profanity but we all share the frustration. Thefuck will correctyour mistake.   ZSH, ohmyzsh, and powerlevel10k: ZSH, or Z shell, is an extended version of the Bourne Shell (sh), with numerousnew features, plugins, and themes. To change your shell to ZSH    1$ chsh -s $(which zsh) Ohmyzsh is a plugin manager for ZSH. I recommend at least two plugins. autosuggestions makes zsh like fish.   syntax-highlighting colorizesthe command and strings, providing visual warning for typos.   "},{id:26,url:"/promo/",title:"Software Engineering Levels and Promotion",body:"2020/08/31 - This post explains the expectation of each engineering level in the most conciseand company-agnostic way and reveals the steps towards promotion. Levels: The first step to prepare for promotion is to understand the expectation ofeach level. Having worked at several companies ranging fromFAANG to startups inthe valley, I noticed that the engineering leveling systems are awfully similar. New grad / Entry-level (L3) Deliver assigned tasks timely and independentlySoftware Engineer (L4) Design, implement, and operate scalable systems for well-scoped projectsSenior Engineer (L5) Define and scope undefined problems given defined goals Substantial impact within the team     Mentoring junior engineers   Technical oversight   Staff Engineer (L6) Propose and drive cross-functional initiatives, define goals, identify andconnect stakeholdersSenior Staff Engineer / Principal Engineer (L7+) It becomes stratospheric from here. Engineers at such levels are experts intheir domains.  Invent and evangelize new products and technologiesPromotions: There are many moving pieces when you work towards a promo. If you forgeteverything I shared in this post, try to remember this: the single mostimportant thing to do for a promo is consistently delivering projects at thenext level of scope. This is also one of the reasons (besides budgeting) why promotion is gettingslower and harder to come by at some of FAANG, because with the money-printingsuccess of the company\u2019s business model, development work gradually shiftstowards keeping the lights on. Below are generic steps towards a promotion. Step 1: Reasonable manager: The promotion process, as well as one\u2019s life, depends more or less on luck ongetting the right manager and projects. Without a supportive manager, promotionis rather difficult, because your manager is the person who represents you inthe calibration and promo committee review. At least, your manager should view people development as part of her job andbelieve that you have the potential to grow into the next level. If youunfortunately end up in sometoxicwork environment, there are ways to get promo too that I do not recommend. Instead, my suggestion in this case would be to run away screaming (change teamor company). Step 2: Identify gaps: Engage with your manager in your weekly one on one, to shareyour career aspiration and together identify the gaps between your current andthe next level. Two things to keep in mind: Seek actionable feedback. If you are L3 and your manager says the tasks youhave delivered are not technically challenging enough, this feedback probably isnot actionable for you, because you are likely not the person deciding on whichtasks to focus on. An example of actionable feedback is to \u201cbe more vocal aboutyour opinions by speaking up more in meetings and commenting on design docs\u201d. Perception is reality. If your manager thinks you have a communication problem,then you need to solve this communication problem to get promo. You may disagreewith such diagnosis, or there might be underlying causes for such a symptom, butyour manager\u2019s perception is what matters here. Step 3: Find the right projects: To repeat from the opening of this section, the key to promotion is to deliverprojects at the next level of impact. At L3/4, finding such projects are easy. Most projects permit designs that goon an extra mile in terms of feature, scalability, testability, and operationalexcellence. At L5 and beyond, it gets a bit tricky. Most teams do not have a backlog of L6projects, which is one of the reasons why many plateau at L5. So again, luck(right place and the right time) is needed for promo. Your manager and maybetech lead are responsible for prioritization and project scoping. Make sure toloop them in to strategize for these projects. Step 4: Learn and grow: Learn all the new technologies. Build healthy relationships with sister teams. Execute on the projects. Establish competencies. Step 5: Demonstrate impact: You may believe in meritocracy and think the excellence of your work warrantsattention and rewards, or perhaps you are being humble. Yet the reality is thatmost people are too busy to look if you do not point them in your direction. Bombast about yourself feels awkward and might backfire. The trick is to letother people praise you, especially those who are at high levels than yourtarget level. Relationship is important not only because it may unblock yourproject dependencies but also because many, after all, judge books by covers andbelieve trusts are transitive. Prefer written evidence if you can, such as design docs you authored, videorecording of your demo at all-hands, and thank you notes / peer bonus. Following these steps and with sustained performance at the next level,promotion is yours. If not, did I mention luck and having a reasonable manager?My run-away-screaming advice still holds. "},{id:27,url:"/one-on-one/",title:"What to Talk about in Effective 1-on-1s",
body:"2020/06/21 - Unlike in school when we get grades on every assignment and in every course, we get less frequent feedback in professional life, usually once or twice per year, which is also when compensation and level adjustment happens. The infrequent feedback might be frustrating, since it introduces artificial delays in recognizing and acting on improvement opportunities. Careers management is not so different from product management\u2014fast iterations of small improvements are the proven route towards success. One-on-one meetings allow a much smaller feedback cycle. You should schedule regular one-on-ones with your direct manager, if not your skip manager as well. However, it is not trivial to run effective one-on-ones. Sometimes they become status updates or are simply skipped over (Pro tip: don\u2019t cancel but reschedule instead). Just like you own your career, you also own the one-on-ones. Here are some good questions to ask your manager or share your answers with your manager. What to talk about: In this context, you means the manager and I represents the individual contributor. Career and Performance: How am I meeting your expectation?Could you help me understand what the expectations of me are at my current level? What does \u201cExceeds Expectation\u201d rating mean in the context of our team? From what you have seen in calibration and performance review, what are the set of projects and impacts I should aim for to reach such a rating? What is my next career goal? Could you help me identify the specific gaps between where I am and my goal? What are the and action items to bridge the gap? To grow into a senior/staff engineer or an engineering manager requires different skills and experience. You want your manager to know your aspiration and together devise a plan to achieve it. How can the manager be helpful without knowing what you want? Whose job do I want in the future? Do I know what their functions are?Team and Satisfaction: Am I happy with the projects I am working on? Is it too much operation than development? Am I learning new skills? Can I accomplish my tasks?     An ideal job/project should mandate a level of expertise of which I satisfy 60%. Anything more will be trivial to solve and not enough growth for me, but anything less will be overwhelming and set me up for failure.     Are these projects meaningful or impactful? I am very excited about the XYZ project that Jane Doe is working on. She would be a great mentor for me and XYZ is a top priority of our team. How can I contribute? Would you be supportive of me to involve in that project for my career development?Am I happy on this team?Are my peers capable of delivering results? Do they have a strong personality that makes it hard to work with them sometimes? Do they write awesome reviews on my pull requests, or am I tired of their nit-picking? Am I happy with my manager?Do I get the support I need? Does my manager maintain a healthy cross-functional relationship? Do I wish my manager to lean more toward technical leadership or people management? Am I happy at this company?Are there certain policies I like or dislike? Do I enjoy the culture? Do I trust senior leadership? Projects and Processes:  Instead of providing status updates, ask: Do you feel we have a strong tracking process to keep you updated on the project status? Are we setting realistic goals? Do you like our quarterly/sprint planning process?"},{id:28,url:"/istio-short/",title:"The Good, Bad, and Ugly: Istio for Short-lived Pods",body:"2020/04/26 - Kubernetes does not differentiate sidecars and application containers in a Pod. Hence, enabling Istio for short-running workloads imposes additional challengesto the conventional approach of injecting an Envoy sidecar to the istio-enabledPod.  The proxy sidecar is long-running and prevents the Pod from finishingeven after the application container has completed.  We cannot ensurethe proxy to be running and healthy before the application container starts. Without Istio proxy, requests to downstream services may fail if serviceauthentication is enabled, and thus the app container would fail its health check,causing the Pod to be recreated. The pod may be stuck in a crash loop for a while. Straw-man attempts: At first sight, it seems the initContainers in Pod is exactly what we wanted. After all, don\u2019t initContainers always run before containers in Pod? Yet,no containers start until all initContainers have exited successfully. If werun Istio proxy in initContainers, then we have to kill it manually before theapp container could start, which requires the proxy to connect to the service mesh. Some popular mitigationI have seen is to change the entrypoint of the app containerto wait a few seconds before executing the app binary. But how long of a waitis enough? Starting too soon risks proxy not healthy, but starting too late isa waste of time and resources. Scheduling is never deterministic. Worse, onewill have to update the Pod spec themselves, which is not amenable to all theworkloads in the cluster. An Automated and Robust Solution: Start Pod: Mutating Webhook + Command Overwrites: You may build a binary (say, valet) that polls the /ready endpoint of theproxy, and then exec the app binary afterwards. We can use the mutating webhookfrom Kubernetes to automatically rewrite the container command to includevalet. The valet binary can be downloaded using a init container and copiedto the app container using a shared volume. The following Kubernetes manifestshows this approach.    123456789101112131415161718192021222324252627282930313233343536373839apiVersion: batch/v1kind: Jobmetadata: name: short-livedspec: completions: 1 parallelism: 1 template:  metadata:   name: short-lived   annotations:    sidecar. istio. io/inject:  true    labels:    app: short-lived    myapp. com/valet:  true   spec:   initContainers:   - name: valet-init    image: myapp/valet:latest    command:    - /bin/sh    args:    - -c    - cp /valet /shared/valet    volumeMounts:    - mountPath: /shared     name: shared   containers:   - name: short-lived    image: myapp:latest    command:    -  /shared/valet     -  myapp     volumeMounts:    - name: shared     mountPath: /shared   volumes:   - name: shared    emptyDir: {} Finish Pod: Pod Exec + Kill Proxy: The valet binary also supports a -stop-proxy flag, which will stop Envoyproxy by calling the /quitquitquit endpoint on the pilot-agent. In addition tothat, the valet controller will be watching all the pods that are labeled asmyapp. com/valet:  true , and it will issue a valet -stop-proxy command whenthe short-lived containers are all completed. It will essentially be the below call.    1kubectl exec -it $pod -c short-lived -- /shared/valet -stop-proxy "},{id:29,url:"/dns-udp/",title:"DNS, UDP, IP Anycast, and All That",body:"2020/04/05 - DNS prefers UDP. There are times when DNS must run on TCP (request orresponse size exceeds a single packet, perhaps due to too many responserecords), but UDP is perferred if possible. The reasons are  Constraints from IP Anycasts that favor stateless applications such as DNS.  Performance gain with UDP over TCP (2 vs 11 IP packets, no connection state management). IP Anycast: Anycast is one of the fiveaddressing methodsin IP, where multiple endpoint destinations share the same address. A request tosuch address expects only one response from any of the destinations. Routers mayselect the desired path based on costs, latency, and congestion. Anotheraddressing method, Unicast address uniquely identifies a single receiver endpoint. Anycast can be implemented by usingBorder Gateway Protocol (BGP) and Unicast. Multiple hosts (likely in different regions) are given the same unicast IP address. Different routes to the address are advertised through BGP, as if they are alternativeroutes to the same destination when in fact they actually route to differentdestinations with the same address. As usual, routers select a route by whatevermetric (cost, congestion, distance, etc). Selecting a route in this design amounts to selecting a destination. However, routing changes could break open connections to an anycast address,since IP packets could be routed to a different host that has no context on theconnection state (such as TCP sequence numbers). With a normal unicast address,a routing change is not a problem at all, as packets eventually arrive at thesame destination. Hence, anycast is often used with connection-less protocols to provide highavailability and load balancing for stateless services. DNS is a great fit. Theroot name servers need to be accessible at a well-known address (or we need anothername server to find these name servers but what is the address to that name server?). The actual backend serving the query should be as close as possible to reduce responselatency (as DNS lookup is often in the hot path before TCP, TLS, and finally HTTP). UDP: UDP is best-effort (unreliable) and packets may be delivered out-of-order. However,a DNS query usually fits in a single packet and does not require an ordered byte stream. Hence, out-of-order delivery is not an issue;checksum and retransmission are enough to ensure integrity. In comparison, queries over TCP require 11 IP packets to complete:  Three-way handshake to establish connection (SYN, SYN-ACK, ACK) Query by client, ACK by server Response by server, ACK by client Four-way handshake to close connection (FIN, ACK, FIN, ACK)Using UDP allows the name server to scale better because it does not allocate or manageconnection states, such as Receive and Send buffers, Sequence and AcknowledgeNumbers, and flow-control and congestion-control parameters, etc. "},{id:30,url:"/gke-scaling/",title:"Lessons from Scaling GKE: L4 ILB Tops at 250 Nodes",body:"2020/03/20 - My team at Cruise operates tens of Kubernetes clusters with 10,000s coresand 100s of TB of RAM. Since migration to GCP, we have hit several interesting scaling issues. One of those caused cluster-wide ingress outage for all tenants. In this post, I will revisitthe symptom, root cause, and mitigations for this incident. We struggled so you do not have to. Architecture: Private Ingress with ILB and Nginx Controller:    1234567891011121314151617181920      Client       |       | L3/L4       |       vInternal TCP/UDP Load Balancer       |       | L7 HTTP       |--------------+------------------|       |         ||       v         ||  Nginx Ingress Controllers  ||       |         ||       | L7 HTTP     ||       |         ||       v         ||      Pods        ||GKE              |--------------------------------- Symptom: The Internal Load Balancer (ILB) used for private ingress in the cluster stoppedresponding to connections. New connection requests hung (e. g. curl &amp; traceroute)except for requests originated from within the cluster. Root Causes:    Currently, L4 ILB only supports at most 250 backends(source). The Kubernetes cloud provider that controls ILB creation sets all Kubernetes nodesin the cluster as ILB backends. As a result, the maximum number of GKE nodes that gets traffic from the ILB is 250. When more than 250 nodes present in the GKE cluster, ILB will deterministically (using node ID)select a subset of 250 backends for healthcheck.     The Nginx controllers are deployed in the cluster as well. They useexternalTrafficPolicy: local to bypass iptables and sends traffic directly toany pods located on the host. From the perspective of the ILB every node that hasthe destination pod will be healthy and ones that do not will be permanently unhealthy.     A tenant workload scale-up caused the number of nodes in the cluster toautoscale above 250. None of the nodes that Nginx resides on were selected bythe ILB. As a result, the ILB considered all nodes in the subset as unhealthy. When all ILB backends are unhealthy, no traffic through the ILB will be passedto the cluster.  Mitigations: Immediate: In order to immediately restore cluster ingress service, another ILB was manually created using the cluster\u2019s well-known ingress IP address. This ILB was configured to point to a couple instance groups such that the number of backing nodes did not exceed the limit of 250. Short-term: One could deploy a cluster with larger nodes (vertical scaling) in the hope thatthe total number of nodes will be within 250. Alternatively, one may useexternalTrafficPolicy: cluster so Nginx service will use kube-proxy/iptablesto load balance traffic to the correct pod(s) regardless of which node trafficlands on in the cluster. The downside of this solution is that  Extra latency. When the ILB is sending traffic to the 250-node subset,traffic will go through a second hop to get to the destination pod IPs.  Less secure, because it requires opening firewalls for port 80/443 on all nodes.  The client source IP is not preserved, because of the iptables hop. Long-term: Move Nginx controllers out of the cluster and into a GCE instance group, so ILBhealthcheck will be done properly. "},{id:31,url:"/go-opts/",title:"Parameters with Defaults in Go: Functional Options",body:"2020/03/01 - Unlike C++ or Python, Go does not support function parameters with default valuesif unspecified. Specifically, we want that  Passing multiple parameters is supported.  Interface remains backward-compatible when the number of type of one or more parameter changes.  Parameters have default values that can be overridden. In search of a general and elegant solution to this problem, wepresent a few straw-man approaches as motivations for thefunctional options and With*() pattern, which are presented last. Straw-man: Update function signature to accept more inputs: Suppose you have the following Foo struct with the most basic constructor New().    123456789101112type Foo struct { num int str string}func New(num int, str string) *Foo { // . . . initialization return &amp;Foo{  num: num,  str: str, }} Imagine that we want to add more fields to Foo and therefore to change theconstructor into func New(num, num2 int, str, str2 string, bar *Bar) *Foo. Notonly is this function incompatible with existing calls, it also becomes less readableas we add more parameters. Keep the old functions:    123456789type Foo struct { num, num2 int str, str2 string bar    *Bar}func New(num int, str string) *Foo { /* . . . */ }func New2(num, num2 int, str, str2 string, bar *Bar) *Foo { /* . . . */ } We could keep the old functions while adding new ones to support more fields,but it also means the total number of functions grows exponentially (power of 2) with thenumber of parameters, since each parameter could be included or excluded in afunction. Our file would soon become unmaintainable. Putting all params in a struct:    12345678910111213141516171819202122232425type Foo struct { Option}type Option struct { Num int Str string}func New(opt Option) *Foo { // Default values for Foo.  foo := &amp;Foo{  Num: 10,  Str:  hello  } // Set overwrites.  if opt. Num != 0 {  foo. Num = opt. Num } if opt. Str !=    {  foo. Str = opt. Str } return foo} Doing so addresses the compatibility issue and seems to achieve default values. However, it is impossible to determine whether the function caller explicitly setsopt. Num to zero or did not specify Num at all and therefore using the defaultvalue 10. In fact, such confusion around zero-value as input happens not just for Optionstruct but whenever and whatever parameters are passed directly to functions. Passing a struct pointer:    123456func New(opt *Option) *Foo { if opt == nil {  // Use all values in opt to create Foo.  } // Use all default values. } The nil pointer is a good way distinguish set vs unset. However,The new problem is that either all fields need to use thedefault values or none of them does. Hence, if the user only wants to overwriteone parameter and use defaults for the rest, the user must supply the default valuesfor other fields explicitly. Yikes. Make all fields pointers:    1234567891011121314type Option struct { num *int str *string}func New(opt Option) *Foo { foo := newFooWithDefaults() if option. num == nil {  foo. num = opt. num } if option. str == nil {  foo. str = opt. str }} Since the nil pointer is a good way distinguish set vs unset, making all thefields a pointer seems to do the trick. However, this is not user-friendly at all,because in Go there is no such thing as &amp;20 or &amp; hello and the call must assignthe literal value to atemporary variable and then take its address. Not pretty.    1234567num := 20str :=  hello opt := {  num: &amp;num,  str: &amp;str,}foo := New(opt) Variadic functions:    1234567func New(num int, str string, num2 . . . int) { if len(num2) == 0 {  // Did not provide num2, use default.  } else {  // Use num2[0].  }} This alternative approach only works with one optional parameter with dirty semantics,such as the behavior if len(num2) &gt; 1. Functional Options: Finally, we arrive at the functional options pattern that solves optional params(or params with defaults) nicely.    123456789101112131415161718192021222324252627282930313233343536type Foo struct {  Num int  Str string}type Option func(f *Foo)func WithNum(num int) Option { return func(f *Foo) {  f. Num = num }}func WithStr(str string) Option { return func(f *Foo) {  f. Str = str }}func New(someRequiredField string, opts . . . Option) *Foo { foo := &amp;Foo{  Num: 10,  Str:  hello , } for _, applyOpt := range opts {  applyOpt(foo) } return &amp;foo}func main() { foo := New( important , WithNum(30)) foo = New( required , WithNum(20), WithStr( hello ))} More Go Tips: Love what you are reading? My wiki pages have more battle-tested Go lessons:  Useful Go Snippets Go Common Pitfalls More Effective Go Production-ready Go"},{id:32,url:"/k8s-ha/",title:"How to Configure Applications for High Availability in Kubernetes",body:"2019/12/29 - Pods in Kubernetes are the smallest orchestration unit and are ephemeral by definition:  Deployment/StatefulSet/DaemonSet/ReplicaSet updates or patches Nodepool downscaling (compaction) or upgrades (cordoned and drained)Kubernetes simplifies scheduling and orchestration but there are extra hurdles to develop and operate applications with high availability. I list some action items for HA and explain the motivations behind them. Pod: Graceful Termination: When a Pod is evicted,  Pod containers each receive a SIGTERM.  Eviction controller waits terminationGracePeriodSeconds (default 30 seconds) Pod containers each receive a SIGKILL Eviction controller waits for all containers exitHandle the termination signal gracefully because your application may still be serving in-flight requests or need to clean up persistent storage before exit. Health Checks: Health checks are important to ensure that unhealthy\u2014irresponsive, about-to-be-killed, or initializing-and-unready\u2014Pods are not selected in the working set. Health checks can be in forms ofshell commands,HTTP probes,or TCP probes. Readiness ProbesA Pod will only start serving traffic after its Readiness probe succeeded. For example, your web app should establish a connection to your favorite database before serving its API. Liveness ProbesMany applications running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes provides Liveness probes to detect and remedy such situations. Startup ProbesStartup Probes resembles Liveness probes but are intended for slow-initialization applications. Without Startup probes, one may use longer Liveness probes for slow-start applications, but doing so compromises the fast response to deadlocks that motivate Liveness probes in the first place. Priority Class: If a Pod cannot be scheduled, the scheduler tries to preempt (evict) lower priority Pods to make scheduling of the pending Pod possible. Run your mission-critical Pods on high priority. For example, logging, monitoring, backup, and a subset of application services (depending on business logic) should be of high priority, but batched and cron jobs may be not. Resource Requirements and Quotas: Pod resources are CPU counts, memory space, and ephemeral storage size. Resource requirements include requests (minimum needed to run) and limits (maximum allowed). If you configure the same value for requests and limits, your Pods are of Guaranteed (highest) Quality of Service (QoS) Classes. If the node is under memory pressure, burstable or best effort Pods are killed and rescheduled first. However, depending on your node size, choose sensible resource requirements. If your node has 64 vCPU, asking for 60 is just not cloud-native. Replicated Pods: Load Balancing - Ingress and Service: Put your replicated Pods behind a load balancer to ensure your HTTP/TCP/UDP application remains accessible when Pod health or membership changes. Such a load balancer could be an Ingress or a Service, depending on whether you want this service to be accessible only in the cluster network or your company VPC or the public internet. Leader Election: Perhaps not all of the replicated Pods should be considered active. For example, a Kubernetes controller on CRDs should logically be just one instance. Leader election allows such an application to still be replicated while preserving the single-instance abstraction. Pod Disruption Budget (PDB): A PDB limits the number Pods of a replicated application that are down simultaneously from voluntary disruptions. For example, a quorum-based application would like to ensure that the number of replicas running is never brought below the number needed for a quorum.    123456789apiVersion: policy/v1beta1kind: PodDisruptionBudgetmetadata: name: zk-pdbspec: minAvailable: 2 selector:  matchLabels:   app: zookeeper Anti-Affinity Preference: Anti-Affinity Preference allow replicated Pods to be scheduled to different nodes. Without such preference, a single node failure could knock out multiple Pods if they happen to be scheduled on the same node, which is possible since scheduling is done by resource allocation. Autoscaling: Autoscaling allows the service backends to adjust to the request rate and to mitigate overloading. Vertical AutoscalingVertical autoscaling modifies the resources allocated to your Pods dynamically.    1234567891011apiVersion: autoscaling. k8s. io/v1beta2kind: VerticalPodAutoscalermetadata: name: my-vpaspec: targetRef:  apiVersion:  extensions/v1beta1   kind:    Deployment  name:    my-deployment updatePolicy:  updateMode: Initial If you need to limit the number of concurrent pod restarts, use a Pod Disruption Budget. Be extra careful if you are using ingress solution from your cloud vendor, such as GCLB, it may take minutes to update backend routes which can cause routing failure and downtime when combined with frequent rescheduling. Horizontal AutoscalingHorizontal autoscaling dynamically modifies the replica count of a Deployment. By default, the HPA only supports scaling based on CPU or memory usage, which are often suboptimal for request-based or queue-based workloads. HPA supports autoscaling based on custom metrics, but doing so requires elevated permissions to register a Custom Metrics Adapter to serve a cluster-level API endpoint, which may not be an option if you are running in a multi-tenant environment. Monitoring and Alerts: API Uptime and Latency: This is probably the key KPI that your customers care about. Visibility on the API uptime and latency allows quick responses to incidents. Managed solutions include Runscope, Pingdom, DataDog, UpDown. io, StackDriver, etc. It is also important to alert on these metrics to PagerDuty, Slack, or Email. Resource Usage - CPU, RAM, Disk, I/O: Eccentric resource usage is usually a precursor for outages. Usually, developers or the language runtime will embed in the application Prometheus endpoints for scraping these metrics. DataDog agents make collecting them easy. Unavailable Pods: You could have the right configurations for HA but still have unschedulable Pods\u2014image pull unsuccessful, or container stuck in crash loop, or node auto scaler has reached max, etc. "},{id:33,url:"/advices/",title:"Advices I wish I got at the start of my career",
body:"2019/11/03 - When I was a kid playing chess with my dad, he sometimes would offer me hints onsome good moves. I would never make those moves. I would rather make othersub-optimal moves for the sake of not taking on his advices, believing that myeventual success\u2014much rarer than I would hope\u2014should be attributed solely tomy own volition and endeavor. I find such irrational rationale the perfect metaphor for many adventures in mylife. I preferred to learn things the hard way by myself and I did. It took me awhile to realize the story I touted that I was a self-made man could not havebeen more misleading. Despite all my trials and errors, I would have comenowhere close to where I am had I not had the help from my parents, friends,advisors, and colleagues. Hence, though I would never stop experimenting, I started to seek advices fromall sorts of venues. Many lessons need not be learned the hard way. If you sharemy goal of personal and career growth, here is a collection of wisdom I gainedover the years from either misadventures of my own or people I admire andrespect. Learning is Compounding: Prioritize learning over pay. At the start of one\u2019s career, the pay differencesamong offers are negligible after taxes. At 22, your largest investment capitalis your time. Invest early and continuously in yourselves to leverage thecompounding effects, which apply to both learning and career. Your personalgrowth and career growth is not a forcing function that only increases withtime\u2014you are not growing but just getting older if you are essentially doingthe same job for many years. Inductively, your next opportunity directly relatesto your current knowledge and experience. Optimize for growth to get the bestopportunity, and the pay always follows.  We tend to massively underestimate the compounding returns of intelligence. Ashumans, we need to solve big problems. If you graduate Stanford at 22 andGoogle recruits you, you\u2019ll work a 9-to-5. It\u2019s probably more like an 11-to-3in terms of hard work. They\u2019ll pay well. It\u2019s relaxing. But what they areactually doing is paying you to accept a much lower intellectual growth rate. When you recognize that intelligence is compounding, the cost of that missinglong-term compounding is enormous. They\u2019re not giving you the best opportunityof your life. Then a scary thing can happen: You might realize one day thatyou\u2019ve lost your competitive edge. You won\u2019t be the best anymore. You won\u2019t beable to fall in love with new stuff. Things are cushy where you are. You getcomplacent and stall.  \u2013 Stephen Cohen, Co-founder and Executive VP of Palantir Join Companies on the Breakout Trajectory: Your scope and impact will be much larger than what your title suggests. Youwill work with the smartest people on the latest technologies without servicinglegacy baggage. You will likely work with open-source software (OSS) that grantsyou directly transferable skills. Your patches back to the OSS community giveyou unparalleled external visibility. Companies on the breakout trajectoryentail less risk than do seed-round startups struggling to find product-marketfit and yet more upside than established counterparts. The company itself will be focusing on exponential growth in business andpeople. There will always be more work than people on it and fewerpolitics\u2014hardly fights over interesting projects or he-said-she-said nonsense. In the future, having this company on your resume gives you more credit than youdeserve. Being at Paypal around 2000, Google in 2005, and Facebook in 2010 isworth more than any advanced degree will buy in this market. Career takes care of itself. Your career growth is the superposition of yourpersonal growth and company growth. Companies often prefer to promote fromwithin than to hire external talent to preserve the culture and spare knowledgetransfer. With your large scope and impact and expanding headcounts of alllevels in all departments, promotion is fast. Your experience means a lotexternally as well. Everyone wants to recruit from successful companies in thehope that people carry the lessons of success with them.  If you\u2019re offered a seat on a rocket ship, don\u2019t ask what seat. Just get on.  \u2013 Sheryl Sandberg, COO of Facebook  All our advice on Silicon Valley careers is based on a simple idea: that yourchoice of company trumps everything else. It\u2019s more important than your jobtitle, your pay or your responsibilities.  \u2013 Andy Rachleff, Executive Chairman of Wealthfront, Partner at BenchmarkCapital Prestige Matters But Can Be Manufactured: At first sight, prestige seems misplaced in Silicon Valley. I have troublethinking of a more meritocratic industry than tech and engineering. Yet prestigestill matters. At virtually every company, the hiring process biases towardsrespected colleges for the promise of rigorous curricula and talented studentsand from other elite companies, sayFAANG.  Many desirable things that you want over the course of your career will begated by mechanisms that favor folks with prestige. You can be justly upset bythat fact, but upset is an insufficient catalyst for change, and ultimatelyyou\u2019ll have to develop your own prestige to gain access to those scarceopportunities and resources. Prestige makes everything more attainable: auniversal lubricant.  \u2013 Will Larson, Head of Foundation engineering at Stripe But fear not! Prestige can be manufactured. Promote yourselves with blogs,newsletters, podcasts, conference talks, and open-source contributions. Thingsget easier and compound over time. Your first few blogs may lead to columnwriting, which may lead to book writing invitation by publishers. Your youtubechannel or podcasts enable you to talk on meetups and eventually conferences. All these will allow you to be discoverable by the next best opportunities inlife. Your Manager and Colleagues, Not Company, Determine 95% of Your Experience: Before taking the offer, talk to your future direct manager and colleagues andask hard questions. Everyone has their style of working, and different teamshave disparate prospects, focus, and dynamics. Try really understand what youare getting yourselves into and if this is the right opportunity for you. Hereare some questions that I find helpful to uncover this information. Formanagers,  Is this position open because the last person left or your team expanded? What are the projects that I will be working on? Who is the customer? Why isit more important than other backlog projects? How many reports do you have? How many of them are senior or staff? How longhave they joined the company? What is your management style? What is the career outlook for this position? What is the next role that I cangrow into? Has anyone done it? How would you help develop the career of your reports?For colleagues,  If you have the power, what is the one thing that you would like to changeabout this company? What projects are you working on? What do you like or dislike about them? How do you solve XYZ? I have used ABC before but I am interested to know whyyour team uses DEF.  How do you like your team/manager? Tell me the workflow of your team from request gathering to production.  Do you know your skip manager and vice versa?You Get What You Put In: Be proactive at work. Help investigate outages even when you are not on call. Read code and documentations aggressively to understand how your and othersystems work. Do not be limited by your sprint goal or responsibility. Fix thethings that were a pain for everyone. Document tribal knowledge that you haveacquired. Attend postmortems and ask questions. Review and comment on designdocuments and pull requests. In essence, you do more to get more. People, Not Jobs, Last Forever: You will likely switch companies every few years, but the people in the industryare here for long. You take all the relationships you have built\u2014good and badones\u2014into your new adventures. Try to build good, lasting relationships. Pickthe people you work with. Capable and affable coworkers can only accelerate yourcareer. These are the group of people to put your name into referrals or to joinyour next undertaking.  Positive relationships enable serendipity, and serendipity is the source of themost interesting opportunities.  \u2013 Will Larson, Head of Foundation engineering at Stripe Switch teams and companies every now and then, help in hiring and interviewing,offline engagements at conferences or meetups will grow your personal network. Some Book Recommendations:  The Hard Thing About HardThings The MythicalMan-Month The LastLecture Site Reliability Engineering atGoogle How to Win Friends &amp; InfluencePeople Difficult Conversations: How to Discuss What MattersMost"},{id:34,url:"/2s-comp/",title:"A Brilliant Hack: Why does Layer 2/3 Checksum use 1\u2019s Complement, Not 2\u2019s",body:"2019/04/07 - A super quick recap, one\u2019s complement represents negative x by reverting every bit of x, while two\u2019s complement negative x as one\u2019s complement of x plus 1. Symbolically,    12one\u2019s complement  -x = ~xtwo\u2019s complement  -x = ~x + 1 Two\u2019s complements seem to have taken over the entire computing world. Some major reasons promoting two\u2019s complements include that the adder in ALU on your processor is the same for both positive and negative operands, that the same adder could be used as subtractor easily, and that there is exactly one way to represent the value of zero. Nonetheless, the error detection checksum makes the conscious decision to use 1\u2019s complement even when its host ISA uses 2\u2019s. A refresh on the checksum algorithm:    1234567891011Send: calculate the 1\u2019s complement sum  over the payload on a 16-bit word boundary negate the sum to use as the checksum append to the payload. Receive: calculate the 1\u2019s complement sum  on a 16-bit word boundary  over the payload INCLUDING the sender checksum assert the result has bits of all ones.  An example implementation might look like the following    12345678910111213u_short cksum(u_short *buf, int count) {  register u_long sum = 0;  while (count--) {    sum += *buf++;    if (sum &amp; 0xFFFF0000) {      // One's complement add requires      // adding carry bit back to sum.       sum &amp;= 0xFFFF;      sum++;    }  }  return ~(sum &amp; 0xFFFF);} Notice that the receiver does not sum over payload to see if it equals the sender checksum, but rather the sender sends negated sum and the receiver sums (payload + checksum). Checking if all bits are ones are super easy and quick by checking if its negated value is zero, a property that is only true if one\u2019s complement is used. Perhaps such implementation is slightly faster than that using two\u2019s complement, but I found the explanation so far unsatisfactory. I did some more digging. I realized the performance boost from using 1\u2019s complement sum is endianness independent. Endianness matters because for a 16-bit checksum, its bytes are swapped when read from link to CPU. Different hosts use different endianness. Using 2\u2019s complement in checksum mandates calling ntohs and htons on EVERY router along the way. 1\u2019s complement does not need to reorder bytes. It asserts the final sum has all bits of ones. Blazing fast, and a brilliant hack. "},{id:35,url:"/blue-green/",title:"Service API Changes: Prefer Blue-green Update to Rolling Update",body:"2019/03/24 -  Summary:  To achieve zero-downtime service update, Kubernetes rolling update implies the API must be both forward and backward compatible. Forward compatibility is hard if at all makes sense. Blue-green update requires only backward compatibility to ensure zero downtime. Blue-green update is not supported by the Kubernetes core API but achievable with simple scripts or CRD + controller. The Deployment object in Kubernetes supports service rolling update in the hope of providing zero downtime service update. The rolling update is done by scaling out a new replica set of pods using containers with the new version and shrinking the old replica set. However, rolling updates are insufficient to avoid downtime when the service API changes. The API must be both forward and backward compatible due to the coexistence of both versions of servers and clients.      As the new replica set is scaling out, the Service label selector will include all the available pods spanning both replica sets, which means both versions of the server are serving requests. Imagine these are web servers and v1 (v2) servers hand out v1 (v2) JS client that runs in browsers. During the update, many browsers still run v1 client, but the service starts giving out v2 clients. The Service object load balances the ingress to both v1 and v2 servers. Hence, requests from v2 client could be routed to v1 server (forward compatible) and v1 client could be routed to v2 server (backward compatible). The semantics of forward compatibility is always hairy. One could \u201cgracefully\u201d handle an unsupported request by ignoring it, returning the not-found status code, or returning a response asking for a retry in the hope of landing on a server with the new version (and good luck with sticky sessions). None of these is satisfactory. Blue-green update does not require forward compatibility because it ensures a single version of servers. A new Deployment is made and the Service label selector is updated.    There could still be older versions of clients after the update is complete (backward compatible), but the client of the latest version of will not be handled by the servers of old versions. Blue-green update is easy even by hand. Alternatively, one could leverage the BlueGreenDeployment CRD (CustomResourceDefinition) and deploy a controller handling updates to the BlueGreenDeployment object. Check out https://github. com/google/blue-green-deployment-controller. "},{id:36,url:"/configmap/",title:"CD Tricks for Kubernetes Deployment + ConfigMap",body:"2019/03/10 - It is common to extract the application configuration to a separate file as a runtime dependency of the container image that includes the application binary. As a result, the same image can be used (thus \u201cpromoted\u201d) across different deployment environments, from dev to staging and prod. Kubernetes offers native support to do exact so, but not without some caveats that I hope to carve out for you. The Kubernetes Deployment is an API object that manages a replica set of Pods. A Pod is a collection of one or more containers and is the smallest atomic unit to be provisioned. A replica set includes a set of identical Pods and ensures the number of replicas conforms to the desired state. The Deployment enables rolling upgrades of your applications with zero downtime by gradually scale out a new replica set of the app with the new version and scale down the old replica set. Hence, the Deployment object has been the de facto way to manage the application life cycles. ConfigMap is another Kubernetes object that essentially represents a set of key-value pairs that represents a configuration. It can also represent a file with the key as the file name and the value as its content. To be accessible to the application, a ConfigMap can be mounted to a Pod as a volume in the file system of the container. You may create/update a ConfigMap with    1234kubectl create configmap myconfig  \xa0\xa0\xa0--from-file=/path/to/config. yaml  \xa0\xa0\xa0--dry-run -o yaml  \xa0\xa0\xa0| kubectl apply -f - The first trick to share is the dry run piping into apply. If there is an existing configmap with the same name, kubectl create will fail, but there isn\u2019t a way for us to update configmap from file like we could with kubectl create. The dry run pipe make updates possible. And then the deployment manifest may look like something like this.    12345678910111213141516171819202122232425apiVersion: app/v1kind: Deploymentmetadata: name: myappspec: replicas: 3 selector:  matchLabels:   app: myapp template:  metadata:   labels:    app: myapp  spec:   containers:    - name: myapp     image: myapp:1. 0. 0-alpine     args: [ --config ,  etc/myapp/config. yaml ]     volumeMounts:      - name: myconfig       mountPath: /etc/myapp    volumes:     - name: myconfig      configMap:       name: myconfig But what if we only want to update the configmap while using the same container image? Although such an update will be immediately reflected in the container file system (i. e. , reading the config file again after the update will retrieve the latest write), most applications only load the config file during initialization. The challenge becomes how to instruct the application deployment to pick up the latest config file with zero downtime. Recall the Deployment object manages the replica set of application containers. The key here is to trigger another Deployment rollout, so the new pods created will pick up the latest config file. Updating the configmap solely will NOT trigger a Deployment rollout. The trick is to include a CONFIG_HASH in the pod template. When its value changes, a Deployment rollout is triggered.    12345678910111213141516171819202122232425262728apiVersion: app/v1kind: Deploymentmetadata: name: myappspec: replicas: 3 selector:  matchLabels:   app: myapp template:  metadata:   labels:    app: myapp  spec:   containers:    - name: myapp     image: myapp:1. 0. 0-alpine     args: [ --config ,  etc/myapp/config. yaml ]     env:      - name: CONFIG_HASH       value: ${CONFIG_HASH}     volumeMounts:      - name: myconfig       mountPath: /etc/myapp    volumes:     - name: myconfig      configMap:       name: myconfig The final deployment script becomes    12345678910kubectl create configmap myconfig  \xa0\xa0\xa0--from-file=/path/to/config. yaml  \xa0\xa0\xa0--dry-run -o yaml  \xa0\xa0\xa0| kubectl apply -f -export CONFIG_HASH=$(  \xa0\xa0\xa0cat /path/to/config. yaml   | shasum | cut -d' ' -f 1)envsubst &lt; deploy. yaml | kubectl apply -f - "},{id:37,url:"/jwt/",title:"JWT + Third-party Oauth in Single Page App",body:"2019/02/13 - Imagine you run a single page app at example. com that communicates with backends over restful API and is authenticated with JWT tokens managed by you, but identities are managed by third-party OAuth vendors (Google, Facebook, GitHub, etc). Integration of JWT and Oauth has proven nontrivial. Especially, how exactly does one issue the JWT token back to the client after the client successfully authenticated with the third-party OAuth vendor? To appreciate this challenge fully, please endure a brief introduction of the OAuth workflow.   The API server does not manage user credentials. The client sends the credentials to the OAuth vendor and receives a redirect URL and a one-time passcode. The redirect URL is a subdomain of yours and is pre-registered by your app with the OAuth vendor. Its purpose is to relay the passcode back to API server so the server could use this code in exchange for the OAuth token. Then the server might use the token to request for some basic user info and then issue a JWT token for this user. How do we send this token back to the client? The client is still waiting for a server response to its \u201cshare code\u201d request. It is worthwhile to note that such request is done by the browser hitting the redirect URL as instructed by the OAuth vendor, so the browser is waiting for a response that it can render. Hence, replying the JWT token in JSON over HTTP just does not cut it. If the server replies just an HTML with javascript assets, all subsequent requests by the client will still be unauthorized. I have seen so many hacks on this problem.    Store session id in cookie and store the token in session on the server side. Then you must make sure session persistence if your API servers are horizontally scaled. If a user logged in on replica A and subsequent requests hit replica B, the user should not be asked to log in again. The use of cookie also defeats the entire point of JWT based API server authentication, since it is not mobile app friendly.     Pass the token through URL params by redirect elsewhere before redirecting to the final logged-in page. In the following example, the callback from OAuth vendor will first redirect the client to /saveToken with JWT token as param. The client extracts the token from the path and at the same time, the server responds dashboard. html. Not only is doing so not secure (referrer HTTP header might keep the token), it also requires a lot more work on the frontend client code to control redirect routings to extract JWT token before sending GET requests to /saveToken.     12345678const auth = passport. authenticate('google', { failureRedirect: '/login', session: false,})app. get('/auth/google/callback', auth, (req, res) =&gt; { const jwt = createJWTFromUserDATA(req. user) res. redirect('/saveToken?jwt='+jwt)}) The best solution that I encountered is the following, which stores the JWT token in local storage and then redirects to the main page with no change to service code and just a few lines of HTML.    123456789101112131415const auth = passport. authenticate('google', { failureRedirect: '/login', session: false,})app. get('/auth/google/callback', auth, (req, res) =&gt; { const jwt = createJWTFromUserDATA(req. user) const htmlWithEmbeddedJWT = `&lt;html&gt; &lt;script&gt;  window. localStorage. setItem('JWT', '${jwt}');  window. location. href = '/'; &lt;/script&gt;&lt;/html&gt;` res. send(htmlWithEmbeddedJWT)}) "},{id:38,url:"/docker_multi_stage/",title:"Docker Multi-stage Build: Fast, Minimal and Secure Images",body:"2019/01/21 - Introduced in version v17. 05, multi-stage builds feature in Dockerfiles enables you to create smaller container images with better caching and smaller security footprint. Fundamentally, the new syntax allows one to name and reference each stage and copy artifacts between them. Fast Images: In this example, the second and third stage will build in parallel.    12345678FROM ubuntu AS baseRUN apt-get update &amp;&amp; apt-get install gitFROM base AS src1RUN git clone . . . FROM base AS src2RUN git clone . . .  Minimal Images: Before multistage builds, one needs quite some heavy lifting to reduce the final image size. Tricks to do so aim at reducing the total number of layers and the size of each layer. It was common to see optimization such as chaining all commands using &amp;&amp; as shown in the following example.    123456FROM debian:wheezyRUN apt-get update &amp;&amp; apt-get install curl ca-certificatesRUN curl -L https://dep. tar. gz -o /tmp/dep. tgz   &amp;&amp; tar xzf /tmp/dep. zip   &amp;&amp; mv /tmp/dep/bin/dep /workingdir/bin   &amp;&amp; rm -rf /tmp/dep* The reason is that each COPY, RUN, ADD is a new layer. Layers are analogous to git commits. They both represent the delta between two snapshots. If you add and remove the same file, the final result is identical but the aggregated deltas are twice the size. Hence, &amp;&amp; not only reduces multiple layers into one but also merges deltas that cancel each other. Without &amp;&amp;, the downloaded zip file or in other cases source code on your local will remain somewhere in the image layers, even though the final image appears not to have them. This trick works but looks cumbersome. With multistage build, small images are so easy.    12345678FROM debian:wheezy AS depRUN apt-get updateRUN apt-get install curl ca-certificatesRUN curl -L https://dep. tar. gz -o /tmp/dep. tgzRUN tar xzf /tmp/dep. zipFROM debian:wheezyCOPY --from=dep /tmp/dep/bin /workingdir/bin It becomes much easier to read and the final image is just one additional layer on top of the base image debian:wheezy. Secure Images: Multistage builds also ensure you do not accidentally push secret credentials along with the image. Image there is a private repo that your code needs as a third party dependency. To pull its source you need to mount your ssh key as part of the build. The last thing you want is to leave your key in the layers and shipped as part of the image. Multistage builds offer a clean solution. You could download and build the private source by mounting your ssh key to the first stage, copy over only the binary output to the second stage, and push the second stage. You may argue there is still a layer on your local that keeps that key. It is true, but the key is from your local anyway. "},{id:39,url:"/sys-design/",title:"System Design Interview: Scaling Single Server",
body:"2018/12/14 - Imagine your app is doing tremendously well with growing traffics. If there is a single server for your app, and the server is approaching its capacity, how would you scale to handle the load?  A slight digression to why it is bad if the server loads go beyond the saturation point. Ideally, we hope that the server throughput increases as its loads, which is only true up to a certain point due to physical limitation \u2013 RAM, CPUs, IO, etc. Beyond that, one could manage to sustain the same level of throughput if they know what they are doing (more blogs on this to come), but a more common scenario would be an exponential decrease in throughput due to receiver livelock where network packets are handled as interrupts at a higher priority than the server process. At high arrival rate, the CPU is constantly preempted to handle the incoming packets, only to drop them because of full buffer / queue, hence the server process hardly ever gets run to deplete the queue. The moral of the story is to get close to but not over the saturation point for reasonable latency and throughput. Back to the starting issue. What if the traffic going beyond the saturation point? The simplest answer would be to scale up / vertically, as in running your server process on a much more powerful machine, perhaps more RAM for caching and more cores for throughputs. But there is a limit on the state-of-the-art configuration \u2013 there are only so many cores on a single chip or so much RAM that the motherboard supports. Hence, the server must scale out / horizontally, be able to do more when it needs more. Stateless Frontend / Edge / API Servers:  To do so, the server has to be stateless, or all states must be externalized to a durable, secondary storage. A stateless server is beneficial because crash recovery is easy \u2013 just restart and connect to the database \u2013 and to scale out means to provision another identical instance connected to the same storage. Then you essentially doubled the throughput, going from one server instance to two, assuming the storage has yet to reach its capacity. The new server instance could (should) be deployed at a different geographic location so their faults are isolated. If one of your data centers is out of power, the rest of your servers could pick up the load. Usually, all of the servers sit behind a load balancer / reverse proxy, so that a single IP address is exposed but server membership could also be easily managed. Doing so also helps achieve even distribution of loads on the active set, with your favorite load balancing scheme \u2013 L4, L7, round robin, random, shortest queue, etc.   As we add more servers, the storage becomes the bottleneck. There are essentially two ways to scale the storage \u2013 split the storage into multiple shards / chunks / partitions so they can serve at the same time (and thus throughput multiplies) and a hot cache to intercept most of the loads to storage. Memory Cache Cluster:  The number one rule of any architecture design is to make common cases fast. An epitome of such design philosophy, memory caches (memcached, redis) are another downstream service that caches frequently accessed server responses only in memory. No persistency means no disk I/O and hence much lower latency and higher throughput. Prominent use cases include celebrity tweets or trending YouTude videos, where 10% of your content attracts 90% of the traffic. It is okay to miss. It is okay to fail. Those are two import points people often missed about memory caches. Cache is by definition not the storage layer. If the cache missed, the client could always fall back to DB. Indeed, one has to mind the cascading failure where if the cache fails entirely, all traffic hits and essentially kills the persistent storage, but building a replicating consensus quorum on top of memory caches is the anti-pattern of caches. Doing so introduces additional round trips in delay by the farthest replica. The better approach should be a cluster of memory caches, sharded but not replicated. If any of those failed, traffic on those partition keys hits the DB but should be miniscule on the grand scale. In the meantime, erect another instance of memory cache elsewhere and when it is ready, routes traffic on the previous cache there and none of these operations need to be synchronous \u2013 if the cache fails, hit DB; if the cache is cold / just up, hit DB and write through; if the cache is up but some client has yet to discover, hit DB. Key to success is fine-grained partition space. Sharding:  Each piece of data is assigned to a particular shard using a partition key, which is usually the primary key. Such assignment has to deterministic, so that the data can be retrieved / queried, a task not as trivial as it seems. Imagine the strawman example with N shards to start with and an assignment function partitionKey mod N. Suppose such set up is falling behind the loads. Adding any additional shards changes N and thus breaks the assignment scheme, since the same partition key is likely to be mapped to a different shard using the mod function given that the total number of shards have changed. The assignment task is exactly like hashing where a key is mapped to a bucket. Worse, to fix this mismatch, the entire storage has to reshuffled so a key to put to the right shard, and you have to do this for every expansion. Our goal is to devise a consistent hashing scheme with minimum redistribution. There isan awesome paper on this\xa0and I will not bore you with the details, but a 1,000-foot view is the following.   You have a ring that corresponds to the range [0, 1). With your favorite hashing function and modulo operation, each partition key and each shard corresponds to a point on this ring. The assignment is done by walking the ring counterclockwise and the first shard you meet is the shard you go. If a new shard is provisioned, say shard 2, then only the highlighted arc needs to be redistributed from shard 1 to 2. Even better, such redistribution could be done asynchronously in the background, so that shard 1 continues to serve while shard 2 ramps up with the highlighted arc. When shard 2 is ready, it atomically joins the set, and all traffics flows accordingly, given the hashing scheme presented. Replication: Sharding introduces a new challenge. If each shard has a failure rate of p, which means a probability of (1-p) of being up and running, and the storage is partitioned into n shards, then the chance of the storage being failure free is (1-p)^n, assuming independent failures. A large n means some failure is almost always guaranteed, and that any clients hitting on those keys on the failed shard will also fail. To increase the availability of the sharded storage, we could replicate each shard independently as Replicating State Machines (RSM). The idea is that if all operations are deterministic, we could model each replica as a state machine such that by replicating the operations in the same order, states are also replicated. Popular algorithms include Raft, Paxos, Viewstamp Replication, where they all have a concept of a master/leader to process all reads and writes. Quorum-based replication often uses the client as the coordinator rather than using server-side nodes. Quorum-based replications are strongly consistent if every quorum (read or write) intersects with every write quorum, which guarantees that at least one node has seen the latest write. Load Balancers:  Recall that there is one load balancer (LB) as a reverse proxy on top of the set of API servers. What if the load balancer fails, or if the load balancer becomes the system bottleneck? In essence, one needs an LB cluster, and each LB could independently route to all API server. But how do you load balance on load balancers? The answer is to leverage multiple DNS A records such as the following    123192. 0. 2. 1 \xa0\xa0\xa0A \xa0\xa0\xa0example. com192. 0. 2. 2 \xa0\xa0\xa0A \xa0\xa0\xa0example. com192. 0. 2. 3 \xa0\xa0\xa0A \xa0\xa0\xa0example. com What if the DNS server is down? The good news is that the DNS responses are cached (TTL) almost everywhere. The bad news is that all bets are off when caches expired and the DNS server is still down. Deeper Pipeline: Services:  If you remember anything from the fetch-decode-execute cycles from your pipelined processor, you know the pipeline that splits the processor into stages reduces the latency of each stage, so the processor can be clocked at a much higher frequency and hence higher throughput. The same principle applies in the server architecture. Instead of having the API server do all the work, build a pipeline of aggregation and services. A service might depend on other services, meaning a service backend might become the service frontend of other services. Then you need to deal with service discovery, authentication, internal load balancing, etc. Perhaps Istio is worth considering. Take another look at the graph. There is no wonder why it is called a service mesh, with everything tied to everything. But how do you scale those services behind a service? If the recursive nature is still not clear, read this passage front the top. "},{id:40,url:"/docker-intro/",title:"Docker: The Container Metaphor with Profound Revolution",body:"2018/02/24 - Many regard containers as a virtualization technology. They are missing out. Docker has much more to offer. It is a graceful solution to some of the most painful experience in development and deployment we have ever had. Just imagine the all-too-common case where before the pending deployment the Dev has to ask Ops to upgrade in production one of the binary dependencies to release x. y. z or to supply new environment variables and command-line flags to enable some new features. This stinks on so many levels. Besides the velocity loss and communication overhead, it is just so easy for the application to accidentally rely on artifacts from the previous release or outdated environment variables we forget to unset. If the upgrade aborts, rolling back all the (not atomic, not idempotent) steps will be just as painful and error-prone. Isn\u2019t it wonderful if developers could install all the dependencies themselves, implement the application, ship exactly what they have just built, and deploy it in an atomic, hermetic unit? It is such a simple idea with immense repercussion. Docker is the state-of-the-art implementation of this idea. Hermetic Encapsulation of Application, Configuration and Dependency: Product shipping includes not just the application but also its configuration (environment variables, flags, etc) and external dependencies (Debian packages, vender of Golang, node_modules of node. js, etc. ). These are packaged into a single Docker image. A Dockerfile outlines the steps of how a Docker image is built. By defining all the configuration and installing all dependencies through Dockerfile, the output Docker image is self-contained with all we need to run what we ship. It offers hermetic deployment because the container abstraction provides simple isolation boundary that ensures that the environment configuration does not escape to host or to other containers (sandbox) and it cannot be altered by either the host OS or other containers without explicit authorization (lockbox). When the container is brought down, the entire environment is brought down with the application. Nothing in the environment lives longer than the application itself, a simple yet powerful abstraction. The Container Metaphor: Universal Tooling regardless of Language and OS Distro: Docker produces a single artifact from each build. No matter what language or framework the application is implemented in, no matter what distribution of the Linux the host runs, the output is always multi-layered Docker image, which is built and handled by the Docker tooling. This is the shipping container metaphor. A single, transferable unit that universal tooling can handle, regardless of what it contains. Like a container port, any Docker tooling (such as an image registry that stores and transport images like a host git repository would for codebase) has to ever deal with one kind of package \u2013 the Docker images. This is powerful in two ways. It means all the Docker tooling you or others make will be reusable to everyone else. It also means the build artifacts are extremely portable, which can be deployed on any system with a Docker daemon running (profound impact in the world of hybrid cloud). Atomic Upgrade: As presented in the example at the beginning, upgrades in the system are often performed in a non-atomic, multi-step operation \u2013 download, patch, coalesce. Is it possible to simply pull down a new (light) release image, deploy it, and if things go south, quickly roll back to the previous image we were using? Docker does exactly this, because a Docker image encapsulates all the patches and configuration. Docker can instantiate an instance using the new image just pulled and if everything passes, do an atomic rename. Fast, Ephemeral and Stateless Instances: A container is just a process that talks to the Linux kernel directly. It takes seconds to start and run a Docker container, while it takes minutes at best for a virtual machine. Virtual machines are long-lived in nature, as they aim to simulate actual hardware and runs guest OS that might be different from that of the host. Containers, on the other hand, share the Linux kernel and the images from which they instantiate are often just a few megabytes, while a VM image usually takes a few gigabytes. Such lightweight property means containers are perfect for stateless, ephemeral, elastically scaling systems. "},{id:41,url:"/istio/",title:"Istio: Noninvasive Governance of Microservices on Hybrid Cloud",body:"2018/02/02 - As presented in my previous post, microservices are the state-of-the-art architecture for building scalable, highly-available, manageable backend. \xa0No more 30-minute build time, single point of failure, and constant regression from crazy linking or backdoor dependency. But as one breaks off the monolithic application into microservices, new challenges surface. Procedure calls are now inter-process communication through the asynchronous network, which needs service discovery, load balancing, authentication, authorization, liveness detection, traffic monitoring, etc. Handling all these not only is a ton of work but also requires code change to your services.   Istio addresses many of such challenges during the migration towards service mesh. The term service mesh is often used to describe the network of microservices that make up such applications and the interactions between them. The most profound innovation of Istio is perhaps the noninvasive approach to connect, manage, and secure microservices. It does so using Envoy, which is a high-performance proxy from Lyft. Istio pairs each of your containers with a proxy that intercepts and redirects all of your ingress and egress traffic. By configuring on the proxy specific routing rules, access control, and certificates through the control plane, microservices management can be easily achieved.    Take some powerful use cases for examples.   Canary release or A/B testing: configure your routing rules to redirect your downstream service requests to the instances of different versions. Secured RPC: Receive certificates from control plane which acts as CA / root of trust. Bootstrap secure channel using recipient\u2019s public key in certs. Exchange symmetric key\xa0through this channel. Carry on communication using the symmetric key. Policy enforcement (access control, quota limit, etc. ): Proxy queries the control plane for policies regarding the type of request it received and the client identity associated with such request. Cache the results afterward. Enforce the policy on proxy. Telemetry: Monitor all traffic in the service mesh with data reported by proxy. But how does Istio ensure that a proxy is paired with a container on creation? The answer is Kubernetes (k8s) pods. A pod is an atomic orchestration unit that consists of one or many containers with shared storage, network namespace, and a specification on how to run the containers. It is an atomic unit because all containers in a pod are always run together, creating a close coupling that is exactly what Istio wants for the proxy and service. But Istio is noninvasive, and the pod spec from the application does not include the pairing of proxy. Then we are back to the question of how does Istio ensure that a proxy is paired with a container on creation? The answer lies still in k8s. Istio uses the k8s initializer, which is an extension mechanism that allows the injection of a container before the pod (or any k8s objects) is created without modifying the pod spec. Apparently, Istio injects a proxy to each container in the pod. Back in the days when pods and k8s initializers were first introduced, people doubted that such feature is useful at all and might just be over-design. But after the debut of Istio that takes advantage of this, the visionary and insightfulness of its creators cannot be more evident. "},{id:42,url:"/mq/",title:"Killer Apps of Message Queues",body:"2018/01/16 - Message queues are an asynchronous inter-process communication protocol that gains much of its glory with the recent hypes in microservices. Senders and receivers do not interact with the middleware at the same time, so message queues are a great way of decoupling communication from business logic since upstream services may talk to the middleware only and not with any other services.   MQs are not a panacea. It introduces additional components to the system and thus new challenges in availability and reliability. The extra roundtrip to downstream receivers means extra delay. The rule of thumb would be whenever synchronous invocation makes more sense, do RPC and not MQ. Let\u2019s focus on the killer applications of MQ in the following text. Buffering Traffic Spikes to Downstream Services: One downside of RPC framework is that the downstream services/callees have no flow control. If the downstream service demand is much higher than upstream request rates, especially so when downstream handles complex business logic with atomic, durable operations, then essentially the downstream is DOS-ed and that service is gone. Common solutions include upstream queuing to limit request rate or downstream queuing to limit execution rate, yet both introduces unnecessary complexity to the service code.   With MQ, flow control could be easily achieved with downstream pulling from MQ-middleware at its own rate that matches its service demand, which may be further optimized with batch-writes or asynchronous durable writes. Pub/Sub to remove reverse dependency: Imagine a user makes a new post on a forum, which is going to reward the user with some points, update the user personal stats, push to other user feeds, etc. It seems an invocation relationship between upstream and downstream services, but the upstream hardly cares\xa0about the results of downstream execution and just want to make sure things eventually get executed. Using RPCs puts reverse dependency into upstream because whenever a new set of logic or a new service was added to the list of callees, developers have to change the upstream service code, build, and deploy. Downstream service outage also affects upstream availability.   Using MQ, new downstream service could subscribe to the upstream and thus no code change to the upstream. The upstream also no longer needs to wait for all downstreams to reply, reduces service demand, and returns success once MQ says the message is committed. Higher throughputs in long, synchronous tasks: There are also times when the upstream cares about the results but execution takes a while (for example third-party service call through public network, such as requests to PayPal payment). The upstream may call the API directly and receive an acknowledgment. After the transaction is processed, Paypal invokes a callback to a uniform gateway of ours, which pushes to MQ. The upstream subscribes to such messages and gets results eventually.   "},{id:43,url:"/sec/",title:"A Primer on Secure Communication Channels",body:"2017/09/10 - In the world of internet, sending messages in clear text is like swimming naked. We would love some secure communication channels free from eavesdropping or tampering. Security as such is not some trivial question. An evolution of designs is presented below to illustrate the challenge we face and how we end up trusting trust (PKI-CA). Design 0: Clear text: The reason clear text transmission is unsafe is that the connection between the two parties is logical. Packets are in fact routed multiple times before reaching destination, which means any intermediate node has access to the payload, hence no privacy. Exposures to the intermediate nodes are unavoidable. It is the way the internet was built and the reason the internet can be scalable to billions of users. It is funny that for hundreds of years mathematicians had engaged with cryptography but were unsure how it might be applicable to anything if at all. Luckily when the internet started booming and security concern raised, we had every tool in our hands. Design 1: Symmetric key: It is assumed that an encrypted message cannot be decrypted in tractable time unless one holds the decryption key. When the encryption and decryption keys are the same, it is said to be symmetric. One major challenge when using cryptos is the distribution of secrets, in this case the symmetric key. If the internet is the only communication medium, then any sharing of keys is in clear text (no encryption before knowing the key), and all bets are off once any third party knows the secret. If you want to encrypt the key before sending over, then you need another key for this encryption and you still face the challenge of secret sharing. We might approach the problem by hard coding it to the two communication parties, but the client code is not secure. It is subjective to manipulation even in binary format. Design 2: Public key and private key: We may choose different keys for encryption and decryption respectively and share the encryption (public) key only. Any message encrypted with this public key can only be decrypted with the private key, which is assumed to be known only to the principal. Each party mints a key pair and it never has to change. But sharing the public key leads to the initial problem of secret distribution. When Bob receives a key marked as Alice\u2019s, how can Bob be sure? Mallory in the middle could easily swap the key with hers so that when Bob thinks he is talking to Alice, it is actually Mallory. Design 3: Root of trust and public key as identity: To ensure that the key Bob receives is indeed Alice\u2019s public key, Bob needs to hear it from someone he trusts before he can trust Alice, if we assume trust is transitive. However, to find that someone Bob trusts, Bob has to trust another one who trusts that someone. To end the inductive chain, we must have some root of trust. In the internet, it is the certificate authority (CA). A certificate of Alice is Alice\u2019s public key digitally signed by the CA. The digital signature of any content is the hash of that content then encrypted with the CA\u2019s public key. All CA\u2019s public keys are baked in the client, like your browsers. If Bob trusts the CA, who trusts Alice, then Bob could trust Alice given a signed certificate. With knowledge of Alice\u2019s public key, Bob may bootstrap a secured communication with Alice by encrypting his public key using Alice\u2019s public key. Only Alice will be able to decrypt this message, who may reply messages encrypted using Bob\u2019s public key. Since asymmetric keys are computationally expensive to use, they are often used to bootstrap a secure channel through which a symmetric key is shared. Subsequent communication is then encrypted and decrypted using that symmetric key. Problem remains: The trust aforementioned refers to the confidence in one\u2019s identity but says nothing about the information provided or assertion made by that principal. Also in such centralized design, once the root of trust compromises, then all bets are off. DigiNotor is a contemporary example. "},{id:44,url:"/multi-repo/",title:"Dependency Update and Artifacts Promotion in Multi-repo Project",body:"2017/08/13 -  We all know Google employs a version tracking system that uses a single repository/depot. Every close-source google product that you love is tracked by this single repo, which is so large that it cannot fit onto a single disk drive and must be hosted on the cloud. During my first encounter with the monstrous system, I was just as baffled as you might be, but I have come to appreciate the benefits (and costs) of a single repo after I start working on the Istio project. An open-sourced management platform for microservices, Istio presents a uniform abstraction over heterogeneous cloud vendors to support canary release, policy enforcement, telemetry, and much more (shameless plug). It consists of multiple repositories on Github with the vision that each module could be used independently outside of Istio, except that for Istio itself the independence we would like does not exist due to the dependency among these repos. By dependency, I mean each repo needs other repos to build itself. Istio uses Bazel\xa0build tool. All dependencies are defined in the WORKSPACE file at the root directory of each repo, and each dependency includes a commit SHA pointer that specifies the exact version used in this build.   One nice feature of a single repo is the ability to make atomic changes given the property of a single point of serialization. Each commit/snapshot/diff/view may touch multiple files and is applied to the code base atomically (analogous to transactions). For files spanning multiple repos, atomicity is no longer guaranteed. The major challenge in a multi-repo scheme is staleness. When the stable branch of a dependency advances to a new version, the parent repo still uses the dependency of the older version, unaware of such an update. To change the SHA pointer that the parent repo has, it is going to be a separate PR on the parent aside from the one that updates the dependency, which is the reason atomicity is gone. Yet having one single repo makes continuous integration slow, where each PR must pass the pre-submit test before merging. Even a one-line change runs tests on the entire project to prevent regressions. As the repo grows in size, so does the test suite. When each CI takes two hours to complete, productivity suffers. Google\u2019s solution is Blaze (whose open source version is Bazel, shout out the to the anagram), which defines a hierarchical build dependency so that the affected files by any code change and be exactly identified, so only the affected tests need to be run. Multiple repos, on the other hand, make CI much easier since tests could be partitioned in different repos and PRs on each repo only triggers tests on that repo. Back to our problem of stale dependency. For changes involving separated shards/participants, the first idea that comes to mind is distributed transactions (say two-phase locking and two-phase commit). But it means an entire revamp that requires additional tooling on code reviewing multi-repo, aggregating pre-submit CI testing, etc, which is likely to take up the entire quarter and everyone on the dev team still suffers in the meantime. We prefer something less invasive but sooner to deployment. The design is to let go of strong consistency for eventual one, by running cron jobs periodically checking dependency versions and if changed, create a PR on the parent repo. It may take a few runs for a change in the leaves to propagate to the root, but it is okay for the repo to be stale by a couple of hours, because we know eventually the entire project is consistent. If you are interested in the binary used to do this, check this out. "},{id:45,url:"/session/",title:"Session Consistency in Replicated Frontend Servers",body:"2017/07/02 - HTTP provides an abstraction of short connections. Unlike the continuous byte streams in TCP, exchanges between client and server over HTTP starts with a client request and ends with server response, which is meant to be stateless. Sometimes, the server needs more context of the conversation to properly respond, for example, confirmation of user login. Information as such persists throughout the session with the server. When there is only one instance of the server, that is where the session resides and session consistency follows because of the trivial single-copy semantics.   This design, however, is by no means scalable or highly available. Oftentimes, such frontend servers are identically geo-replicated and routed with a reverse proxy. The challenge is how to maintain a session even if subsequent client requests hit some different server. Several solutions are presented here and the pros and cons discussed. Server synchronization: gossip/epidemic/anti-entropy: If the session is synchronized among all servers, then it does not matter which one the client hits. We do so by having servers exchange all session information.   It is nice that such a change affects no application code. However, synchronous server exchanges impose additional delays to ensure all other replica servers have received such session info, and worse, then entire endpoint becomes irresponsive during network partition among servers. Asynchronous server exchanges, on the other hand, provide only weak/eventual consistency, so in our example, users might be asked to log in again even after they have done so. Lastly, such a scheme does not scale as the number of exchange messages grows exponentially with the number of servers. Consistent hashing: If we ensure that requests from the same client always reach to the same server, then we do not have to deal with session replication and consistency anymore. Instead of doing randomization or round robin, perhaps the reverse proxy could be based on IP address (Level 4 routing) or business logic property such as user_id, order_id, item_id, etc (Level 7 routing). This is nice since we only need to change the Nginx configuration and application/server code remains intact. Loads at the servers are also balanced, assuming good hashing scheme. Also scalable with more server instances. However, this does not solve the high availability part, since whenever the server fails or partition happens, clients are unable to talk to the original server and must be remapped to a different server and log in again. Upon replica set resizing, redistribution of session information must happen the same client might be hashed to the newly added servers. Client-side storage: To manage the session information on the client side, one might take advantage of the browser cookies, which relieves bandwidth and storage burden from servers. However, it might be subjected to illegal mutation, leaking, and other security threats. The session is also limited by cookie size. Server-side durable storage: This is, in my opinion, the best solution. We keep the session information in our backend storage, which makes the frontend server truly identical and stateless. You may ask what if the storage fails. That is an excellent question, and the state-of-the-art answer would be sharding, replication and high-power consensus (paxos, raft, viewstamp). It makes more sense to consolidate these properties as a service and build other services on top of that (Chubby, Zookeeper, BigTable).   "},{id:46,url:"/pagination/",title:"Pagination Ordered by Secondary Keys on Sharded Stores",
body:"2017/04/23 - A common design for content display, pagination partitions information into multiple pages and serves one at a time. We have seen it in search results, message history, and cascading news feed, etc. It shrinks the payload of server response and therefore reduces the response latency. It is often the case that the table in question (1) has a primary key, such as orderId, itemId, msgId, and (2) pagination is sorted on some secondary key such as time, price, popularity. When the table is small, indexing on the secondary key plus the SQL offset/limit in will do. For higher throughput, the table might be sharded on a partition key, and each table running on different nodes governs a disjoined segment of key space. The partition key is often chosen as the primary key, which makes pagination using secondary keys much more challenging, because none of the nodes have a global view of the entire dataset. We describe three approaches to this problem. External Merging: Say the client wants the 3rd page sorted by time from the storage of two shards. Entries are likely interleaved between the two shards, but an extreme case would be that the latest 3 pages all reside on either of the shards. For correctness, we must read the first 3 pages (not just the 3rd) from each shard, then merge the responses in memory on service tier and return the 3rd page from the total order. This may be the most obvious solution, but it does not scale. Imagine the client wants the 100th page and each page has 200 entries. Or you have got multiple shards that are geo-distributed. Now you have to read 20000 entries from each shard and then suffer the high network latency as well as high CPU usage. Compromise: No Skipping but only Next Page:  The limit on the page index query only allows the client to visit the next page (e. g. your facebook news feed). This is essentially an optimization over external merging in that each shard always returns only one page for each request. The coordinator/aggregator merges the result and records the highest value of the secondary key from each shard. On the fetching next page, such value is used as the selection condition to get the next load of one page from each shard. Lookup Twice: Assume each page has 5 rows and we were to get the 200th page. The SQL query is    1select * from T order by time offset 1000 limit 5; If there are 3 shards in total, rewrite the query as    1select * from T order by time offset 333 limit 5; and the following is the result from each shard.   Then, do a range query using the\xa0between directive, which starts from the min and ends at the local_max. The results are shown below.   We easily conclude that\xa0min has offset 333 on shard 0, 331 on shard 1, and 330 on shard 2 (offsets rounded down since min does not really exist on the other shards). It follows that min on the total order has an offset of 333+331+330 = 994. This is a crucial piece of information, because after an in-memory merging (results from each shard already sorted), we know that the entry 6 rows below min is of offset 1000 on the global order, and limit 5 is just the next 5 immediate neighbors. "},{id:47,url:"/kip/",title:"Kip\u2019s Warehouse: Building Scalable, Reliable, Consistent Web Application from the Ground Up",body:"2017/02/25 - I have been working with another three wonderful people on the senior design project, which is a web application of an inventory management system, and the production is up at kipswarehouse. com. The Department of Electrical and Computer Engineering at Duke will be the first customer for our system to manage the items and requests, etc for all ECE labs. Our tech stack is summarized below.    12345678910111213Front-end \xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 React + BlueprintJSBack-end \xa0\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0node. js + ExpressWeb Server \xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 nginxDatabase \xa0\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0MySQLORM \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0SequelizeBackup \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0AWS S3 + GlacierAPI \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0json RESTCICD \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0TravisCIVersion Control \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 GithubCode Review \xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 PhabricatorVPS \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0DigitalOceanDNS + CDN \xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 CloudFlareRemote Process Monitor \xa0\xa0\xa0\xa0keymetrics This is a super awesome opportunity. For all my previous internships, I came in with a colossal amount of legacy code that needs to be maintained and carefully updated. This project the very first time where we decide on EVERYTHING in our tech stack, deploy and evolve. This is also the first node. js application I built. For a semester-long project like this, we prefer fast development and well support. The node engine interprets the script at runtime, so every time we make a change to the server code, no recompilation is required. Our frontend is ReactJS, so it makes sense for our RESTful API to be done over json, and serialization/marshaling/flattening/pickling for RPCs is given for free. We are also using Sequelize as our ORM to avoid handing writing SQL query ourselves to risk performance. Sequelize by itself has been robust and well documented, but it certainly brings new complexity, if not unnecessary. One notorious case we run into is the transactions on the\xa0database. In SQL query, a transaction is a really simple yet powerful abstraction. Start Transaction; Commit; Boom. Two extra lines and you got atomic, consistent, independent, durable records. Sequelize, however, implements it with promises and callbacks. Doing transactions in Sequelize turns into nested callbacks with the transaction object passed all the way into the deepest one. Worse off, if one runs into bugs such as accessing fields on an undefined object, the transaction simply aborts without telling what went wrong in between (which reminds me the dark old days of mysterious segfaults). Normally node. js handles this case well, but now Sequelize masks it and debugging could have been easier. I attach some slides I used for the presentation of our alpha release. We are definitely hyped about this and will keep y\u2019all updated! (Oh good lord of Richard Hendricks, please don\u2019t sue us. )          "},{id:48,url:"/git/",title:"Git as Version Vector",body:"2016/12/22 - Git is one of the most widely used version control systems. Traditionally, arepository on git is considered as a complete history of the entire project inthe form of chains of blocks. Each block is a commit, and one could create aversion branching from any point throughout the chain. Alternatively, one may view git as a highly available system that implementsversion vectors. It is highly available in that whenever you push to your localrepo git never rejects you, but of course that local update is tentative untilit is appended or merged with master. The interesting aspect of git arises whenyou branch off master and work on a feature branch, while others at the sametime could do exactly the same. This is a concurrent update. We are speaking of concurrency in a rather strict sense. Consider thefollowing example.   When C3 branches off C1, C3 must have seen C1 as well as C0, so we say C0causally precedes C1, and C1 causally precedes C3. Causality here is defined asa happened-before relationship, where C0 might not necessarily cause\xa0C1 but itcould. However, when someone else appends C2 onto C1, he or she has not seen C3or C4, and the person working on C3 and C4 has not seen C2, such that C4 doesnot causally precede\xa0C2 and C2 does not causally precede C4. Therefore, C2 andC4 are concurrent. Remember that git ensures a total order of updates/commits as seen by all theparticipants, which means concurrent modification as such must eventually beserialized as either C2 precedes C3 and then C4, or C3, C4 and then C2. Whichorder to choose is rather arbitrary but what matters is that once a causal orderis chosen, the rest of all participants must agree and observe such ordering. Concurrent updates might be conflicting, and that is why the git mergingprocedure requires user involvement to resolve merge conflicts. But how does git know whether two updates are concurrent? Version vectors. Itall started with the hall of fame paper Time, clocks, and the ordering of eventsby Leslie Lamport, who defined a total causal order using the logical clock. Thelogical clock (LC) is just a monotonically increasing counter and each branchhas its own logical clock. Each commit on branch i is then stamped with &lt;LC_i,i&gt; and a causal order could be defined as from lower LC to higher LC and breakthe tie using i. Notice that there could multiple causal orders given a set of events, and theorder derived from LC while preserving causality imposes an unnecessary orderingconstraint on concurrent events, and worse, using just logically clock isinsufficient to tell whether two events are concurrent. Users might desire adifferent order after all. So we need a vector clock, which is a vector whose element at index i is thelogical clocks of branch i. Now we know update u causally precedes update v ifVC(u) dominates VC(v), and such domination is true if each logical clock inVC(u) is greater than or equal to that in VC(v) at the same index. Therefore, ifneither vector clock dominates the other, then we conclude that the two updatesare concurrent, which is where a merging procedure is needed. "},{id:49,url:"/quorum/",title:"Sloppy Quorum And Eventual Consistency",body:"2016/11/18 - Here is where we stand. Fisher-Lynch-Patterson has shown that consensus is notguaranteed in bounded time in a purely asynchronous network. The CAP theoremshows that from consistency, availability, and partition-resilience, we couldonly choose two. We have seen systems using ACID transactions and high poweredconsensus protocols such as Paxos, Viewstamp Replication, and Raft. We have beenchoosing to stand on the CP side, forgoing availability to ensure consistencyunder network partition. After all, isn\u2019t it nice for a read to always see the latest write? But availability is quite a sacrifice. When you Google something, or check yourTwitter NewsFeed, would you rather have some results/tweets show up, albeit alittle stale, than be denied service just because you happen to be on theminority side of a network partition? Or when I add an item to shopping cart, Ireally do not expect it to reject such operation in any case. It is worthwhileto reconsider the tradeoff between availability and consistency under thesescenarios. Classic consensus algorithms serialize all operations at the primary / leader /master / coordinator that maintains the single-copy semantics to preserveconsistency. Consistency could be generalized to a multi-coordinator case withNWR quorum where a write quorum intersects with a read quorum, or W + R &gt; N. Inthe following example, N is 5 and W = R = 3.   We could even optimize read for lower latency by letting R = 2 and W = 4. Thenodes might scatter across a wide area, and the less confirmations we have towait, the sooner we could commit.   We could push further down for lower latency in that R + W &lt; N + 1 where we haveonly eventual consistency. It is weaker since a write might not be immediatelyavailable for subsequent read, but eventually it will, as two quorums might bedisjoint. In the next example, Alice first writes a new value to v and then tellBob to read it, but Bob still observes a stale value.   Eventually, Bob will be able to read the latest write. But how eventual? Howlong does Bob have to wait? The answer is that we can give promises but we couldprovide expectations using probability bounded staleness as a way to quantifylatency-consistency trade-offs. We define t-visibility as getting consistentreads with probability p after t seconds. Here is some interesting figure fromLinkedIn.   "},{id:50,url:"/fourier/",title:"Fourier, Phasors, LTI and All That",body:"2016/10/08 - We all share the sorrow and misery from that signal processing class. \xa0You werethrown at some\xa0crazy formula, kind of know how to use them but probably neverunderstand why we are doing this after all. I hope this post helps with yourlingering confusion to have you realize the power if not the\xa0beauty of the toolsin your hand. Consider some\xa0vector in a 3D space. It could be represented by rectangularcoordinates &lt;x, y, z&gt; or spherical coordinates &lt;r, theta, phi&gt;. It is the samevector, after all, represented using different basis vectors. A time-varying signal is no different. It is represented as a linear combinationof an infinite set of functions each defined only in only a single point in timeand whose time components are separated by an infinitesimal\xa0piece. Theinteresting thing is, we could choose a different set of basis vectors that alsospan such infinite\xa0vector space to represent the very same signal. And this iswhen the Fourier basis comes into play. Representing the signal using the Fourier basis essentially transforms thesignal from the time domain to frequency domain. Why is this helpful? Well, anytwo\xa0Fourier basis vectors are orthogonal to each other, which means their innerproduct is zero, which is a fancy\xa0way of saying they are \u201cperpendicular\u201d to eachother, even though it is hard to visualize what it looks like in theinfinite-dimension space. This property makes the calculation of thecoefficients of the linear combination of the\xa0Fourier basis really easy toderivate. The full power of Fourier transformation is realized in the LTI system. LTIstands for linear, time-invariant. Any LTI system possesses both scaling andsuperposition properties. It means if an input signal is represented as a linearcombination of\xa0Fourier basis vectors, then we could analyze it on a term by termbasis, and the final output is the addition of all. But the punch line is this: each Fourier basis vector is the Eigenfunction ofthe LTI system, so if it is the input to some LTI, then its output is the samebasis vector scaled by an\xa0Eigenvalue. The\xa0Eigenvalue here is what we referred toas the frequency response of the LTI system, which is also a phasor. We are using vectors and functions interchangeably in this particular construct. Once we know the frequency response, finding the output signal of this oneFourier basis vector is easy, and the total output is just the sum of outputsfrom the spectrum of frequencies. "},{id:51,url:"/rsm/",title:"Reliable & Consistent Service: Linearizable RPC and Replicated State Machine",body:"2016/09/13 - Remote Procedure Call (RPC) is a canonical structuring paradigm forclient-server/request-response services.   This simplified diagram overlooks the challenges we face such as unreliablenetworks and remote server crash and recovery. In the famous Birrell/Nelsonpaper, at-most-once semantics was proposed that request is resent upon timeoutso it eventually does reach the\xa0server, and that server is stateless so itrestarts when failed and continues service probably retransmitted request (afailed server is a slow server). But then a request might be retransmitted right before the response\xa0arrives\xa0sothat the operation might be executed twice.   In the 80s, people believe that requests are essentially read and write, both ofwhich are idempotent, meaning executing them twice will not change the response,so at-least-once semantics are fine. \xa0For a long time, we thought at-least-oncesemantics + idempotent operations = linearizable RPC, but it turns out it is notnecessarily true. Consider the following example. The very last read shouldreturn 3 in a linearizable system. The takeaway is we need exactly-oncesemantics.   Reply cache has come to rescue. Each RPC is tagged with an id and its returnvalue is put in the reply cache such that when the retransmitted RPC arrives,the server does not re-execute and return the value in cache immediately. Thecache entry is evicted when the client ACKs. Using a reply cache achievesat-most-once semantics, which combined with retransmission achievesexactly-once\xa0semantics. But what about failovers? When an instance of the server fails, all loads areshifted to other servers. To get\xa0consistent results requires the reply cache tobe durable, which means each operation replicated across the entire server grid. This is why linearizability is so important. A concurrent implementation iscorrect if its behavior is equivalent to some serial order. Each server could beviewed as a state machine and with a sequence (linearized) of applied actions,its final state is deterministic and could be replicated. Paxos, ViewstampedReplication, Raft are all consensus algorithms for replication. The one last twist is that upon failovers, the client needs to keep track of thenew elected master/leader and send requests to her. Of course, the clientapplication\xa0is not the one to\xa0do such bookkeeping.   Take Viewstamped Replication for example. VR Proxy on the client side keepstrack of the current master. On the server side, it is the VR code that managesreplication, election, etc. It is therefore independent of the underlying servercode and could be reused readily.   "},{id:52,url:"/microservices/",title:"Service-Oriented Architecture: Why did Microservices Catch On",body:"2016/09/04 -    All teams will henceforth expose their data and functionality through service interfaces.   There will be no other form of inter-process communication (IPC) allowed: nodirect linking, no direct reads of another team\u2019s data store, noshared-memory model, no back-doors whatsoever.   The only communication allowed is via service interface calls over the network.   Anyone who doesn\u2019t do this will be fired. \xa0Thank you; have a nice day!  \u2013\xa0Jeff Bezo, Amazon CEO You may have been wondering why in the world would anybody prefer the extralayer of abstraction (the service tier) instead of just doing things the oldway.   Microservices are really just a more concrete and modern interpretation ofservice-oriented architectures (SOA). It goes one step further by taking on theUNIX design philosophy where each service/command does only one thing and doesit well. \xa0To help understand the advantages of SOA,\xa0I have listed out somepotential problems that one could encounter without the service tier and I willexplain how SOA has\xa0cracked them. Duplicated code: Consider the following hierarchy where\xa0all three types of requests do queries on user data.   Without the service tier, if requests are handled by different servers, theneach of the servers need to implement the SQL query individually, whichessentially creates duplicated code. Spread of Complexity: As concurrency bulks up, database queries become bottlenecks. Cache is thenintroduced to the to hierarchy to mitigate\xa0query\xa0volumes that actually hit DB. Without a unified service tier for user_db queries, upper layers must upgradeto deal with the new cache layer directly. This new directly-exposedrequest-unrelated complexity forces all servers to upgrade. Imagine doing thisover and over as your business goes. Did I mention you also need to keep trackof all different places that are impacted every time you make a new\xa0change? Reused but\xa0Coupled Library \xa0: SOA is not the only solution to the two problems aforementioned. Extractingcommon\xa0actions to a library, say user. so, \xa0is mostly the first solution found. Single point of update, hidden implementation, unified interface, great. Butdoing so introduces new problems \u2013 version maintenance\xa0and coupling of code. Ifserver pipeline A handling request type A upgrades user. so from version 1 toversion 2, other\xa0server pipelines are forced to upgrade to be compatible withthe new version of user. so, the comeback of problem 2. Or, we could retain adifferent version of\xa0user. so based on the pipeline, but then problem 1 arises. Performance Impact by\xa0Other Server Pipelines: Each server executes SQL queries on its own. If server A does a full scan onuser_db and takes DB CPU \xa0to 100% to usage, it essentially blocks all otherservers from making progress. \xa0  How SOA solves them:  Single point of change. Fix one bug in service tier you fix them all.  Unified abstraction for upper layers hides implementation details. Changessuch as new cache, extract db, split\xa0table preserves the interface exposed toclients, who no longer needs to keep on updating along with the service.  With the new service tier  Now the user_service could mitigate/schedule queries to DB and no single server could have it all, which preserves the minimum performance spec.  Easy to use. Before you need crazy linking and binding for communication, ifnot serialization, reliable transmit, background execution, etc. Now it is allRPC which appears no different from invoking local procedures. "},{id:53,url:"/git2/",title:"Git: Branch off An Unmerged Branch While Committing Often - Disasters and Salvage",body:"2016/08/14 - Committing often and pushing often has been advocated as good practice whenusing Git, which saves your latest work on remote even if your hard drive diesright after and which provides a more fine-grained\xa0selection when rolling back. It is also good practice to make\xa0each pull request consist of\xa0only\xa0one commit,so that it is easier to cherry-pick stable features from master to releasebranch. So what I do is to squash all commits into one right before I send themout for review. Sometimes, I would branch off the current branch I was\xa0working on but wasblocked when the project was building/compiling or my reviewers have yet to getback to me. It works out by having multiple local repos in differentdirectories. The problem comes in when I later make more commits onto the original branch Ijust branched off, as illustrated below.   Say we rebase from the master branch at commit C0, and we branch off fromFeature A at commit C1 to work on Feature B. Later, our reviewers finallygot back to us and we continue work on Feature A to create commits\xa0C2 andC5. Now Feature A is ready for the pull request if squashed. The problemshows up when\xa0some files are touched by both\xa0the branch Feature A and FeatureB. If we squash Feature A,\xa0submit\xa0the pull request from Feature A againstmaster, and finally rebase Feature B from master, crazy merge conflictsarise.   The conflict results from the fact that both Feature A and Feature B touchthe same files and that we rewrite history by squashing (interactive rebase) allcommits of Feature A, including those between C0 and C1. Now S0 is anew\xa0commit different from any of Feature B, and the merge conflicts really isabout the conflict between S0 and C0-C1. Resolving this conflict by hand is a nightmare, because Git applies commits oneby one chronologically, which means we have to resolve the conflict again andagain until the last commit. I encountered this problem in my last internship. At first, I tried to resolveall conflicts by hand and it was not pretty and took me an hour to do so. Thisproblem kept showing up and I believe there must be some other way. I did eventually find a solution.   I first squash all commits on Feature B right until the one it branches offFeature A. Then, squash Feature A until C0. Notice that S2 is now danglingas its parent is not in the source tree anymore. \xa0Then, instead of a merge,cherry-pick S2 to apply onto S1, and do a hard reset to have Feature Bpoint to the new commit. Now approve the pull request to merge\xa0S1 to master. After that, when youcheckout Feature B and do a rebase from master, since the exact S1 isalready in master, no conflicts arise because of Feature A. (there mightstill be conflicts of course if your teammate pushed the delta on the same filesright before you rebase). The bane of the problem might just be that the Git way of taking snapshots of abranch, in that it allows the coexistence of\xa0multiple snapshots (i. e. commits)on a branch, which only comes in handy on fine-grained rollback but also bringsmore trouble in a more common setting in my opinion. Google uses customizedPiper for version control, in which deltas are submitted in the quantum of achange list of files, which minimizes the problem encountered here. "}],idx=lunr(function(){this.ref("id"),this.field("title"),this.field("body"),documents.forEach(function(e){this.add(e)},this)});$(function(){$("#lunrsearchresults").on("click","#btnx",function(){$("#lunrsearchresults").hide(5),$("body").removeClass("modal-open")})});